<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[远程连接Oracle数据库(本地不安装Oracle)]]></title>
    <url>%2F2019%2F07%2F12%2FdevNote%2F%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5Oracle%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[下载Oracle客户端instantclient-basic-windows.x64-11.2.0.4.0网盘链接：https://pan.baidu.com/s/1cmLH2EWHqZ-VI0QBCDy0kQ提取码：zlxu 配置Oracle客户端 将客户端压缩包解压 进入解压目录，新建文件夹NETWORK并进入其再新建文件夹ADMIN 进入文件夹ADMIN，创建tnsnames.ora文件并编辑 12345678910ORCL = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = YOURSERVERIP )(PORT = 1521)) ) (CONNECT_DATA = (SERVICE = DEDICATED) (SERVICE_NAME = YOURDBNAME) ) ) ORCL：本机识别服务名，可任意命名，不重复即可 YOURSERVERIP：待连接Oracle数据库服务器IP地址 1521：Oracle数据库暴露的端口号，默认则无需修改 YOURDBNAME：远程数据库服务名，需要按照数据库的服务器进行设置，如远程数据库名为test_db，则设置为test_db。 环境变量准备 新建变量NLS_LANG，值为AMERICAN_AMERICA.AL32UTF8 新建变量TNS_ADMIN，值为${ClientPath}\NETWORK\ADMIN，比如我的设置为D:\Software\DevSoftware\instantclient_12_1\NETWORK\ADMIN。 配置图形化界面软件Navicat 点击工具-&gt;选项-&gt;环境 修改OCI环境，将OCI library(oci.dll)*的值设置为Oracle客户端解压文件中的oci.dll 重启Navicat，再次连接远端Oracle数据库即可成功 连接数据库配置，选择TNS方式连接 PL/SQL Developer 进入PL/SQL，不填数据库选项，选择Cancle即可进入PL/SQL 点击Tools-&gt;Preferencesj进行配置客户端信息 重启输入Oracle信息即可成功连接 Username：为远程数据库用户名 Password：远程数据库密码 Database：与tnsnames.ora与本机识别服务名相同 Connect as：根据用户再数据库中的角色选择即可]]></content>
      <categories>
        <category>JavaUtils</category>
      </categories>
      <tags>
        <tag>JavaUtils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java HotSpot VM选项]]></title>
    <url>%2F2019%2F07%2F11%2F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2FJava%20HotSpot%20VM%E9%80%89%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[Java HotSpot VM选项以下选项分为不同类别。 行为选项会更改VM的基本行为。 垃圾优先（G1）垃圾收集选项 性能调整选项是旋钮，可用于调整VM性能。 调试选项通常可以跟踪，打印或输出VM信息。 官方文档 G1 垃圾收集选项参数 选项及默认值 说明 -XX:+UseG1GC 使用垃圾优先（G1）收集器 -XX:MaxGCPauseMillis=n 设置最大GC暂停时间的目标。这是一个软目标，JVM将尽最大努力实现它。 -XX:InitiatingHeapOccupancyPercent=n 启动并发GC循环的（整个）堆占用百分比。它由GC使用，它基于整个堆的占用而不仅仅是其中一代（例如，G1）触发并发GC循环。值0表示“执行恒定GC循环”。默认值为45。 -XX:NewRatio=n 旧/新一代尺寸的比例。默认值为2。 -XX:SurvivorRatio=n eden/survivor区空间大小的比率。默认值为8。 -XX:MaxTenuringThreshold=n 终身临界值的最大值。默认值为15。 -XX:ParallelGCThreads=n 设置垃圾收集器并行阶段使用的线程数。默认值因运行JVM的平台而异。 -XX:ConcGCThreads=n 并发垃圾收集器将使用的线程数。默认值因运行JVM的平台而异。 -XX:G1ReservePercent=n 设置保留为false上限的堆的数量，以减少促销失败的可能性。默认值为10。 -XX:G1HeapRegionSize=n 使用G1，Java堆被细分为大小均匀的区域。这设置了各个子部门的大小。根据堆大小，符合人体工程学地确定此参数的默认值。最小值为1Mb，最大值为32Mb。 行为选项及默认值选项参数 选项及默认值 说明 -XX:-AllowUserSignalHandlers 如果应用程序安装了信号处理程序，请不要抱怨。（仅与Solaris和Linux相关。） -XX:AltStackSize=16384 备用信号堆栈大小（以千字节为单位）。（仅与Solaris相关，从5.0中删除。） -XX:-DisableExplicitGC 默认情况下，启用对System.gc（）的调用（-XX：-DisableExplicitGC）。使用-XX：+ DisableExplicitGC禁用对System.gc（）的调用。请注意，JVM在必要时仍会执行垃圾回收。 -XX:+FailOverToOldVerifier 新类型检查程序失败时故障转移到旧验证程序。（在6中引入） -XX:+HandlePromotionFailure 最年轻的一代系列不需要保证所有活动对象的完全升级。（在1.4.2更新11中引入）[5.0及更早版本：false。] -XX:+MaxFDLimit 将文件描述符的数量转换为最大值。（仅与Solaris相关。） -XX:PreBlockSpin=10 用于-XX的旋转计数变量：+ UseSpinning。控制在输入操作系统线程同步代码之前允许的最大旋转迭代。（在1.4.2中引入。） -XX:-RelaxAccessControlCheck 放宽验证程序中的访问控制检查。（在6中引入） -XX:+ScavengeBeforeFullGC 在完整GC之前进行年轻一代GC。（在1.4.1中介绍。） -XX:+UseAltSigs 对于VM内部信号，使用备用信号而不是SIGUSR1和SIGUSR2。（在1.3.1更新9中引入，1.4.1。仅与Solaris相关。） -XX:+UseBoundThreads 将用户级线程绑定到内核线程。（仅与Solaris相关。） -XX:-UseConcMarkSweepGC 为旧一代使用并发标记扫描集合。（在1.4.1中引入） -XX:+UseGCOverheadLimit 使用策略限制在引发OutOfMemory错误之前在GC中花费的VM时间的比例。（在6中引入） -XX:+UseLWPSynchronization 使用基于LWP而不是基于线程的同步。（在1.4.0中引入。仅与Solaris相关。） -XX:-UseParallelGC 使用并行垃圾收集清除。（在1.4.1中引入） -XX:-UseParallelOldGC 对完整集合使用并行垃圾回收。启用此选项会自动设置-XX：+ UseParallelGC。（在5.0更新中引入6.） -XX:-UseSerialGC 使用串行垃圾收集。（5.0中引入。） -XX:-UseSpinning 在进入操作系统线程同步代码之前，在Java监视器上启用朴素旋转。（仅与1.4.2和5.0相关。）[1.4.2，多处理器Windows平台：true] -XX:+UseTLAB 使用线程局部对象分配（在1.4.0中引入，在此之前称为UseTLE。）[1.4.2和更早版本，x86或使用-client：false] -XX:+UseSplitVerifier 使用具有StackMapTable属性的新类型检查器。（在5.0中引入。）[5.0：false] -XX:+UseThreadPriorities 使用本机线程优先级。 -XX:+UseVMInterruptibleIO 在EINTR之前或与EINTR进行I / O操作的线程中断导致OS_INTRPT。（仅在Solaris中引入。仅与Solaris相关。） 性能选项参数 选项及默认值 说明 -XX:+AggressiveOpts 打开预期在即将发布的版本中默认的点性能编译器优化。（在5.0更新中引入6.） -XX:CompileThreshold=10000 编译前的方法调用/分支数[-client：1,500] -XX:LargePageSizeInBytes=4m 设置用于Java堆的大页面大小。（在1.4.0更新1中引入。）[amd64：2m。] -XX:MaxHeapFreeRatio=70 GC后最大堆积空闲百分比以避免收缩。 -XX:MaxNewSize=size 新一代的最大大小（以字节为单位）。从1.4开始，MaxNewSize被计算为NewRatio的函数。[1.3.1 Sparc：32m; 1.3.1 x86：2.5米。] -XX:MaxPermSize=64m 永久代的大小。[5.0及更新版本：64位虚拟机缩放30％; 1.4 amd64：96m; 1.3.1 -client：32m。] -XX:MinHeapFreeRatio=40 GC后最小的堆积百分比以避免扩展。 -XX:NewRatio=2 旧/新一代尺寸的比例。[Sparc -client：8; x86 -server：8; x86 -client：12。] - 客户端：4（1.3）8（1.3.1 +），x86：12] -XX:NewSize=2m 新一代的默认大小（以字节为单位）[5.0及更新版本：64位虚拟机缩放30％; x86：1米; x86,5.0及更早版本：640k] -XX:ReservedCodeCacheSize=32m 保留代码缓存大小（以字节为单位） - 最大代码缓存大小。[Solaris 64位，amd64和-server x86：2048m; 在1.5.0_06及更早版本中，Solaris 64位和amd64：1024m。 -XX:SurvivorRatio=8 eden/survivor区空间大小的比例[Solaris amd64：6; Sparc在1.3.1：25; 5.0及更早版本中的其他Solaris平台：32] -XX:TargetSurvivorRatio=50 清除后使用的幸存者空间的所需百分比。 -XX:ThreadStackSize=512 线程堆栈大小（以KB为单位）。（0表示使用默认堆栈大小）[Sparc：512; Solaris x86：320（在5.0及更早版本中为256之前的版本）; Sparc 64位：1024; Linux amd64：1024（5.0及更早版本中为0）; 所有其他的0.] -XX:+UseBiasedLocking 启用偏置锁定。有关更多详细信息，请参阅此调整示例。（在5.0更新中引入6.）[5.0：false] -XX:+UseFastAccessorMethods 使用优化版本的Get Field。 -XX:-UseISM 使用亲密共享内存。[不适用于非Solaris平台。]有关详细信息，请参阅私密共享内存。 -XX:+UseLargePages 使用大页面内存。（在5.0更新5中引入。）有关详细信息，请参阅Java支持大内存页面。 -XX:+UseMPSS 使用具有4mb页面的多页面大小支持。不要与ISM一起使用，因为这取代了对ISM的需求。（在1.4.0更新1中引入，与Solaris 9及更新版本相关。）[1.4.1及更早版本：false] -XX:+UseStringCache 启用常用分配字符串的缓存。 -XX:AllocatePrefetchLines=1 使用JIT编译代码中生成的预取指令在最后一次对象分配后加载的高速缓存行数。如果最后分配的对象是实例，则默认值为1;如果是数组，则默认值为3。 -XX:AllocatePrefetchStyle=1 生成预取指令的代码样式。0 - 没有生成预取指令 d ，1 - 在每次分配后执行预取指令，2 - 在执行预取指令时使用TLAB分配水印指针到门。 -XX:+UseCompressedStrings 对字符串使用byte []，可以表示为纯ASCII。（在Java 6 Update 21性能发布中引入） -XX:+OptimizeStringConcat 尽可能优化字符串连接操作。（在Java 6 Update 20中引入） 调试选项及默认值参数 选项及默认值 说明 -XX:-CITime 打印在JIT编译器中花费的时间。（在1.4.0中引入。） -XX:ErrorFile=./hs_err_pid.log 如果发生错误，请将错误数据保存到此文件中。（在6中引入） -XX:-ExtendedDTraceProbes 启用影响性能的dtrace探针。（仅在Solaris中引入。仅与Solaris相关。） -XX:HeapDumpPath=./java_pid.hprof 堆转储的目录或文件名的路径。可管理。（在1.4.2更新12,5.0更新7中引入。） -XX:-HeapDumpOnOutOfMemoryError 抛出java.lang.OutOfMemoryError时转储堆到文件。可管理。（在1.4.2更新12,5.0更新7中引入。） -XX:OnError=”;“ 在致命错误上运行用户定义的命令。（在1.4.2更新9中引入） -XX:OnOutOfMemoryError=”;“ 首次抛出OutOfMemoryError时运行用户定义的命令。（在1.4.2更新12,6中引入） -XX:-PrintClassHistogram 在Ctrl-Break上打印类实例的直方图。可管理。（在1.4.2中引入。）jmap -histo命令提供了等效的功能。 -XX:-PrintConcurrentLocks 在Ctrl-Break线程转储中打印java.util.concurrent锁。可管理。（在6.中引入）jstack -l命令提供等效功能。 -XX:-PrintCommandLineFlags 打印出现在命令行上的标志。（5.0中引入。） -XX:-PrintCompilation 编译方法时打印消息。 -XX:-PrintGC 在垃圾收集中打印消息。可管理。 -XX:-PrintGCDetails 在垃圾收集中打印更多细节。可管理。（在1.4.0中引入。） -XX:-PrintGCTimeStamps 打印垃圾回收时的时间戳。可管理（1.4.0中引入） -XX:-PrintTenuringDistribution 打印终身年龄信息。 -XX:-PrintAdaptiveSizePolicy 允许打印有关自适应生成大小的信息。 -XX:-TraceClassLoading 跟踪类的加载。 -XX:-TraceClassLoadingPreorder 跟踪按引用顺序（未加载）加载的所有类。（在1.4.2中引入。） -XX:-TraceClassResolution 跟踪按引用顺序（未加载）加载的所有类。（在1.4.2中引入。） -XX:-TraceClassUnloading 跟踪卸载类。 -XX:-TraceLoaderConstraints 跟踪记录装载机约束。（在6中引入） -XX:+PerfDataSaveToFile 退出时保存jvmstat二进制数据。 -XX:ParallelGCThreads=n 设置年轻和旧的并行垃圾收集器中的垃圾收集线程数。默认值因运行JVM的平台而异。 -XX:+UseCompressedOops 允许使用压缩指针（对象引用表示为32位偏移而不是64位指针），以优化64位性能，Java堆大小小于32gb。 -XX:+AlwaysPreTouch 在JVM初始化期间预先触摸Java堆。因此，堆的每个页面在初始化期间都是需求归零，而不是在应用程序执行期间递增。 -XX:AllocatePrefetchDistance=n 设置对象分配的预取距离。将使用新对象的值写入的内存以超出最后分配的对象的地址的距离（以字节为单位）预取到高速缓存中。每个Java线程都有自己的分配点。默认值因运行JVM的平台而异。 -XX:InlineSmallCode=n 仅当先前编译的方法生成的本机代码大小小于此值时，才对其进行内联。默认值因运行JVM的平台而异。 -XX:MaxInlineSize=35 要内联的方法的最大字节码大小。 -XX:FreqInlineSize=n 要内联的频繁执行方法的最大字节码大小。默认值因运行JVM的平台而异。 -XX:LoopUnrollLimit=n 展开循环体，服务器编译器中间表示节点计数小于此值。服务器编译器使用的限制是此值的函数，而不是实际值。默认值因运行JVM的平台而异。 -XX:InitialTenuringThreshold=7 设置用于并行年轻收集器中自适应GC大小调整的初始时效阈值。终结阈值是对象在被提升为旧的或终身的一代之前在年轻的集合中存活的次数。 -XX:MaxTenuringThreshold=n 设置用于自适应GC大小调整的最大暂定阈值。当前最大值为15.并行收集器的默认值为15，CMS为默认值。 -Xloggc: 将GC详细输出记录到指定文件。详细输出由正常的详细GC标志控制。 -XX:-UseGCLogFileRotation 启用GC日志轮换，需要-Xloggc。 -XX:NumberOfGClogFiles=1 设置旋转日志时要使用的文件数，必须&gt; = 1.旋转的日志文件将使用以下命名方案， .0， .1，…， .n- 1。 -XX:GCLogFileSize=8K 日志将在何时旋转日志文件的大小必须&gt; = 8K。]]></content>
      <categories>
        <category>性能优化</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git配置.gitignore]]></title>
    <url>%2F2019%2F07%2F10%2FGit%2Fgit%E9%85%8D%E7%BD%AE.gitignore%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324target/!.mvn/wrapper/maven-wrapper.jar### STS ###.apt_generated.classpath.factorypath.project.settings.springBeans### IntelliJ IDEA ###.idea*.iws*.iml*.ipr### JRebel ###rebel.xml### MAC ###.DS_Store### Other ###logs/temp/.mvn/mvnwmvnw.cmd]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 性能优化]]></title>
    <url>%2F2019%2F07%2F07%2F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2FTomcat%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[配置优化减少配置优化 场景一：假设当前 REST 应用(微服务)、不需要JSP的情况 分析：它不需要静态资源，Tomcat 容器静态和动态 静态处理：DefaultServlet 优化方案：通过移除conf/web.xml中的 DefaultServlet 12345678910111213&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 动态处理：应用Servlet、JspServlet 优化方案一：通过移除conf/web.xml中的 JspServlet 12345678910111213&lt;servlet&gt; &lt;servlet-name&gt;jsp&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.jasper.servlet.JspServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;fork&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;xpoweredBy&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;3&lt;/load-on-startup&gt;&lt;/servlet&gt; Spring Web MVC 应用 Servlet：DispatcherServlet JspServlet：编译并且执行Jsp页面 DefaultServlet：Tomcat处理静态资源的 Servlet 优化方案二：移除&lt;welcome-file-list&gt;元素 12345&lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt; 优化方案三：移除 Session设置，即将session-timeout元素值设置为-1 123&lt;session-config&gt; &lt;session-timeout&gt;30&lt;/session-timeout&gt;&lt;/session-config&gt; 对于微服务/REST应用，不需要 Session，因为不需要状态 Spring Security OAuth 2.0、JWT Session 通过 jsessionId进行用户跟踪，HTTP无状态，需要一个ID与当前用户会话联系。Spring Session HttpSession jessionId 作为 Redis，实现多个机器登录，用户会话不丢失。 存储方式：Cookie、URL重写、SSL 优化方案四：移除 conf\server.xml中的 Valve元素 123&lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; Valve类似于 Filter 移除AccessLogValue，可以通过Nginx的Access替代 场景二：需要JSP的情况 分析：JspServlet无法移除，了解JspServlet处理原理 Servlet 周期： 实例化：Servlet 和 Filter 实现类必须包含默认构造器。反射的方式进行实例化 初始化：Servlet 容器调用 Servlet 或 Filter init() 方法 销毁：Servlet 容器关闭时，Servlet 或者 Filter destory() 方法被调用 Servlet 或者 Filter 在一个容器中，一般情况下在一个 Web App 中是一个单例，不排除应用定义多个。 JspServlet 相关优化参数： 需要编译 compiler modificationTestInterval 不需要编译 development设置为 false development = false ，那么，这些 JSP 编译优化方法： Ant Task 执行 JSP 编译 Maven 插件：GAV如下 1234567&gt; &lt;!-- https://mvnrepository.com/artifact/org.codehaus.mojo.jspc/jspc-maven-plugin --&gt;&gt; &lt;dependency&gt;&gt; &lt;groupId&gt;org.codehaus.mojo.jspc&lt;/groupId&gt;&gt; &lt;artifactId&gt;jspc-maven-plugin&lt;/artifactId&gt;&gt; &lt;version&gt;2.0-alpha-3&lt;/version&gt;&gt; &lt;/dependency&gt;&gt; 总结：conf/web.xml作为 Servlet 应用的默认web.xml，实际上，应用程序存在两份web.xml，其中包括应用的web.xml，最终将两者合并 JspServlet 如果 development 参数为 true，它会自定检查文件是否修改，如果修改重新翻译，再编译(加载和执行)。言外之意，JspServlet 开发模式可能会导致内存溢出(卸载 Class 不及时所导致 Perm 区域不够)。 配置调整关闭自动重载修改conf/server.xml中的&lt;Context&gt;标签中的reloadable属性为false(如果是单独的context.xml文件，则修改该文件中的相关属性值) 1&lt;Context docBase="$&#123;AppAbsolutePath&#125;" reloadable="false"&gt;&lt;/Context&gt; 修改连接线程池数量通过修改conf/server.xml 1234567&lt;Executor name="tomcatThreadPool" namePrefix="catalina-exec-" maxThreads="150" minSpareThreads="4"/&gt;&lt;Connector executor="tomcatThreadPool" port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; 其中&lt;Executor&gt;实际的Tomcat接口 org.apache.catalina.Executor， 扩展了J.U.C标准接口java.util.concurrent.Executor， 实现org.apache.catalina.core.StandardThreadExecutor 线程数量 123456789101112131415161718192021222324/*** max number of threads*/protected int maxThreads = 200;/*** min number of threads*/protected int minSpareThreads = 25; public void setMaxThreads(int maxThreads) &#123; this.maxThreads = maxThreads; if (executor != null) &#123; executor.setMaximumPoolSize(maxThreads); &#125;&#125;public void setMinSpareThreads(int minSpareThreads) &#123; this.minSpareThreads = minSpareThreads; if (executor != null) &#123; executor.setCorePoolSize(minSpareThreads); &#125;&#125; 线程池：org.apache.tomcat.util.threads.ThreadPoolExecutor(java.util.concurrent.ThreadPoolExecutor) 总结：Tomcat IO 连接器使用的线程池实际是标准的 Java 线程池的扩展，最大线程数量和最小线程数量实际上分别是 MaximumPoolSize和 CorePoolSize。]]></content>
      <categories>
        <category>性能优化</category>
      </categories>
      <tags>
        <tag>Tomcat 优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat Maven 插件]]></title>
    <url>%2F2019%2F07%2F07%2FdevNote%2FTomcat8%20Maven%20%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Tomcat7 Maven 插件地址123456&lt;!-- https://mvnrepository.com/artifact/org.apache.tomcat.maven/tomcat7-maven-plugin --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt;&lt;/dependency&gt; Tomcat8 Maven 插件地址123456&lt;!-- https://mvnrepository.com/artifact/org.apache.tomcat.maven/tomcat8-maven-plugin --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat8-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.0-r1756463&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>Tomcat8</category>
      </categories>
      <tags>
        <tag>Tomcat8 Plugin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从实际案例聊聊Java应用的GC优化]]></title>
    <url>%2F2019%2F07%2F06%2F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F%E4%BB%8E%E5%AE%9E%E9%99%85%E6%A1%88%E4%BE%8B%E8%81%8A%E8%81%8AJava%E5%BA%94%E7%94%A8%E7%9A%84GC%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[原文地址：https://tech.meituan.com/2017/12/29/jvm-optimize.html 当Java程序性能达不到既定目标，且其他优化手段都已经穷尽时，通常需要调整垃圾回收器来进一步提高性能，称为GC优化。但GC算法复杂，影响GC性能的参数众多，且参数调整又依赖于应用各自的特点，这些因素很大程度上增加了GC优化的难度。即便如此，GC调优也不是无章可循，仍然有一些通用的思考方法。本篇会介绍这些通用的GC优化策略和相关实践案例，主要包括如下内容： 优化方法: 介绍调优的一般流程：明确优化目标→优化→跟踪优化结果。 &gt; 优化案例: 简述笔者所在团队遇到的GC问题以及优化方案。 一、优化前的准备 GC优化需知为了更好地理解本篇所介绍的内容，你需要了解如下内容。 1. GC相关基础知识，包括但不限于： a) GC工作原理。 b) 理解新生代、老年代、晋升等术语含义。 c) 可以看懂GC日志。 GC优化不能解决一切性能问题，它是最后的调优手段。 如果对第一点中提及的知识点不是很熟悉，可以先阅读小结-JVM基础回顾；如果已经很熟悉，可以跳过该节直接往下阅读。 JVM基础回顾JVM内存结构简单介绍一下JVM内存结构和常见的垃圾回收器。 当代主流虚拟机（Hotspot VM）的垃圾回收都采用“分代回收”的算法。“分代回收”是基于这样一个事实：对象的生命周期不同，所以针对不同生命周期的对象可以采取不同的回收方式，以便提高回收效率。 Hotspot VM将内存划分为不同的物理区，就是“分代”思想的体现。如图所示，JVM内存主要由新生代、老年代、永久代构成。 新生代（Young Generation）：大多数对象在新生代中被创建，其中很多对象的生命周期很短。每次新生代的垃圾回收（又称Minor GC）后只有少量对象存活，所以选用复制算法，只需要少量的复制成本就可以完成回收。 老年代（Old Generation）：在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代，该区域中对象存活率高。老年代的垃圾回收（又称Major GC）通常使用“标记-清理”或“标记-整理”算法。整堆包括新生代和老年代的垃圾回收称为Full GC（HotSpot VM里，除了CMS之外，其它能收集老年代的GC都会同时收集整个GC堆，包括新生代）。 永久代（Perm Generation）：主要存放元数据，例如Class、Method的元信息，与垃圾回收要回收的Java对象关系不大。相对于新生代和年老代来说，该区域的划分对垃圾回收影响比较小。 常见垃圾回收器不同的垃圾回收器，适用于不同的场景。常用的垃圾回收器： 串行（Serial）回收器是单线程的一个回收器，简单、易实现、效率高。 并行（ParNew）回收器是Serial的多线程版，可以充分的利用CPU资源，减少回收的时间。 吞吐量优先（Parallel Scavenge）回收器，侧重于吞吐量的控制。 并发标记清除（CMS，Concurrent Mark Sweep）回收器是一种以获取最短回收停顿时间为目标的回收器，该回收器是基于“标记-清除”算法实现的。 GC日志每一种回收器的日志格式都是由其自身的实现决定的，换而言之，每种回收器的日志格式都可以不一样。但虚拟机设计者为了方便用户阅读，将各个回收器的日志都维持一定的共性。JavaGC日志 中简单介绍了这些共性。 参数基本策略各分区的大小对GC的性能影响很大。如何将各分区调整到合适的大小，分析活跃数据的大小是很好的切入点。 活跃数据的大小是指，应用程序稳定运行时长期存活对象在堆中占用的空间大小，也就是Full GC后堆中老年代占用空间的大小。可以通过GC日志中Full GC之后老年代数据大小得出，比较准确的方法是在程序稳定后，多次获取GC数据，通过取平均值的方式计算活跃数据的大小。活跃数据和各分区之间的比例关系如下（见参考文献1）： 空间 倍数 总大小 3-4 倍活跃数据的大小 新生代 1-1.5 活跃数据的大小 老年代 2-3 倍活跃数据的大小 永久代 1.2-1.5 倍Full GC后的永久代空间占用 例如，根据GC日志获得老年代的活跃数据大小为300M，那么各分区大小可以设为： 总堆：1200MB = 300MB × 4 新生代：450MB = 300MB × 1.5 老年代： 750MB = 1200MB - 450MB* 这部分设置仅仅是堆大小的初始值，后面的优化中，可能会调整这些值，具体情况取决于应用程序的特性和需求。 二、优化步骤GC优化一般步骤可以概括为：确定目标、优化参数、验收结果。 确定目标明确应用程序的系统需求是性能优化的基础，系统的需求是指应用程序运行时某方面的要求，譬如： - 高可用，可用性达到几个9。 - 低延迟，请求必须多少毫秒内完成响应。 - 高吞吐，每秒完成多少次事务。 明确系统需求之所以重要，是因为上述性能指标间可能冲突。比如通常情况下，缩小延迟的代价是降低吞吐量或者消耗更多的内存或者两者同时发生。 由于笔者所在团队主要关注高可用和低延迟两项指标，所以接下来分析，如何量化GC时间和频率对于响应时间和可用性的影响。通过这个量化指标，可以计算出当前GC情况对服务的影响，也能评估出GC优化后对响应时间的收益，这两点对于低延迟服务很重要。 举例：假设单位时间T内发生一次持续25ms的GC，接口平均响应时间为50ms，且请求均匀到达，根据下图所示： 那么有(50ms+25ms)/T比例的请求会受GC影响，其中GC前的50ms内到达的请求都会增加25ms，GC期间的25ms内到达的请求，会增加0-25ms不等，如果时间T内发生N次GC，受GC影响请求占比=(接口响应时间+GC时间)×N/T 。可见无论降低单次GC时间还是降低GC次数N都可以有效减少GC对响应时间的影响。 优化通过收集GC信息，结合系统需求，确定优化方案，例如选用合适的GC回收器、重新设置内存比例、调整JVM参数等。 进行调整后，将不同的优化方案分别应用到多台机器上，然后比较这些机器上GC的性能差异，有针对性的做出选择，再通过不断的试验和观察，找到最合适的参数。 验收优化结果将修改应用到所有服务器，判断优化结果是否符合预期，总结相关经验。 接下来，我们通过三个案例来实践以上的优化流程和基本原则（本文中三个案例使用的垃圾回收器均为ParNew+CMS，CMS失败时Serial Old替补)。 三、GC优化案例案例一 Major GC和Minor GC频繁确定目标服务情况：Minor GC每分钟100次 ，Major GC每4分钟一次，单次Minor GC耗时25ms，单次Major GC耗时200ms，接口响应时间50ms。 由于这个服务要求低延时高可用，结合上文中提到的GC对服务响应时间的影响，计算可知由于Minor GC的发生，12.5%的请求响应时间会增加，其中8.3%的请求响应时间会增加25ms，可见当前GC情况对响应时间影响较大。 （50ms+25ms）× 100次/60000ms = 12.5%，50ms × 100次/60000ms = 8.3% 。 优化目标：降低TP99、TP90时间。 优化首先优化Minor GC频繁问题。通常情况下，由于新生代空间较小，Eden区很快被填满，就会导致频繁Minor GC，因此可以通过增大新生代空间来降低Minor GC的频率。例如在相同的内存分配率的前提下，新生代中的Eden区增加一倍，Minor GC的次数就会减少一半。 这时很多人有这样的疑问，扩容Eden区虽然可以减少Minor GC的次数，但会增加单次Minor GC时间么？根据上面公式，如果单次Minor GC时间也增加，很难保证最后的优化效果。我们结合下面情况来分析，单次Minor GC时间主要受哪些因素影响？是否和新生代大小存在线性关系？ 首先，单次Minor GC时间由以下两部分组成：T1（扫描新生代）和 T2（复制存活对象到Survivor区）如下图。（注：这里为了简化问题，我们认为T1只扫描新生代判断对象是否存活的时间，其实该阶段还需要扫描部分老年代，后面案例中有详细描述。） 扩容前：新生代容量为R ，假设对象A的存活时间为750ms，Minor GC间隔500ms，那么本次Minor GC时间= T1（扫描新生代R）+T2（复制对象A到S）。 扩容后：新生代容量为2R ，对象A的生命周期为750ms，那么Minor GC间隔增加为1000ms，此时Minor GC对象A已不再存活，不需要把它复制到Survivor区，那么本次GC时间 = 2 × T1（扫描新生代R），没有T2复制时间。 可见，扩容后，Minor GC时增加了T1（扫描时间），但省去T2（复制对象）的时间，更重要的是对于虚拟机来说，复制对象的成本要远高于扫描成本，所以，单次Minor GC时间更多取决于GC后存活对象的数量，而非Eden区的大小。因此如果堆中短期对象很多，那么扩容新生代，单次Minor GC时间不会显著增加。下面需要确认下服务中对象的生命周期分布情况： 通过上图GC日志中两处红色框标记内容可知： 1. new threshold = 2（动态年龄判断，对象的晋升年龄阈值为2），对象仅经历2次Minor GC后就晋升到老年代，这样老年代会迅速被填满，直接导致了频繁的Major GC。 2. Major GC后老年代使用空间为300M+，意味着此时绝大多数(86% = 2G/2.3G)的对象已经不再存活，也就是说生命周期长的对象占比很小。 由此可见，服务中存在大量短期临时对象，扩容新生代空间后，Minor GC频率降低，对象在新生代得到充分回收，只有生命周期长的对象才进入老年代。这样老年代增速变慢，Major GC频率自然也会降低。 优化结果通过扩容新生代为为原来的三倍，单次Minor GC时间增加小于5ms，频率下降了60%，服务响应时间TP90，TP99都下降了10ms+，服务可用性得到提升。 调整前： 调整后： 小结如何选择各分区大小应该依赖应用程序中对象生命周期的分布情况：如果应用存在大量的短期对象，应该选择较大的年轻代；如果存在相对较多的持久对象，老年代应该适当增大。 更多思考关于上文中提到晋升年龄阈值为2，很多同学有疑问，为什么设置了MaxTenuringThreshold=15，对象仍然仅经历2次Minor GC，就晋升到老年代？这里涉及到“动态年龄计算”的概念。 动态年龄计算：Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值。在本案例中，调优前：Survivor区 = 64M，desired survivor = 32M，此时Survivor区中age&lt;=2的对象累计大小为41M，41M大于32M，所以晋升年龄阈值被设置为2，下次Minor GC时将年龄超过2的对象被晋升到老年代。 JVM引入动态年龄计算，主要基于如下两点考虑： 如果固定按照MaxTenuringThreshold设定的阈值作为晋升条件： a）MaxTenuringThreshold设置的过大，原本应该晋升的对象一直停留在Survivor区，直到Survivor区溢出，一旦溢出发生，Eden+Svuvivor中对象将不再依据年龄全部提升到老年代，这样对象老化的机制就失效了。 b）MaxTenuringThreshold设置的过小，“过早晋升”即对象不能在新生代充分被回收，大量短期对象被晋升到老年代，老年代空间迅速增长，引起频繁的Major GC。分代回收失去了意义，严重影响GC性能。 相同应用在不同时间的表现不同：特殊任务的执行或者流量成分的变化，都会导致对象的生命周期分布发生波动，那么固定的阈值设定，因为无法动态适应变化，会造成和上面相同的问题。 总结来说，为了更好的适应不同程序的内存情况，虚拟机并不总是要求对象年龄必须达到Maxtenuringthreshhold再晋级老年代。 案例二 请求高峰期发生GC，导致服务可用性下降确定目标GC日志显示，高峰期CMS在重标记（Remark）阶段耗时1.39s。Remark阶段是Stop-The-World（以下简称为STW）的，即在执行垃圾回收时，Java应用程序中除了垃圾回收器线程之外其他所有线程都被挂起，意味着在此期间，用户正常工作的线程全部被暂停下来，这是低延时服务不能接受的。本次优化目标是降低Remark时间。 优化解决问题前，先回顾一下CMS的四个主要阶段，以及各个阶段的工作内容。下图展示了CMS各个阶段可以标记的对象，用不同颜色区分。 1. Init-mark初始标记(STW) ，该阶段进行可达性分析，标记GC ROOT能直接关联到的对象，所以很快。 2. Concurrent-mark并发标记，由前阶段标记过的绿色对象出发，所有可到达的对象都在本阶段中标记。 3. Remark重标记(STW) ，暂停所有用户线程，重新扫描堆中的对象，进行可达性分析，标记活着的对象。因为并发标记阶段是和用户线程并发执行的过程，所以该过程中可能有用户线程修改某些活跃对象的字段，指向了一个未标记过的对象，如下图中红色对象在并发标记开始时不可达，但是并行期间引用发生变化，变为对象可达，这个阶段需要重新标记出此类对象，防止在下一阶段被清理掉，这个过程也是需要STW的。特别需要注意一点，这个阶段是以新生代中对象为根来判断对象是否存活的。 4. 并发清理，进行并发的垃圾清理。 可见，Remark阶段主要是通过扫描堆来判断对象是否存活。那么准确判断对象是否存活，需要扫描哪些对象？CMS对老年代做回收，Remark阶段仅扫描老年代是否可行？结论是不可行，原因如下： 如果仅扫描老年代中对象，即以老年代中对象为根，判断对象是否存在引用，上图中，对象A因为引用存在新生代中，它在Remark阶段就不会被修正标记为可达，GC时会被错误回收。 新生代对象持有老年代中对象的引用，这种情况称为“跨代引用”。因它的存在，Remark阶段必须扫描整个堆来判断对象是否存活，包括图中灰色的不可达对象。 灰色对象已经不可达，但仍然需要扫描的原因：新生代GC和老年代的GC是各自分开独立进行的，只有Minor GC时才会使用根搜索算法，标记新生代对象是否可达，也就是说虽然一些对象已经不可达，但在Minor GC发生前不会被标记为不可达，CMS也无法辨认哪些对象存活，只能全堆扫描（新生代+老年代）。由此可见堆中对象的数目影响了Remark阶段耗时。 分析GC日志可以得出同样的规律，Remark耗时&gt;500ms时，新生代使用率都在75%以上。这样降低Remark阶段耗时问题转换成如何减少新生代对象数量。 新生代中对象的特点是“朝生夕灭”，这样如果Remark前执行一次Minor GC，大部分对象就会被回收。CMS就采用了这样的方式，在Remark前增加了一个可中断的并发预清理（CMS-concurrent-abortable-preclean），该阶段主要工作仍然是并发标记对象是否存活，只是这个过程可被中断。此阶段在Eden区使用超过2M时启动，当然2M是默认的阈值，可以通过参数修改。如果此阶段执行时等到了Minor GC，那么上述灰色对象将被回收，Reamark阶段需要扫描的对象就少了。 除此之外CMS为了避免这个阶段没有等到Minor GC而陷入无限等待，提供了参数CMSMaxAbortablePrecleanTime ，默认为5s，含义是如果可中断的预清理执行超过5s，不管发没发生Minor GC，都会中止此阶段，进入Remark。 根据GC日志红色标记2处显示，可中断的并发预清理执行了5.35s，超过了设置的5s被中断，期间没有等到Minor GC ，所以Remark时新生代中仍然有很多对象。 对于这种情况，CMS提供CMSScavengeBeforeRemark参数，用来保证Remark前强制进行一次Minor GC。 优化结果经过增加CMSScavengeBeforeRemark参数，单次执行时间&gt;200ms的GC停顿消失，从监控上观察，GCtime和业务波动保持一致，不再有明显的毛刺。 小结通过案例分析了解到，由于跨代引用的存在，CMS在Remark阶段必须扫描整个堆，同时为了避免扫描时新生代有很多对象，增加了可中断的预清理阶段用来等待Minor GC的发生。只是该阶段有时间限制，如果超时等不到Minor GC，Remark时新生代仍然有很多对象，我们的调优策略是，通过参数强制Remark前进行一次Minor GC，从而降低Remark阶段的时间。 更多思考案例中只涉及老年代GC，其实新生代GC存在同样的问题，即老年代可能持有新生代对象引用，所以Minor GC时也必须扫描老年代。 JVM是如何避免Minor GC时扫描全堆的？ 经过统计信息显示，老年代持有新生代对象引用的情况不足1%，根据这一特性JVM引入了卡表（card table）来实现这一目的。如下图所示： 卡表的具体策略是将老年代的空间分成大小为512B的若干张卡（card）。卡表本身是单字节数组，数组中的每个元素对应着一张卡，当发生老年代引用新生代时，虚拟机将该卡对应的卡表元素设置为适当的值。如上图所示，卡表3被标记为脏（卡表还有另外的作用，标识并发标记阶段哪些块被修改过），之后Minor GC时通过扫描卡表就可以很快的识别哪些卡中存在老年代指向新生代的引用。这样虚拟机通过空间换时间的方式，避免了全堆扫描。 总结来说，CMS的设计聚焦在获取最短的时延，为此它“不遗余力”地做了很多工作，包括尽量让应用程序和GC线程并发、增加可中断的并发预清理阶段、引入卡表等，虽然这些操作牺牲了一定吞吐量但获得了更短的回收停顿时间。 案例三 发生Stop-The-World的GC确定目标GC日志如下图（在GC日志中，Full GC是用来说明这次垃圾回收的停顿类型，代表STW类型的GC，并不特指老年代GC），根据GC日志可知本次Full GC耗时1.23s。这个在线服务同样要求低时延高可用。本次优化目标是降低单次STW回收停顿时间，提高可用性。 优化首先，什么时候可能会触发STW的Full GC呢？ 1. Perm空间不足； 2. CMS GC时出现promotion failed和concurrent mode failure（concurrent mode failure发生的原因一般是CMS正在进行，但是由于老年代空间不足，需要尽快回收老年代里面的不再被使用的对象，这时停止所有的线程，同时终止CMS，直接进行Serial Old GC）； 3. 统计得到的Young GC晋升到老年代的平均大小大于老年代的剩余空间； 4. 主动触发Full GC（执行jmap -histo:live [pid]）来避免碎片问题。 然后，我们来逐一分析一下： - 排除原因2：如果是原因2中两种情况，日志中会有特殊标识，目前没有。 - 排除原因3：根据GC日志，当时老年代使用量仅为20%，也不存在大于2G的大对象产生。 - 排除原因4：因为当时没有相关命令执行。 - 锁定原因1：根据日志发现Full GC后，Perm区变大了，推断是由于永久代空间不足容量扩展导致的。 找到原因后解决方法有两种： 1. 通过把-XX:PermSize参数和-XX:MaxPermSize设置成一样，强制虚拟机在启动的时候就把永久代的容量固定下来，避免运行时自动扩容。 2. CMS默认情况下不会回收Perm区，通过参数CMSPermGenSweepingEnabled、CMSClassUnloadingEnabled ，可以让CMS在Perm区容量不足时对其回收。 由于该服务没有生成大量动态类，回收Perm区收益不大，所以我们采用方案1，启动时将Perm区大小固定，避免进行动态扩容。 优化结果调整参数后，服务不再有Perm区扩容导致的STW GC发生。 小结对于性能要求很高的服务，建议将MaxPermSize和MinPermSize设置成一致（JDK8开始，Perm区完全消失，转而使用元空间。而元空间是直接存在内存中，不在JVM中），Xms和Xmx也设置为相同，这样可以减少内存自动扩容和收缩带来的性能损失。虚拟机启动的时候就会把参数中所设定的内存全部化为私有，即使扩容前有一部分内存不会被用户代码用到，这部分内存在虚拟机中被标识为虚拟内存，也不会交给其他进程使用。 四、总结结合上述GC优化案例做个总结： 1. 首先再次声明，在进行GC优化之前，需要确认项目的架构和代码等已经没有优化空间。我们不能指望一个系统架构有缺陷或者代码层次优化没有穷尽的应用，通过GC优化令其性能达到一个质的飞跃。 2. 其次，通过上述分析，可以看出虚拟机内部已有很多优化来保证应用的稳定运行，所以不要为了调优而调优，不当的调优可能适得其反。 3. 最后，GC优化是一个系统而复杂的工作，没有万能的调优策略可以满足所有的性能指标。GC优化必须建立在我们深入理解各种垃圾回收器的基础上，才能有事半功倍的效果。 本文中案例均来北京业务安全中心（也称风控）对接服务的实践经验。同时感谢风控的小伙伴们，是他们专业负责的审阅，才让这篇文章更加完善。对于本文中涉及到的内容，欢迎大家指正和补充。]]></content>
      <categories>
        <category>性能优化</category>
      </categories>
      <tags>
        <tag>GC优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(七)使用MAT的Histogram和Dominator Tree定位溢出源]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E4%B8%83)%E4%BD%BF%E7%94%A8MAT%E7%9A%84Histogram%E5%92%8CDominator%20Tree%E5%AE%9A%E4%BD%8D%E6%BA%A2%E5%87%BA%E6%BA%90%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com 基础概念先列出几个基础的概念： Shallow Heap 和 Retained HeapShallow Heap表示对象本身占用内存的大小，不包含对其他对象的引用，也就是对象头加成员变量（不是成员变量的值）的总和。 Retained Heap是该对象自己的Shallow Heap，并加上从该对象能直接或间接访问到对象的Shallow Heap之和。换句话说，Retained Heap是该对象GC之后所能回收到内存的总和。 把内存中的对象看成下图中的节点，并且对象和对象之间互相引用。这里有一个特殊的节点GC Roots，这就是reference chain的起点。 从obj1入手，上图中蓝色节点代表仅仅只有通过obj1才能直接或间接访问的对象。因为可以通过GC Roots访问，所以左图的obj3不是蓝色节点；而在右图却是蓝色，因为它已经被包含在retained集合内。所以对于左图，obj1的retained size是obj1、obj2、obj4的shallow size总和；右图的retained size是obj1、obj2、obj3、obj4的shallow size总和。obj2的retained size可以通过相同的方式计算。 对象引用（Reference）对象引用按从最强到最弱有如下级别，不同的引用（可到达性）级别反映了对象的生命周期： 强引用（Strong Ref）：通常我们编写的代码都是强引用，于此相对应的是强可达性，只有去掉强可达性，对象才能被回收。 软引用（Soft Ref）：对应软可达性，只要有足够的内存就一直保持对象，直到发现内存不足且没有强引用的时候才回收对象。 弱引用（Weak Ref）：比软引用更弱，当发现不存在强引用的时候会立即回收此类型的对象，而不需要等到内存不足。通过java.lang.ref.WeakReference和java.util.WeakHashMap类实现。 虚引用（Phantom Ref）：根本不会在内存中保持该类型的对象，只能使用虚引用本身，一般用于在进入finalize()方法后进行特殊的清理过程，通过java.lang.ref.PhantomReference实现。 GC Roots和Reference ChainJVM在进行GC的时候是通过使用可达性来判断对象是否存活，通过GC Roots（GC根节点）的对象作为起始点，从这些节点开始进行向下搜索，搜索所走过的路径成为Reference Chain（引用链），当一个对象到GC Roots没有任何引用链相连（用图论的话来说就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。 如下图所示，对象object 5、object 6、object 7虽然互相有关联，它们的引用并不为0，但是它们到GC Roots是不可达的，因此它们将会被判定为是可回收的对象。 Histogram（直方图）视图点击工具栏上的 Histogram图标可以打开 Histogram（直方图）视图，可以列出每个类产生的实例数量，以及所占用的内存大小和百分比。主界面如下图所示： 图中Shallow Heap 和 Retained Heap分别表示对象自身不包含引用的大小和对象自身并包含引用的大小，具体请参考下面 Shallow Heap 和 Retained Heap 部分的内容。默认的大小单位是 Bytes，可以在 Window - Preferences 菜单中设置单位，图中设置的是KB。 通过直方图视图可以很容易找到占用内存最多的几个类（通过Retained Heap排序），还可以通过其他方式进行分组（见下图）。 如果存在内存溢出，时间久了溢出类的实例数量或者内存占比会越来越多，排名也越来越靠前。可以点击工具类上的右上角图标进行对比，通过多次对比不同时间点下的直方图对比就很容易把溢出的类找出来。 还有一种对比直方图的方式，首先通过 Window 菜单打开 Navigation History 视图，选中直方图右键并选中 Add to Compare Basket项目，将直方图添加到 Compare Basket 中。 然后在 Compare Basket 中点击右上角的 红色感叹号 按钮，可以分别列出对比的所有结果，见下图： 并且在上面的可以设置不同的对比方式。 Dominator Tree视图点击工具栏上的Dominator Tree图标可以打开Dominator Tree（支配树）视图，在此视图中列出了每个对象（Object Instance）与其引用关系的树状结构，同时包含了占用内存的大小和百分比。 通过Dominator Tree视图可以很容易的找出占用内存最多的几个对象（根据Retained Heap或Percentage排序），和Histogram类似，可以通过不同的方式进行分组显示： 定位溢出源Histogram视图和Dominator Tree视图的角度不同，前者是基于类的角度，后者是基于对象实例的角度，并且可以更方便的看出其引用关系。 首先，在两个视图中找出疑似溢出的对象或者类（可以通过Retained Heap排序，并且可以在Class Name中输入正则表达式的关键词只显示指定的类名），然后右键选择Path To GC Roots（Histogram中没有此项）或Merge Shortest Paths to GC Roots，然后选择 exclude all phantom/weak/soft etc. reference： GC Roots意为GC根节点，其含义见上面的 GC Roots和Reference Chain 部分，后面的 exclude all phantom/weak/soft etc. reference 意思是排除虚引用、弱引用和软引用，即只剩下强引用，因为除了强引用之外，其他的引用都可以被JVM GC掉，如果一个对象始终无法被GC，就说明有强引用存在，从而导致在GC的过程中一直得不到回收，最终就内存溢出了。 通过结果就可以很方便的定位到具体的代码，然后分析是什么原因无法释放该对象，比如被缓存了或者没有使用单例模式等等。 下面是执行的结果： 上图中保留了大量的VelocitySqlBulder的外部引用，后来查看了代码，原来每次调用的时候都实例化一个新的对象，由于VelocitySqlBulder类是无状态的工具类，因此修改为单例方式就可以解决这个问题。 后续观察根据上面分析的结果对问题进行处理之后，再对照之前的操作，看看对象是否还再持续增长，如果没有就说明这个地方的问题已经解决了。 最后再用 jstat 持续跟踪一段时间，看看Old和Perm区的内存是否最终稳定在一个范围之内，如果长时间稳定在一个范围说明溢出问题得到了解决，否则还要继续进行分析和处理，一直到稳定为止。]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(六)JVM Heap Dump（堆转储文件）的生成和MAT的使用]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E5%85%AD)JVM%20Heap%20Dump%EF%BC%88%E5%A0%86%E8%BD%AC%E5%82%A8%E6%96%87%E4%BB%B6%EF%BC%89%E7%9A%84%E7%94%9F%E6%88%90%E5%92%8CMAT%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com JVM Heap Dump（堆转储文件）的生成正如Thread Dump文件记录了当时JVM中线程运行的情况一样，Heap Dump记录了JVM中堆内存运行的情况。可以通过以下几种方式生成Heap Dump文件： 使用 jmap 命令生成jmap 命令是JDK提供的用于生成堆内存信息的工具，可以执行下面的命令生成Heap Dump： 1jmap -dump:live,format=b,file=heap-dump.bin &lt;pid&gt; 其中的pid是JVM进程的id，heap-dump.bin是生成的文件名称，在执行命令的目录下面。推荐此种方法。 使用 JConsole 生成JConsole是JDK提供的一个基于GUI查看JVM系统信息的工具，既可以管理本地的JVM，也可以管理远程的JVM，可以通过下图的 dumpHeap 按钮生成 Heap Dump文件。 在JVM中增加参数生成在JVM的配置参数中可以添加 -XX:+HeapDumpOnOutOfMemoryError 参数，当应用抛出 OutOfMemoryError 时自动生成dump文件；在JVM的配置参数中添加 -Xrunhprof:head=site 参数，会生成java.hprof.txt 文件，不过这样会影响JVM的运行效率，不建议在生产环境中使用（未亲测）。 常见的Heap Dump文件分析工具JVM Heap Dump文件可以使用常用的分析工具如下： jhatjhat 是JDK自带的用于分析JVM Heap Dump文件的工具，使用下面的命令可以将堆文件的分析结果以HTML网页的形式进行展示： 1jhat &lt;heap-dump-file&gt; 其中 heap-dump-file 是文件的路径和文件名，可以使用 -J-Xmx512m 参数设置命令的内存大小。执行成功之后显示如下结果： Snapshot resolved.Started HTTP server on port 7000Server is ready. 这个时候访问 http://localhost:7000/ 就可以看到结果了。 Eclipse Memory Analyzer(MAT)Eclipse Memory Analyzer(MAT)是Eclipse提供的一款用于Heap Dump文件的工具，操作简单明了，下面将详细进行介绍。 IBM Heap AnalyzerIBM Heap Analyzer 是IBM公司推出的一款用于分析Heap Dump信息的工具，下载之后是一个jar文件，执行结果如下： Memory Analyzer的安装和使用如前文所述，Eclipse Memory Analyzer（简称MAT）是一个功能丰富且操作简单的JVM Heap Dump分析工具，可以用来辅助发现内存泄漏减少内存占用。使用 Memory Analyzer 来分析生产环境的 Java 堆转储文件，可以从数以百万计的对象中快速计算出对象的 Retained Size，查看是谁在阻止垃圾回收，并自动生成一个 Leak Suspect（内存泄露可疑点）报表。 下载与安装Eclipse Memory Analyzer（MAT）支持两种安装方式，一是Eclipse插件的方式，另外一个就是独立运行的方式，建议使用独立运行的方式。在 http://www.eclipse.org/mat/downloads.php 下载安装MAT，启动之后打开 File - Open Heap Dump… 菜单，然后选择生成的Heap DUmp文件，选择 “Leak Suspects Report”，然后点击 “Finish” 按钮。 主界面第一次打开因为需要分析dump文件，所以需要等待一段时间进行分析，分析完成之后dump文件目录下面的文件信息如下： 上图中 heap-27311.bin 文件是原始的Heap Dump文件，zip文件是生成的html形式的报告文件。 打开之后，主界面如下所示： 接下来介绍界面中常用到的功能： OverviewOverview视图，即概要界面，显示了概要的信息，并展示了MAT常用的一些功能。 Details 显示了一些统计信息，包括整个堆内存的大小、类（Class）的数量、对象（Object）的数量、类加载器（Class Loader)的数量。 Biggest Objects by Retained Size 使用饼图的方式直观地显示了在JVM堆内存中最大的几个对象，当光标移到饼图上的时候会在左边Inspector和Attributes窗口中显示详细的信息。 Actions 这里显示了几种常用到的操作，算是功能的快捷方式，包括 Histogram、Dominator Tree、Top Consumers、Duplicate Classes，具体的含义和用法见下面； Reports 列出了常用的报告信息，包括 Leak Suspects和Top Components，具体的含义和内容见下； Step By Step 以向导的方式引导使用功能。 Histogram直方图，可以查看每个类的实例（即对象）的数量和大小。 Dominator Tree支配树，列出Heap Dump中处于活跃状态中的最大的几个对象，默认按 retained size进行排序，因此很容易找到占用内存最多的对象。 OQLMAT提供了一个对象查询语言（OQL），跟SQL语言类似，将类当作表、对象当作记录行、成员变量当作表中的字段。通过OQL可以方便快捷的查询一些需要的信息，是一个非常有用的工具。 Thread Overview此工具可以查看生成Heap Dump文件的时候线程的运行情况，用于线程的分析。 Run Expert System Test可以查看分析完成的HTML形式的报告，也可以打开已经产生的分析报告文件，子菜单项如下图所示： 常用的主要有Leak Suspects和Top Components两种报告： Leak Suspects 可以说是非常常用的报告了，该报告分析了 Heap Dump并尝试找出内存泄漏点，最后在生成的报告中对检测到的可疑点做了详细的说明； Top Components 列出占用总堆内存超过1%的对象。 Open Query Browser提供了在分析过程中用到的工具，通常都集成在了右键菜单中，在后面具体举例分析的时候会做详细的说明。如下图： 这里仅针对在 Overview 界面中的 Acations中列出的两项进行说明： Top Consumers 按类、类加载器和包分别进行查询，并以饼图的方式列出最大的几个对象。菜单打开方式如下： Duplicate Classes 列出被加载多次的类，结果按类加载器进行分组，目标是加载同一个类多次被类加载器加载。使用该工具很容易找到部署应用的时候使用了同一个库的多个版本。菜单打开方式如下图： Find Object by address通过十六进制的地址查找对应的对象，见下图：]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(五)常见的Thread Dump日志案例分析]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E4%BA%94)%E5%B8%B8%E8%A7%81%E7%9A%84Thread%20Dump%E6%97%A5%E5%BF%97%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com 症状及解决方案下面列出几种常见的症状即对应的解决方案： CPU占用率很高，响应很慢按照《Java内存泄漏分析系列之一：使用jstack定位线程堆栈信息》中所说的方法，先找到占用CPU的进程，然后再定位到对应的线程，最后分析出对应的堆栈信息。在同一时间多次使用上述的方法，然后进行对比分析，从代码中找到问题所在的原因。如果线程指向的是”VM Thread”或者无法从代码中直接找到原因，就需要进行内存分析，具体的见下一篇文章。 CPU占用率不高，但响应很慢在整个请求的过程中多次执行Thread Dump然后进行对比，取得 BLOCKED 状态的线程列表，通常是因为线程停在了I/O、数据库连接或网络连接的地方。 关注点概况在Thread Dump文件中，线程的状态分成两种：Native Thread Status和JVM Thread Status，具体的含义可以参考上一篇文章。在分析日志的时候需要重点关注如下几种线程状态： 系统线程状态为 deadlock线程处于死锁状态，将占用系统大量资源。 系统线程状态为 waiting for monitor entry 或 in Object.wait()如上一篇文章中所说，系统线程处于这种状态说明它在等待进入一个临界区，此时JVM线程的状态通常都是 java.lang.Thread.State: BLOCKED。 如果大量线程处于这种状态的话，可能是一个全局锁阻塞了大量线程。如果短期内多次打印Thread Dump信息，发现 waiting for monitor entry 状态的线程越来越多，没有减少的趋势，可能意味着某些线程在临界区里呆得时间太长了，以至于越来越多新线程迟迟无法进入。 系统线程状态为 waiting on condition系统线程处于此种状态说明它在等待另一个条件的发生来唤醒自己，或者自己调用了sleep()方法。此时JVM线程的状态通常是java.lang.Thread.State: WAITING (parking)（等待唤醒条件）或java.lang.Thread.State: TIMED_WAITING (parking或sleeping)（等待定时唤醒条件）。 如果大量线程处于此种状态，说明这些线程又去获取第三方资源了，比如第三方的网络资源或读取数据库的操作，长时间无法获得响应，导致大量线程进入等待状态。因此，这说明系统处于一个网络瓶颈或读取数据库操作时间太长。 系统线程状态为 blocked线程处于阻塞状态，需要根据实际情况进行判断。 案例分析下面通过几个案例进行分解来获得解决问题的方法。 waiting for monitor entry 和 java.lang.Thread.State: BLOCKED1234567891011121314151617181920212223&quot;DB-Processor-13&quot; daemon prio=5 tid=0x003edf98 nid=0xca waiting for monitor entry [0x000000000825f000]java.lang.Thread.State: BLOCKED (on object monitor) at beans.ConnectionPool.getConnection(ConnectionPool.java:102) - waiting to lock &lt;0xe0375410&gt; (a beans.ConnectionPool) at beans.cus.ServiceCnt.getTodayCount(ServiceCnt.java:111) at beans.cus.ServiceCnt.insertCount(ServiceCnt.java:43)&quot;DB-Processor-14&quot; daemon prio=5 tid=0x003edf98 nid=0xca waiting for monitor entry [0x000000000825f020]java.lang.Thread.State: BLOCKED (on object monitor) at beans.ConnectionPool.getConnection(ConnectionPool.java:102) - waiting to lock &lt;0xe0375410&gt; (a beans.ConnectionPool) at beans.cus.ServiceCnt.getTodayCount(ServiceCnt.java:111) at beans.cus.ServiceCnt.insertCount(ServiceCnt.java:43)&quot;DB-Processor-3&quot; daemon prio=5 tid=0x00928248 nid=0x8b waiting for monitor entry [0x000000000825d080]java.lang.Thread.State: RUNNABLE at oracle.jdbc.driver.OracleConnection.isClosed(OracleConnection.java:570) - waiting to lock &lt;0xe03ba2e0&gt; (a oracle.jdbc.driver.OracleConnection) at beans.ConnectionPool.getConnection(ConnectionPool.java:112) - locked &lt;0xe0386580&gt; (a java.util.Vector) - locked &lt;0xe0375410&gt; (a beans.ConnectionPool) at beans.cus.Cue_1700c.GetNationList(Cue_1700c.java:66) at org.apache.jsp.cue_1700c_jsp._jspService(cue_1700c_jsp.java:120) 上面系统线程的状态是 waiting for monitor entry，说明此线程通过 synchronized(obj) { } 申请进入临界区，但obj对应的 Monitor 被其他线程所拥有，所以 JVM线程的状态是 java.lang.Thread.State: BLOCKED (on object monitor)，说明线程等待资源超时。 下面的 waiting to lock &lt;0xe0375410&gt; 说明线程在等待给 0xe0375410 这个地址上锁（trying to obtain 0xe0375410 lock），如果在日志中发现有大量的线程都在等待给 0xe0375410 上锁的话，这个时候需要在日志中查找那个线程获取了这个锁 locked &lt;0xe0375410&gt;，如上面的例子中是 “DB-Processor-14” 这个线程，这样就可以顺藤摸瓜了。上面的例子是因为获取数据库操作等待的时间太长所致的，这个时候就需要修改数据库连接的配置信息。 如果两个线程相互都被对方的线程锁锁住，这样就造成了 死锁 现象，如下面的例子所示： waiting on condition 和 java.lang.Thread.State: TIMED_WAITING1234567891011&quot;RMI TCP Connection(idle)&quot; daemon prio=10 tid=0x00007fd50834e800 nid=0x56b2 waiting on condition [0x00007fd4f1a59000]java.lang.Thread.State: TIMED_WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000000acd84de8&gt; (a java.util.concurrent.SynchronousQueue$TransferStack) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198) at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424) at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323) at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:945) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907) at java.lang.Thread.run(Thread.java:662) JVM线程的状态是 java.lang.Thread.State: TIMED_WAITING (parking)，说明线程处于定时等待的状态，parking指线程处于挂起中。 waiting on condition需要结合堆栈中的 parking to wait for (a java.util.concurrent.SynchronousQueue$TransferStack) 一起来分析。首先，本线程肯定是在等待某个条件的发生来把自己唤醒。其次，SynchronousQueue并不是一个队列，只是线程之间移交信息的机制，当我们把一个元素放入到 SynchronousQueue 中的时候必须有另一个线程正在等待接受移交的任务，因此这就是本线程在等待的条件。 in Object.wait() 和 java.lang.Thread.State: TIMED_WAITING12345678&quot;RMI RenewClean-[172.16.5.19:28475]&quot; daemon prio=10 tid=0x0000000041428800 nid=0xb09 in Object.wait() [0x00007f34f4bd0000]java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000aa672478&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118) - locked &lt;0x00000000aa672478&gt; (a java.lang.ref.ReferenceQueue$Lock) at sun.rmi.transport.DGCClient$EndpointEntry$RenewCleanThread.run(DGCClient.java:516) at java.lang.Thread.run(Thread.java:662) 本例中JVM线程的状态是 java.lang.Thread.State: TIMED_WAITING (on object monitor)，说明线程调用了 java.lang.Object.wait(long timeout) 方法而进入了等待状态。 “Wait Set”中等待的线程状态就是 in Object.wait()，当线程获得了 Monitor进入临界区之后，如果发现线程继续运行的条件没有满足，它就调用对象（通常是被 synchronized 的对象）的wait()方法，放弃了Monitor，进入 “Wait Set” 队列中。只有当别的线程在该对象上调用了 notify()或notifyAll()方法， “Wait Set” 队列中线程才得到机会去竞争，但是只有一个线程获得对象的 Monitor，恢复到的运行态。 另外需要注意的是，是先 locked 然后再 waiting on ，之所以如此，可以通过下面的代码进行演示： 123456789101112static private class Lock &#123; &#125;;private Lock lock = new Lock();public Reference&lt;? extends T&gt; remove(long timeout) &#123; synchronized (lock) &#123; Reference&lt;? extends T&gt; r = reallyPoll(); if (r != null) return r; for (;;) &#123; lock.wait(timeout); r = reallyPoll(); // …… &#125;&#125; 线程在执行的过程中，先用 synchronized 获得了这个对象的 Monitor（对应 locked ），当执行到 lock.wait(timeout); 的时候，线程就放弃了Monitor的所有权，进入 “Wait Set” 队列（对应 waiting on ）。 前面几篇文章详细说明了如何分析Thread Dump文件，除此之外还可以通过分析JVM堆内存信息来进一步找到问题的原因。]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(三)jstat命令的使用及VM Thread分析]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E4%B8%89)jstat%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8AVM%20Thread%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com 使用jstat命令当服务器CPU100%的时候，通过定位占用资源最大的线程定位到 VM Thread： 1&quot;VM Thread&quot; prio=10 tid=0x00007fbea80d3800 nid=0x5e9 runnable 这个时候需要使用 jstat -gc &lt;pid&gt; &lt;period&gt; &lt;times&gt; 命令查看gc的信息，显示结果如下： 12S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT64.0 64.0 0.0 0.0 332992.0 0.0 666304.0 73192.5 83968.0 83967.9 6893 17.576 6882 2705.923 2723.499 结果中每个项目的含义可以参考官方对jstat的文档，简单翻译如下： S0C: Young Generation第一个survivor space的内存大小 (kB). S1C: Young Generation第二个survivor space的内存大小 (kB). S0U: Young Generation第一个Survivor space当前已使用的内存大小 (kB). S1U: Young Generation第二个Survivor space当前已经使用的内存大小 (kB). EC: Young Generation中eden space的内存大小 (kB). EU: Young Generation中Eden space当前已使用的内存大小 (kB). OC: Old Generation的内存大小 (kB). OU: Old Generation当前已使用的内存大小 (kB). PC: Permanent Generation的内存大小 (kB) PU: Permanent Generation当前已使用的内存大小 (kB). YGC: 从启动到采样时Young Generation GC的次数 YGCT: 从启动到采样时Young Generation GC所用的时间 (s). FGC: 从启动到采样时Old Generation GC的次数. FGCT: 从启动到采样时Old Generation GC所用的时间 (s). GCT: 从启动到采样时GC所用的总时间 (s). JDK8的结果稍微有所不同，结果含义可以参考：http://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstat.html。 JVM内存模型上面中的Young Generation、Permanent Generation和Old Generation等概念有一些混乱，这里简要的进行说明。简单来说，JVM内存由堆（Heap）和非堆（Non-heap）内存组成，前者共运行在JVM之上的程序使用，后者供JVM自己使用。 堆内存的组成如下： 非堆内存由 Permanent Generation 和 Code Cache 两部分组成： Permanent Generation（持久代）: 保存虚拟机自己的静态(refective)数据，主要存放加载的Class类级别静态对象如class本身，method，field等等。permanent generation空间不足会引发full GC； Code Cache: 用于编译和保存本地代码（native code）的内存，JVM内部处理或优化。 JVM内存参数设置堆内存设置 堆内存（总的）由 -Xms 和 -Xmx 分别设置最小和最大堆内存 New Generation 由 -Xmn 设置，-XX:SurvivorRatio=m 设置 Eden和 两个Survivor区的大小比值；-XX:NewRatio=n 设置 New Generation 和 Old Generation 的大小比值。 每个线程的堆栈大小由 ·-Xss· 设置，JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 非堆内存设置非堆内存由 -XX:PermSize=n 和 -XX:MaxPermSize=n 分别设置最小和最大非堆内存大小 日志分析介绍完上面的概念之后，我们再来看最上面的日志信息，有两个地方有问题：一是FGC（完全GC）的数量太大了，正常来说FGC应该占整个GC（YGC+FGC）的1%到5%才正常，上面日志上完全GC的次数太多了；二是日志中PU的值太大了，基本上已经达到设置的PC了，因此需要增大MaxPermSize的值。不过这只是权宜之计，出现这么大的非堆内存，肯定什么地方出现了问题，还需要进一步找到占用内存的原因，这也是后面文章所要说的。]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(二)jstack生成的Thread Dump日志结构解析]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E4%BA%8C)jstack%E7%94%9F%E6%88%90%E7%9A%84Thread%20Dump%E6%97%A5%E5%BF%97%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com 首先对Thread Dump日志文件的结构进行分析。一个典型的thread dump文件主要由一下几个部分组成： 上图将JVM上的线程堆栈信息和线程信息做了详细的拆解。 第一部分：Full thread dump identifier这一部分是内容最开始的部分，展示了快照文件的生成时间和JVM的版本信息。 122017-10-19 10:46:44Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.79-b02 mixed mode): 第二部分：Java EE middleware, third party &amp; custom application Threads这是整个文件的核心部分，里面展示了JavaEE容器（如tomcat、resin等）、自己的程序中所使用的线程信息。这一部分详细的含义见 Java内存泄漏分析系列之四：jstack生成的Thread Dump日志线程状态分析。 1234567&quot;resin-22129&quot; daemon prio=10 tid=0x00007fbe5c34e000 nid=0x4cb1 waiting on condition [0x00007fbe4ff7c000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:315) at com.caucho.env.thread2.ResinThread2.park(ResinThread2.java:196) at com.caucho.env.thread2.ResinThread2.runTasks(ResinThread2.java:147) at com.caucho.env.thread2.ResinThread2.run(ResinThread2.java:118) 第三部分：HotSpot VM Thread这一部分展示了JVM内部线程的信息，用于执行内部的原生操作。下面常见的集中内置线程： “Attach Listener”该线程负责接收外部命令，执行该命令并把结果返回给调用者，此种类型的线程通常在桌面程序中出现。 12&quot;Attach Listener&quot; daemon prio=5 tid=0x00007fc6b6800800 nid=0x3b07 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “DestroyJavaVM”执行main()的线程在执行完之后调用JNI中的 jni_DestroyJavaVM() 方法会唤起DestroyJavaVM 线程。在JBoss启动之后，也会唤起DestroyJavaVM线程，处于等待状态，等待其它线程（java线程和native线程）退出时通知它卸载JVM。 12&quot;DestroyJavaVM&quot; prio=5 tid=0x00007fc6b3001000 nid=0x1903 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Service Thread”用于启动服务的线程 12&quot;Service Thread&quot; daemon prio=10 tid=0x00007fbea81b3000 nid=0x5f2 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE “CompilerThread”用来调用JITing，实时编译装卸CLASS。通常JVM会启动多个线程来处理这部分工作，线程名称后面的数字也会累加，比如CompilerThread1。 12345&quot;C2 CompilerThread1&quot; daemon prio=10 tid=0x00007fbea814b000 nid=0x5f1 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE&quot;C2 CompilerThread0&quot; daemon prio=10 tid=0x00007fbea8142000 nid=0x5f0 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Signal Dispatcher”Attach Listener线程的职责是接收外部jvm命令，当命令接收成功后，会交给signal dispather 线程去进行分发到各个不同的模块处理命令，并且返回处理结果。signal dispather线程也是在第一次接收外部jvm命令时，进行初始化工作。 12&quot;Signal Dispatcher&quot; daemon prio=10 tid=0x00007fbea81bf800 nid=0x5ef runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Finalizer”这个线程也是在main线程之后创建的，其优先级为10，主要用于在垃圾收集前，调用对象的finalize()方法；关于Finalizer线程的几点：（1）只有当开始一轮垃圾收集时，才会开始调用finalize()方法；因此并不是所有对象的finalize()方法都会被执行；（2）该线程也是daemon线程，因此如果虚拟机中没有其他非daemon线程，不管该线程有没有执行完finalize()方法，JVM也会退出；（3）JVM在垃圾收集时会将失去引用的对象包装成Finalizer对象（Reference的实现），并放入ReferenceQueue，由Finalizer线程来处理；最后将该Finalizer对象的引用置为null，由垃圾收集器来回收；（4）JVM为什么要单独用一个线程来执行finalize()方法呢？如果JVM的垃圾收集线程自己来做，很有可能由于在finalize()方法中误操作导致GC线程停止或不可控，这对GC线程来说是一种灾难。 1234567&quot;Finalizer&quot; daemon prio=10 tid=0x00007fbea80da000 nid=0x5eb in Object.wait() [0x00007fbeac044000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135) - locked &lt;0x00000006d173c1a8&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209) “Reference Handler”JVM在创建main线程后就创建Reference Handler线程，其优先级最高，为10，它主要用于处理引用对象本身（软引用、弱引用、虚引用）的垃圾回收问题 。 123456&quot;Reference Handler&quot; daemon prio=10 tid=0x00007fbea80d8000 nid=0x5ea in Object.wait() [0x00007fbeac085000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:503) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133) - locked &lt;0x00000006d173c1f0&gt; (a java.lang.ref.Reference$Lock) “VM Thread”JVM中线程的母体，根据HotSpot源码中关于vmThread.hpp里面的注释，它是一个单例的对象（最原始的线程）会产生或触发所有其他的线程，这个单例的VM线程是会被其他线程所使用来做一些VM操作（如清扫垃圾等）。在 VM Thread 的结构体里有一个VMOperationQueue列队，所有的VM线程操作(vm_operation)都会被保存到这个列队当中，VMThread 本身就是一个线程，它的线程负责执行一个自轮询的loop函数(具体可以参考：VMThread.cpp里面的void VMThread::loop()) ，该loop函数从VMOperationQueue列队中按照优先级取出当前需要执行的操作对象(VM_Operation)，并且调用VM_Operation-&gt;evaluate函数去执行该操作类型本身的业务逻辑。VM操作类型被定义在vm_operations.hpp文件内，列举几个：ThreadStop、ThreadDump、PrintThreads、GenCollectFull、GenCollectFullConcurrent、CMS_Initial_Mark、CMS_Final_Remark….. 有兴趣的同学，可以自己去查看源文件。 1&quot;VM Thread&quot; prio=10 tid=0x00007fbea80d3800 nid=0x5e9 runnable 第四部分：HotSpot GC ThreadJVM中用于进行资源回收的线程，包括以下几种类型的线程： “VM Periodic Task Thread”该线程是JVM周期性任务调度的线程，它由WatcherThread创建，是一个单例对象。该线程在JVM内使用得比较频繁，比如：定期的内存监控、JVM运行状况监控。 1&quot;VM Periodic Task Thread&quot; prio=10 tid=0x00007fbea82ae800 nid=0x5fa waiting on condition 可以使用jstat 命令查看GC的情况，比如查看某个进程没有存活必要的引用可以使用命令 jstat -gcutil &lt;pid&gt; 250 7 参数中pid是进程id，后面的250和7表示每250毫秒打印一次，总共打印7次。这对于防止因为应用代码中直接使用native库或者第三方的一些监控工具的内存泄漏有非常大的帮助。 “GC task thread#0 (ParallelGC)”垃圾回收线程，该线程会负责进行垃圾回收。通常JVM会启动多个线程来处理这个工作，线程名称中#后面的数字也会累加。 1234567&quot;GC task thread#0 (ParallelGC)&quot; prio=5 tid=0x00007fc6b480d000 nid=0x2503 runnable&quot;GC task thread#1 (ParallelGC)&quot; prio=5 tid=0x00007fc6b2812000 nid=0x2703 runnable&quot;GC task thread#2 (ParallelGC)&quot; prio=5 tid=0x00007fc6b2812800 nid=0x2903 runnable&quot;GC task thread#3 (ParallelGC)&quot; prio=5 tid=0x00007fc6b2813000 nid=0x2b03 runnable 如果在JVM中增加了 -XX:+UseConcMarkSweepGC 参数将会启用CMS （Concurrent Mark-Sweep）GC Thread方式，以下是该模式下的线程类型： “Gang worker#0 (Parallel GC Threads)”原来垃圾回收线程GC task thread#0 (ParallelGC) 被替换为 Gang worker#0 (Parallel GC Threads)。Gang worker 是JVM用于年轻代垃圾回收(minor gc)的线程。 123&quot;Gang worker#0 (Parallel GC Threads)&quot; prio=10 tid=0x00007fbea801b800 nid=0x5e4 runnable &quot;Gang worker#1 (Parallel GC Threads)&quot; prio=10 tid=0x00007fbea801d800 nid=0x5e7 runnable “Concurrent Mark-Sweep GC Thread”并发标记清除垃圾回收器（就是通常所说的CMS GC）线程， 该线程主要针对于年老代垃圾回收。 1&quot;Concurrent Mark-Sweep GC Thread&quot; prio=10 tid=0x00007fbea8073800 nid=0x5e8 runnable “Surrogate Locker Thread (Concurrent GC)”此线程主要配合CMS垃圾回收器来使用，是一个守护线程，主要负责处理GC过程中Java层的Reference（指软引用、弱引用等等）与jvm 内部层面的对象状态同步。 12&quot;Surrogate Locker Thread (Concurrent GC)&quot; daemon prio=10 tid=0x00007fbea8158800 nid=0x5ee waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE 这里以 WeakHashMap 为例进行说明，首先是一个关键点： WeakHashMap和HashMap一样，内部有一个Entry[]数组; WeakHashMap的Entry比较特殊，它的继承体系结构为Entry-&gt;WeakReference-&gt;Reference; Reference 里面有一个全局锁对象：Lock，它也被称为pending_lock，注意：它是静态对象； Reference 里面有一个静态变量：pending； Reference 里面有一个静态内部类：ReferenceHandler的线程，它在static块里面被初始化并且启动，启动完成后处于wait状态，它在一个Lock同步锁模块中等待； WeakHashMap里面还实例化了一个ReferenceQueue列队 假设，WeakHashMap对象里面已经保存了很多对象的引用，JVM 在进行CMS GC的时候会创建一个ConcurrentMarkSweepThread（简称CMST）线程去进行GC。ConcurrentMarkSweepThread线程被创建的同时会创建一个SurrogateLockerThread（简称SLT）线程并且启动它，SLT启动之后，处于等待阶段。CMST开始GC时，会发一个消息给SLT让它去获取Java层Reference对象的全局锁：Lock。直到CMS GC完毕之后，JVM 会将WeakHashMap中所有被回收的对象所属的WeakReference容器对象放入到Reference 的pending属性当中（每次GC完毕之后，pending属性基本上都不会为null了），然后通知SLT释放并且notify全局锁:Lock。此时激活了ReferenceHandler线程的run方法，使其脱离wait状态，开始工作了。ReferenceHandler这个线程会将pending中的所有WeakReference对象都移动到它们各自的列队当中，比如当前这个WeakReference属于某个WeakHashMap对象，那么它就会被放入相应的ReferenceQueue列队里面（该列队是链表结构）。 当我们下次从WeakHashMap对象里面get、put数据或者调用size方法的时候，WeakHashMap就会将ReferenceQueue列队中的WeakReference依依poll出来去和Entry[]数据做比较，如果发现相同的，则说明这个Entry所保存的对象已经被GC掉了，那么将Entry[]内的Entry对象剔除掉。 第五部分：JNI global references count这一部分主要回收那些在native代码上被引用，但在java代码中却没有存活必要的引用，对于防止因为应用代码中直接使用native库或第三方的一些监控工具的内存泄漏有非常大的帮助。 1JNI global references: 830]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(一)使用jstack定位线程堆栈信息]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E4%B8%80)%E4%BD%BF%E7%94%A8jstack%E5%AE%9A%E4%BD%8D%E7%BA%BF%E7%A8%8B%E5%A0%86%E6%A0%88%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com 基本概念在对Java内存泄漏进行分析的时候，需要对jvm运行期间的内存占用、线程执行等情况进行记录的dump文件，常用的主要有thread dump和heap dump。 thread dump 主要记录JVM在某一时刻各个线程执行的情况，以栈的形式显示，是一个文本文件。通过对thread dump文件可以分析出程序的问题出现在什么地方，从而定位具体的代码然后进行修正。thread dump需要结合占用系统资源的线程id进行分析才有意义。 heap dump 主要记录了在某一时刻JVM堆中对象使用的情况，即某个时刻JVM堆的快照，是一个二进制文件，主要用于分析哪些对象占用了太对的堆空间，从而发现导致内存泄漏的对象。 上面两种dump文件都具有实时性，因此需要在服务器出现问题的时候生成，并且多生成几个文件，方便进行对比分析。下面我们先来说一下如何生成 thread dump。 使用jstack生成thread dump当服务器出现高CPU的时候，首先执行 top -c 命令动态显示进程及占用资源的排行，如下图： top后面的参数-c可以显示进程详细的信息。top命令执行的时候还可以执行一些快捷键： 1 对于多核服务器，可以显示各个CPU占用资源的情况 shift+h 显示所有的线程信息 shift+w 将当前 top 命令的设置保存到 ~/.toprc 文件中，这样不用每次都执行快捷键了 以上图为例，pid为1503的进程占用了大量的CPU资源，接下来需要将占用CPU最高进程中的线程打印出来，可以用 top -bn1 -H -p &lt;pid&gt; 命令，执行结果如下： 上面 -bn1 参数的含义是只输出一次结果，而不是显示一个动态的结果。 我个人请喜欢用 ps -mp &lt;pid&gt; -o THREAD,tid,time | sort -k2r 命令查看，后面的sort参数根据线程占用的cpu比例进行排序，结果如下： 接下来我们清楚今天的主角 jstack，这是一个在JDK5开始提供的内置工具，可以打印指定进程中线程运行的状态，包括线程数量、是否存在死锁、资源竞争情况和线程的状态等等。有下面的几个常用的参数： -l 长列表，打印关于锁的附加信息 -m 打印java和jni框架的所有栈信息 因为thread id在栈信息中是以十六进制的形式显示的，因此需要使用 printf &quot;%x \n&quot; &lt;tid&gt; 命令将现场id转成十六进制的值，然后执行 jstack -l &lt;pid&gt; | grep &lt;thread-hex-id&gt; -A 10 命令显示出错的堆栈信息，如下图： 上面命令中 -A 10 参数用来指定显示行数，否则只会显示一行信息。 这样通过上图，可以很快地定位到程序问题的代码，然后对代码进行分析和改进即可。注意：需要在多个时间段提出多个 Thread Dump信息，然后综合进行对比分析，单独分析一个文件是没有意义的。 生成shell文件上面讲述了整个的分析过程，不过所有的命令就是实时的，所以最好创建一个shell脚本瞬间执行完成，下面对 当CPU飙高时，它在做什么 这篇文章中所提供的shell进行了改进如下： 123456789101112131415161718192021222324252627282930313233343536373839404142#!/bin/bashif [ $# -le 0 ]; then echo "usage: $0 &lt;pid&gt; [line-number]" exit 1fi# java homeif test -z $JAVA_HOME then JAVA_HOME='/usr/local/jdk'fi#pidpid=$1# checking pidif test -z "$($JAVA_HOME/bin/jps -l | cut -d '' -f 1 | grep $pid)"then echo "process of $pid is not exists" exitfi#line numberlinenum=$2if test -z $linenumthen linenum=10fistackfile=stack$pid.dumpthreadsfile=threads$pid.dump# generate java stack$JAVA_HOME/bin/jstack -l $pid &gt;&gt; $stackfileps -mp $pid -o THREAD,tid,time | sort -k2r | awk '&#123;if ($1 !="USER" &amp;&amp; $2 != "0.0" &amp;&amp; $8 !="-") print $8;&#125;' | xargs printf "%x\n" &gt;&gt; $threadsfiletids="$(cat $threadsfile)"for tid in $tidsdo echo "------------------------------ ThreadId ($tid) ------------------------------" cat $stackfile | grep 0x$tid -A $linenumdonerm -f $stackfile $threadsfile]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(四)jstack生成的Thread Dump日志线程状态]]></title>
    <url>%2F2019%2F07%2F06%2FJVM%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90%2F(%E5%9B%9B)jstack%E7%94%9F%E6%88%90%E7%9A%84Thread%20Dump%E6%97%A5%E5%BF%97%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81%20%2F</url>
    <content type="text"><![CDATA[原文地址：http://www.javatang.com Thread Dump日志的线程信息以下面的日志为例： 1"resin-22129" daemon prio=10 tid=0x00007fbe5c34e000 nid=0x4cb1 waiting on condition [0x00007fbe4ff7c000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:315) at com.caucho.env.thread2.ResinThread2.park(ResinThread2.java:196) at com.caucho.env.thread2.ResinThread2.runTasks(ResinThread2.java:147) at com.caucho.env.thread2.ResinThread2.run(ResinThread2.java:118)"Timer-20" daemon prio=10 tid=0x00007fe3a4bfb800 nid=0x1a31 in Object.wait() [0x00007fe3a077a000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000006f0620ff0&gt; (a java.util.TaskQueue) at java.util.TimerThread.mainLoop(Timer.java:552) - locked &lt;0x00000006f0620ff0&gt; (a java.util.TaskQueue) at java.util.TimerThread.run(Timer.java:505) 以上依次是： &quot;resin-22129&quot;线程名称：如果使用 java.lang.Thread 类生成一个线程的时候，线程名称为 Thread-(数字) 的形式，这里是resin生成的线程； daemon线程类型：线程分为守护线程 (daemon) 和非守护线程 (non-daemon) 两种，通常都是守护线程； prio=10线程优先级：默认为5，数字越大优先级越高； tid=0x00007fbe5c34e000JVM线程的id：JVM内部线程的唯一标识，通过 java.lang.Thread.getId()获取，通常用自增的方式实现； nid=0x4cb1系统线程id：对应的系统线程id（Native Thread ID)，可以通过 top 命令进行查看，现场id是十六进制的形式； waiting on condition系统线程状态：这里是系统的线程状态，具体的含义见下面 系统线程状态 部分； [0x00007fbe4ff7c000]起始栈地址：线程堆栈调用的其实内存地址； java.lang.Thread.State: WAITING (parking)JVM线程状态：这里标明了线程在代码级别的状态，详细的内容见下面的 JVM线程运行状态 部分。 线程调用栈信息：下面就是当前线程调用的详细栈信息，用于代码的分析。堆栈信息应该从下向上解读，因为程序调用的顺序是从下向上的。 系统线程状态 (Native Thread Status)系统线程有如下状态： deadlock：死锁线程，一般指多个线程调用期间进入了相互资源占用，导致一直等待无法释放的情况。 runnable：一般指该线程正在执行状态中，该线程占用了资源，正在处理某个操作，如通过SQL语句查询数据库、对某个文件进行写入等。 blocked：线程正处于阻塞状态，指当前线程执行过程中，所需要的资源长时间等待却一直未能获取到，被容器的线程管理器标识为阻塞状态，可以理解为等待资源超时的线程。 waiting on condition：线程正处于等待资源或等待某个条件的发生，具体的原因需要结合下面堆栈信息进行分析。 如果堆栈信息明确是应用代码，则证明该线程正在等待资源，一般是大量读取某种资源且该资源采用了资源锁的情况下，线程进入等待状态，等待资源的读取，或者正在等待其他线程的执行等。 如果发现有大量的线程都正处于这种状态，并且堆栈信息中得知正等待网络读写，这是因为网络阻塞导致线程无法执行，很有可能是一个网络瓶颈的征兆： 网络非常繁忙，几乎消耗了所有的带宽，仍然有大量数据等待网络读写； 网络可能是空闲的，但由于路由或防火墙等原因，导致包无法正常到达； 所以一定要结合系统的一些性能观察工具进行综合分析，比如netstat统计单位时间的发送包的数量，看是否很明显超过了所在网络带宽的限制；观察CPU的利用率，看系统态的CPU时间是否明显大于用户态的CPU时间。这些都指向由于网络带宽所限导致的网络瓶颈。 还有一种常见的情况是该线程在 sleep，等待 sleep 的时间到了，将被唤醒。 waiting for monitor entry 或 in Object.wait()：Moniter 是Java中用以实现线程之间的互斥与协作的主要手段，它可以看成是对象或者class的锁，每个对象都有，也仅有一个 Monitor。 从上图可以看出，每个Monitor在某个时刻只能被一个线程拥有，该线程就是 “Active Thread”，而其他线程都是 “Waiting Thread”，分别在两个队列 “Entry Set”和”Waint Set”里面等待。其中在 “Entry Set” 中等待的线程状态是 waiting for monitor entry，在 “Wait Set” 中等待的线程状态是 in Object.wait()。 “Entry Set”里面的线程。 我们称被 synchronized 保护起来的代码段为临界区，对应的代码如下： 1synchronized(obj)&#123;&#125; 当一个线程申请进入临界区时，它就进入了 “Entry Set” 队列中，这时候有两种可能性： 该Monitor不被其他线程拥有，”Entry Set”里面也没有其他等待的线程。本线程即成为相应类或者对象的Monitor的Owner，执行临界区里面的代码；此时在Thread Dump中显示线程处于 “Runnable” 状态。 该Monitor被其他线程拥有，本线程在 “Entry Set” 队列中等待。此时在Thread Dump中显示线程处于 “waiting for monity entry” 状态。 临界区的设置是为了保证其内部的代码执行的原子性和完整性，但因为临界区在任何时间只允许线程串行通过，这和我们使用多线程的初衷是相反的。如果在多线程程序中大量使用synchronized，或者不适当的使用它，会造成大量线程在临界区的入口等待，造成系统的性能大幅下降。如果在Thread Dump中发现这个情况，应该审视源码并对其进行改进。 “Wait Set”里面的线程 当线程获得了Monitor，进入了临界区之后，如果发现线程继续运行的条件没有满足，它则调用对象（通常是被synchronized的对象）的wait()方法，放弃Monitor，进入 “Wait Set”队列。只有当别的线程在该对象上调用了 notify()或者notifyAll()方法，”Wait Set”队列中的线程才得到机会去竞争，但是只有一个线程获得对象的Monitor，恢复到运行态。”Wait Set”中的线程在Thread Dump中显示的状态为 in Object.wait()。通常来说， 通常来说，当CPU很忙的时候关注 Runnable 状态的线程，反之则关注 waiting for monitor entry 状态的线程。 JVM线程运行状态 (JVM Thread Status) 在 java.lang.Thread.State 中定义了线程的状态： NEW：至今尚未启动的线程的状态。线程刚被创建，但尚未启动。 RUNNABLE：可运行线程的线程状态。线程正在JVM中执行，有可能在等待操作系统中的其他资源，比如处理器。 BLOCKED：受阻塞并且正在等待监视器的某一线程的线程状态。处于受阻塞状态的某一线程正在等待监视器锁，以便进入一个同步的块/方法，或者在调用 Object.wait 之后再次进入同步的块/方法。在Thread Dump日志中通常显示为 java.lang.Thread.State: BLOCKED (on object monitor) 。 WAITING：某一等待线程的线程状态。线程正在无期限地等待另一个线程来执行某一个特定的操作，线程因为调用下面的方法之一而处于等待状态： 不带超时的 Object.wait 方法，日志中显示为 java.lang.Thread.State: WAITING (on object monitor) 不带超时的 Thread.join 方法 LockSupport.park 方法，日志中显示为 java.lang.Thread.State: WAITING (parking) TIMED_WAITING：指定了等待时间的某一等待线程的线程状态。线程正在等待另一个线程来执行某一个特定的操作，并设定了指定等待的时间，线程因为调用下面的方法之一而处于定时等待状态： Thread.sleep 方法 指定超时值的 Object.wait 方法 指定超时值的 Thread.join 方法 LockSupport.parkNanos LockSupport.parkUntil TERMINATED：线程处于终止状态。根据Java Doc中的说明，在给定的时间上，一个只能处于上述的一种状态之中，并且这些状态都是JVM的状态，跟操作系统中的线程状态无关。 线程状态样例等待状态样例 1"IoWaitThread" prio=6 tid=0x0000000007334800 nid=0x2b3c waiting on condition [0x000000000893f000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000007d5c45850&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987) at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:440) at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:629) at com.nbp.theplatform.threaddump.ThreadIoWaitState$IoWaitHandler2.run(ThreadIoWaitState.java:89) at java.lang.Thread.run(Thread.java:662) 上面例子中，IoWaitThread 线程保持等待状态并从 LinkedBlockingQueue 接收消息，如果 LinkedBlockingQueue 一直没有消息，该线程的状态将不会改变。 阻塞状态样例 1"BLOCKED_TEST pool-1-thread-1" prio=6 tid=0x0000000006904800 nid=0x28f4 runnable [0x000000000785f000] java.lang.Thread.State: RUNNABLE at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:282) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123) - locked &lt;0x0000000780a31778&gt; (a java.io.BufferedOutputStream) at java.io.PrintStream.write(PrintStream.java:432) - locked &lt;0x0000000780a04118&gt; (a java.io.PrintStream) at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:202) at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:272) at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:85) - locked &lt;0x0000000780a040c0&gt; (a java.io.OutputStreamWriter) at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:168) at java.io.PrintStream.newLine(PrintStream.java:496) - locked &lt;0x0000000780a04118&gt; (a java.io.PrintStream) at java.io.PrintStream.println(PrintStream.java:687) - locked &lt;0x0000000780a04118&gt; (a java.io.PrintStream) at com.nbp.theplatform.threaddump.ThreadBlockedState.monitorLock(ThreadBlockedState.java:44) - locked &lt;0x0000000780a000b0&gt; (a com.nbp.theplatform.threaddump.ThreadBlockedState) at com.nbp.theplatform.threaddump.ThreadBlockedState$1.run(ThreadBlockedState.java:7) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - &lt;0x0000000780a31758&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)"BLOCKED_TEST pool-1-thread-2" prio=6 tid=0x0000000007673800 nid=0x260c waiting for monitor entry [0x0000000008abf000] java.lang.Thread.State: BLOCKED (on object monitor) at com.nbp.theplatform.threaddump.ThreadBlockedState.monitorLock(ThreadBlockedState.java:43) - waiting to lock &lt;0x0000000780a000b0&gt; (a com.nbp.theplatform.threaddump.ThreadBlockedState) at com.nbp.theplatform.threaddump.ThreadBlockedState$2.run(ThreadBlockedState.java:26) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - &lt;0x0000000780b0c6a0&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)"BLOCKED_TEST pool-1-thread-3" prio=6 tid=0x00000000074f5800 nid=0x1994 waiting for monitor entry [0x0000000008bbf000] java.lang.Thread.State: BLOCKED (on object monitor) at com.nbp.theplatform.threaddump.ThreadBlockedState.monitorLock(ThreadBlockedState.java:42) - waiting to lock &lt;0x0000000780a000b0&gt; (a com.nbp.theplatform.threaddump.ThreadBlockedState) at com.nbp.theplatform.threaddump.ThreadBlockedState$3.run(ThreadBlockedState.java:34) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Locked ownable synchronizers: - &lt;0x0000000780b0e1b8&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync) 在上面的例子中，BLOCKED_TEST pool-1-thread-1 线程占用了 锁，然而 BLOCKED_TEST pool-1-thread-2 和 BLOCKED_TEST pool-1-thread-3 threads 正在等待获取锁。 死锁状态样例 1"DEADLOCK_TEST-1" daemon prio=6 tid=0x000000000690f800 nid=0x1820 waiting for monitor entry [0x000000000805f000] java.lang.Thread.State: BLOCKED (on object monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.goMonitorDeadlock(ThreadDeadLockState.java:197) - waiting to lock &lt;0x00000007d58f5e60&gt; (a com.nbp.theplatform.threaddump.ThreadDeadLockState$Monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.monitorOurLock(ThreadDeadLockState.java:182) - locked &lt;0x00000007d58f5e48&gt; (a com.nbp.theplatform.threaddump.ThreadDeadLockState$Monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.run(ThreadDeadLockState.java:135) Locked ownable synchronizers: - None"DEADLOCK_TEST-2" daemon prio=6 tid=0x0000000006858800 nid=0x17b8 waiting for monitor entry [0x000000000815f000] java.lang.Thread.State: BLOCKED (on object monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.goMonitorDeadlock(ThreadDeadLockState.java:197) - waiting to lock &lt;0x00000007d58f5e78&gt; (a com.nbp.theplatform.threaddump.ThreadDeadLockState$Monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.monitorOurLock(ThreadDeadLockState.java:182) - locked &lt;0x00000007d58f5e60&gt; (a com.nbp.theplatform.threaddump.ThreadDeadLockState$Monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.run(ThreadDeadLockState.java:135) Locked ownable synchronizers: - None"DEADLOCK_TEST-3" daemon prio=6 tid=0x0000000006859000 nid=0x25dc waiting for monitor entry [0x000000000825f000] java.lang.Thread.State: BLOCKED (on object monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.goMonitorDeadlock(ThreadDeadLockState.java:197) - waiting to lock &lt;0x00000007d58f5e48&gt; (a com.nbp.theplatform.threaddump.ThreadDeadLockState$Monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.monitorOurLock(ThreadDeadLockState.java:182) - locked &lt;0x00000007d58f5e78&gt; (a com.nbp.theplatform.threaddump.ThreadDeadLockState$Monitor) at com.nbp.theplatform.threaddump.ThreadDeadLockState$DeadlockThread.run(ThreadDeadLockState.java:135) Locked ownable synchronizers: - None 上面的例子中，当线程 A 需要获取线程 B 的锁来继续它的任务，然而线程 B 也需要获取线程 A 的锁来继续它的任务的时候发生的。在 thread dump 中，你能看到 DEADLOCK_TEST-1 线程持有 0x00000007d58f5e48 锁，并且尝试获取 0x00000007d58f5e60 锁。你也能看到 DEADLOCK_TEST-2 线程持有 0x00000007d58f5e60，并且尝试获取 0x00000007d58f5e78，同时 DEADLOCK_TEST-3 线程持有 0x00000007d58f5e78，并且在尝试获取 0x00000007d58f5e48 锁，如你所见，每个线程都在等待获取另外一个线程的锁，这状态将不会被改变直到一个线程丢弃了它的锁。 无限等待的Runnable状态样例 1"socketReadThread" prio=6 tid=0x0000000006a0d800 nid=0x1b40 runnable [0x00000000089ef000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.read(SocketInputStream.java:129) at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:264) at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:306) at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:158) - locked &lt;0x00000007d78a2230&gt; (a java.io.InputStreamReader) at sun.nio.cs.StreamDecoder.read0(StreamDecoder.java:107) - locked &lt;0x00000007d78a2230&gt; (a java.io.InputStreamReader) at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:93) at java.io.InputStreamReader.read(InputStreamReader.java:151) at com.nbp.theplatform.threaddump.ThreadSocketReadState$1.run(ThreadSocketReadState.java:27) at java.lang.Thread.run(Thread.java:662) 上例中线程的状态是RUNNABLE，但在下面的堆栈日志中发现socketReadThread 线程正在无限等待读取 socket，因此不能单纯通过线程的状态来确定线程是否处于阻塞状态，应该根据详细的堆栈信息进行分析。]]></content>
      <categories>
        <category>JVM故障分析</category>
      </categories>
      <tags>
        <tag>JVM故障分析</tag>
        <tag>jstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Stream Kafka 异常]]></title>
    <url>%2F2019%2F07%2F04%2FException%2FSpring%20Cloud%20Stream%20Kafka%20%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[问题描述(一)123452019-07-04 10:12:33.287 ERROR 91464 --- [p-nio-80-exec-9] o.a.c.c.C.[.[.[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.binder.kafka.KafkaMessageChannelBinder$ProducerConfigurationMessageHandler@7cc5f891]; nested exception is org.apache.kafka.common.errors.SerializationException: Can't convert value of class [B to class org.apache.kafka.common.serialization.StringSerializer specified in value.serializer, failedMessage=GenericMessage [payload=byte[11], headers=&#123;contentType=application/json, id=5279bb8b-7a15-6089-79f6-4a655c430ce8, timestamp=1562206353207&#125;]] with root causejava.lang.ClassCastException: [B cannot be cast to java.lang.String at org.apache.kafka.common.serialization.StringSerializer.serialize(StringSerializer.java:28) ~[kafka-clients-2.0.1.jar:na] ...... 解决方案在Spring Boot配置文件中新增配置如下 1spring.cloud.stream.bindings.output.producer.use-native-encoding=true 问题描述(二)12345678910111213141516171819202122org.springframework.context.ApplicationContextException: Failed to start bean 'outputBindingLifecycle'; nested exception is java.lang.IllegalArgumentException: A default binder has been requested, but there is no binder available at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:185) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:53) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:360) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:158) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:122) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:893) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:161) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:552) ~[spring-context-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:742) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:389) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1213) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1202) [spring-boot-2.1.6.RELEASE.jar:2.1.6.RELEASE] at club.codeopen.kafka.SpringCloudStreanKafkaDemoApplication.main(SpringCloudStreanKafkaDemoApplication.java:10) [classes/:na]Caused by: java.lang.IllegalArgumentException: A default binder has been requested, but there is no binder available at org.springframework.util.Assert.notEmpty(Assert.java:508) ~[spring-core-5.1.8.RELEASE.jar:5.1.8.RELEASE] at org.springframework.cloud.stream.binder.DefaultBinderFactory.doGetBinder(DefaultBinderFactory.java:140) ~[spring-cloud-stream-2.1.2.RELEASE.jar:2.1.2.RELEASE] at org.springframework.cloud.stream.binder.DefaultBinderFactory.getBinder(DefaultBinderFactory.java:130) ~[spring-cloud-stream-2.1.2.RELEASE.jar:2.1.2.RELEASE] at org.springframework.cloud.stream.binding.BindingService.getBinder(BindingService.java:337) ~[spring-cloud-stream-2.1.2.RELEASE.jar:2.1.2.RELEASE] at org.springframework.clou 解决方案 检查是否引入依赖spring-cloud-stream-binder-kafka 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;&lt;/dependency&gt; 检查是否引入Spring Cloud版本，若未引入，则需要增加依赖的版本信息 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 问题描述(三)Spring Cloud Stream Kafka 启动成功，发送消息成功，但消费者无法接受 解决方案在Spring Boot配置文件中新增配置如下 1spring.cloud.stream.bindings.output.content-type=text/plain]]></content>
      <categories>
        <category>Exception</category>
      </categories>
      <tags>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群搭建]]></title>
    <url>%2F2019%2F06%2F30%2FLinux%2FKafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[简介Apache Kafka是一款分布式消息发布和订阅系统，具有高性能、高吞吐量的特点而被广泛应用与大数据传输场景。它由LinkedIn公司开发，使用Scala语言编写，之后成为Apache基金会的一个顶级项目。官方网站 集群搭建下载并解压Kafka包官方下载地址 注意Kafka所基于的Scala版本 Binary downloads: Scala 2.11 - kafka_2.11-1.0.0.tgz (asc, sha512) Scala 2.12 - kafka_2.12-1.0.0.tgz (asc, sha512) 解压Kafka：tar -zxf kafka_2.11-2.3.0.tgz 配置Zookeeper 下载Zookeeper 进入Zookeeper安装目录，编辑配置文件vi conf/zoo.cfg 12345678tickTime=2000dataDir=/var/lib/zookeeperclientPort=2181initLimit=5syncLimit=2server.1=192.168.22.200:2888:3888server.2=192.168.22.201:2888:3888server.3=192.168.22.202:2888:3888 配置zookeeper集群的myid 1234# 创建 dataDir 文件夹mkdir /var/lib/zookeeper# 在ZK集群机器分别设置对应的myid,如在 192.168.22.200 节点设置echo "1" &gt; /var/lib/zookeeper/myid 启动zookeeper集群 分别在集群中的机器中Kafka安装目录中执行 1./bin/zkServer.sh start 验证启动成功 12345678910111213141516171819./bin/zkServer.sh status# 可以观察到如下内容# 从节点ZooKeeper JMX enabled by defaultUsing config: /usr/local/kafka/zookeeper-3.4.10/bin/../conf/zoo.cfgStarting zookeeper ... already running as process 9408.[root@root bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/kafka/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower# 主节点ZooKeeper JMX enabled by defaultUsing config: /usr/local/kafka/apache-zookeeper-3.5.5/bin/zookeeper-3.4.10/bin/../conf/zoo.cfgError contacting service. It is probably not running.[root@root bin]# ./zookeeper-3.4.10/bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/kafka/apache-zookeeper-3.5.5/bin/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader 配置Kafka 修改Kafka服务器配置文件 进入Kafka安装目录，编辑配置文件vi config/service.properties，修改以下内容 123456# 修改broker.id，每台服务器的broker.id为唯一的integer类型数字broker.id=0# 添加主节点监听端口，IP为本机IP地址listeners=PLAINTEXT://192.168.22.200:9092# 设置Zookeeper集群连接zookeeper.connect=192.168.22.200:2181,192.168.22.201:2181,192.168.22.202:2181 启动Kafka集群 在每台服务器的Kafka安装目录执行 1./bin/kafka-server-start.sh -daemon config/server.properties 验证 使用zkCli进入Zookeeper，在Zookeeper安装目录执行./bin/zkCli.sh -server 192.168.22.200:2181(Zookeeper集群任意一台均可) 1234ls /# 可以看到多出来许多节点，这些都是Kafka在ZK上注册的节点[cluster, controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config] controller：控制节点 brokers：Kafka集群的broker信息，以及topic信息 consumers：ids/owners/offsets 停止Kafka 1./bin/kafka-server-stop.sh 附录Kafka使用文档：https://kafka.apache.org/10/documentation.html#quickstart]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat外网远程Debug]]></title>
    <url>%2F2019%2F06%2F30%2FdevNote%2FTomcat%E5%A4%96%E7%BD%91%E8%BF%9C%E7%A8%8BDebug%2F</url>
    <content type="text"><![CDATA[保持远端代码版本本地代码版本一致 执行sudo vim ${tomcat}/bin/catalina.sh进行编辑，其中${tomcat}为tomcat的安装目录 添加如下配置 1CATALINA_OPTS=&quot;-server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005&quot; 其中address为开发的远程debug的端口号 把开放远程debug的端口加到防火墙配置中 及时关闭开发的debug端口 编辑sudo vim /etc/sysconfig/iptables 添加如下配置-A INPUT -p tcp -m tcp --dport 5000 -j ACCEPT :wq保存退出 重启防火墙sudo service iptables restart]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat Debug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.2安装htop]]></title>
    <url>%2F2019%2F06%2F30%2FLinux%2FCentOS7.2%E5%AE%89%E8%A3%85htop%2F</url>
    <content type="text"><![CDATA[在CentOS7.2中安装htop安装gcc及其编译的库12345如果没有安装gcc，按如下来安装yum install -y gcc安装后，编译htop需要安装一个编译Linux内核的库yum install -y ncurses-devel htop下载，编译和安装1234567htop下载wget http://sourceforge.net/projects/htop/files/latest/download解压tar -zxf downloadcd htop-1.0.2./configure &amp;&amp; make &amp;&amp; make install 测试12# 执行htop]]></content>
      <categories>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>htop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 18.04配置静态IP地址]]></title>
    <url>%2F2019%2F06%2F30%2FLinux%2FUbuntu%2018.04%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81IP%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[编辑网络配置文件 vi /etc/netplan/50-cloud-init.yaml 将原文件修改为如下内容 12345678910111213141516# This file is generated from information provided by# the datasource. Changes to it will not persist across an instance.# To disable cloud-init&apos;s network configuration capabilities, write a file# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:# network: &#123;config: disabled&#125;network: ethernets: ens33: dhcp4: no dhcp6: no addresses: [192.168.22.100/24] optional: true gateway4: 192.168.22.2 nameservers: addresses: [8.8.8.8,202.96.209.133]version: 2 启用配置netplan apply]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 18.04 vi编辑器原格式粘贴]]></title>
    <url>%2F2019%2F06%2F30%2FLinux%2FUbuntu%2018.04%20vi%E7%BC%96%E8%BE%91%E5%99%A8%E5%8E%9F%E6%A0%BC%E5%BC%8F%E7%B2%98%E8%B4%B4%2F</url>
    <content type="text"><![CDATA[使用vi编辑器打开文件 使用命令编辑模式(即刚使用vi编辑器打开文件状态) 输入:set paste，回车后，按i进入编辑模式，即可使用原样粘贴]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7设置固定IP]]></title>
    <url>%2F2019%2F06%2F30%2FLinux%2FCentOS7%E5%9B%BA%E5%AE%9AIP%2F</url>
    <content type="text"><![CDATA[查看当前启用的网卡1234567891011121314151617[root@root ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:9c:f1:80 brd ff:ff:ff:ff:ff:ff inet 192.168.22.128/24 brd 192.168.22.255 scope global noprefixroute dynamic ens33 valid_lft 1453sec preferred_lft 1453sec inet6 fe80::d5c8:a782:53b7:4e5b/64 scope link noprefixroute valid_lft forever preferred_lft forever[root@root ~]# cd /etc/sysconfig/network-scripts[root@root network-scripts]# ls -ltr ifcfg-*-rw-r--r--. 1 root root 254 1月 3 2018 ifcfg-lo-rw-r--r--. 1 root root 310 6月 24 16:27 ifcfg-ens33 如上所示，修改ifcfg-ens33设置为静态IP 修改配置文件1234567891011121314151617181920212223vi /etc/sysconfig/network-scripts/ifcfg-ens33# 修改为如下内容TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="static"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"IPV6INIT="yes"IPV6_AUTOCONF="yes"IPV6_DEFROUTE="yes"IPV6_FAILURE_FATAL="no"IPV6_ADDR_GEN_MODE="stable-privacy"NAME="ens33"UUID="213b156a-84f4-413f-9e3d-87216e967165"DEVICE="ens33"ONBOOT="yes"IPADDR=192.168.22.200NETMASK=255.255.255.0GATEWAY=192.168.22.2DNS1=114.114.114.114 //DNSNM_CONTROLLED=no //是否由networkmanager控制，no否 保存 重启网络1systemctl restart network.service]]></content>
      <categories>
        <category>FTP</category>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubectl 常用命令]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[小提示： 所有命令前都可以加上 watch 命令来观察状态的实时变化，如：watch kubectl get pods --all-namespaces 查看组件状态12&gt; kubectl get cs&gt; 查看环境信息1kubectl cluster-info 查看 Node1kubectl get nodes -o wide 查看集群配置1kubectl -n kube-system get cm kubeadm-config -oyaml 运行容器1kubectl run nginx --image=nginx --replicas=2 --port=80 暴露服务1kubectl expose deployment nginx --port=80 --type=LoadBalancer 查看命名空间1kubectl get namespace 创建命名空间1234apiVersion: v1kind: Namespacemetadata: name: development 查看容器12kubectl get pods -o widekubectl get deployment -o wide 看服务1kubectl get service -o wide 查看详情123kubectl describe pod &lt;Pod Name&gt;kubectl describe deployment &lt;Deployment Name&gt;kubectl describe service &lt;Service Name&gt; 查看日志1kubectl logs -f &lt;Pod Name&gt; 删除容器和服务12kubectl delete deployment &lt;Deployment Name&gt;kubectl delete service &lt;Service Name&gt; 配置方式运行1kubectl create -f &lt;YAML&gt; 配置方式删除1kubectl delete -f &lt;YAML&gt; 查看配置12kubeadm config viewkubectl config view 查看 Ingress1kubectl get ingress 查看持久卷1kubectl get pv 查看持久卷消费者1kubectl get pvc 查看 ConfigMap1kubectl get cm &lt;ConfigMap Name&gt; 修改 ConfigMap1kubectl edit cm &lt;ConfigMap Name&gt;]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubectl 与 Docker 命令]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubectl%20%E4%B8%8E%20Docker%20%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[概述Docker 命令和 Kubectl 命令有很多相似的地方，Docker 操作容器，Kubectl 操作 Pod（容器的集合）等 运行容器 docker：docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx kubectl： kubectl run --image=nginx nginx-app --port=80 --env=&quot;DOMAIN=cluster&quot; kubectl expose deployment nginx-app --port=80 --name=nginx-http 注意： kubectl run 会创建一个 Deployment 并且默认会在后台运行，以上面的代码为例它的名称为 nginx-app。默认情况 Deployment 并不会将端口暴露出去，所以我们还需要使用 kubectl expose 暴露端口以供访问，此时还会创建一个同名的 Service 查看已运行的容器 docker：docker ps kubectl： kubectl get pods kubectl get deployment kubectl get service 交互式进入容器 docker：docker exec -it &lt;容器 ID/NAME&gt; /bin/bash kubectl：kubectl exec -it &lt;容器名&gt; -- /bin/bash 打印日志 docker：docker logs -f &lt;容器 ID/NAME&gt; kubectl：kubectl logs -f &lt;容器名&gt; 停止和删除容器 docker： docker stop &lt;容器 ID/NAME&gt; docker rm &lt;容器 ID/NAME&gt; kubectl： kubectl delete deployment &lt;Deployment 名称&gt; kubectl delete service &lt;Service 名称&gt; 注意： 不要直接删除 Pod，使用 kubectl 请删除拥有该 Pod 的 Deployment。如果直接删除 Pod，则 Deployment 将会重新创建该 Pod 查看版本 docker：docker version kubectl：kubectl version 查看环境信息 docker：docker info kubectl：kubectl cluster-info]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Dashboard]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubernetes%20Dashboard%2F</url>
    <content type="text"><![CDATA[概述Kubernetes Dashboard 是 Kubernetes 集群的 Web UI，用于管理集群。 安装GitHub 地址：Kubernetes Dashboard 下载配置文件 1wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 修改配置如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 省略部分代码...# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard # 修改镜像地址为阿里云 image: registry.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: # 修改类型为 NodePort 访问 type: NodePort ports: - port: 443 targetPort: 8443 # 设置端口号为 30001 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 部署到集群 1234567# 部署kubectl create -f kubernetes-dashboard.yaml# 查看kubectl -n kube-system get podskubectl -n kube-system get service kubernetes-dashboardkubectl -n kube-system describe service kubernetes-dashboard 访问需要使用 NodeIP:30001 访问 Dashboard，因为证书原因除火狐浏览器外其它浏览器无法直接打开页面 Chrome 浏览器显示如下 点击 接受风险并继续 即可显示欢迎界面 登录 我们采用 Token 方式登录 创建登录账号，创建一个名为 dashboard-adminuser.yaml 的配置文件 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system 1kubectl create -f dashboard-adminuser.yaml 打印 Token 信息 12345678910111213141516kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;')# 输出如下Name: admin-user-token-86cz9Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 3902d3d4-8b13-11e9-8089-000c29d49c77Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTg2Y3o5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzOTAyZDNkNC04YjEzLTExZTktODA4OS0wMDBjMjlkNDljNzciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.pA44wyarsahOwqH7X7RVlcdB1k3_j-L3gwOYlTQ4_Lu5ZmfXDFlhqN-Q1tdryJes_V1Nj_utocnXBAxsGzOGaVR4Te4oli3htSepI9MrggQAyeC3C0_QANXGCE6V5L6B5tGZ6tDsY92VDnlvz2N6OrHaH2IJJd2DlxzYvAPvfAFuPeHWuPeVxUisMfXeW42S7US6skZwbZ06JrPYAFxHjqv3zoxRxI8-bmekltvOamsrL0pAXvIUzaowgbjiQb2NgeLAw9O6qfYcz5DAi2C-7G_yAcve6pgnWcIGhVpKoim9DfJUhe1SVx4H4X5Na6GVaaD6FdUIb7UOgsO1FVpTPw 将 Token 输入浏览器，成功登陆后效果如下]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes ConfigMap]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubernetes%20ConfigMap%2F</url>
    <content type="text"><![CDATA[概述ConfigMap 是用来存储配置文件的 Kubernetes 资源对象，所有的配置内容都存储在 etcd 中。它可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制对象。ConfigMap API 资源提供了将配置数据注入容器的方式，同时保证该机制对容器来说是透明的。配置应该从 Image 内容中解耦，以此来保持容器化应用程序的可移植性。 使用 ConfigMap 配置 MySQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1kind: ConfigMapmetadata: name: mysql-test-configdata: # 这里是键值对数据 mysqld.cnf: | [client] port=3306 [mysql] no-auto-rehash [mysqld] skip-host-cache skip-name-resolve default-authentication-plugin=mysql_native_password character-set-server=utf8mb4 collation-server=utf8mb4_general_ci explicit_defaults_for_timestamp=true lower_case_table_names=1---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: mysql-testspec: replicas: 1 template: metadata: labels: name: mysql-test spec: containers: - name: mysql-test image: mysql imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: "123456" volumeMounts: # 以数据卷的形式挂载 MySQL 配置文件目录 - name: cm-vol-test mountPath: /etc/mysql/conf.d - name: nfs-vol-test mountPath: /var/lib/mysql volumes: # 将 ConfigMap 中的内容以文件形式挂载进数据卷 - name: cm-vol-test configMap: name: mysql-test-config items: # ConfigMap 中的 Key - key: mysqld.cnf # ConfigMap Key 匹配的 Value 写入名为 mysqld.cnf 的文件中 path: mysqld.cnf - name: nfs-vol-test persistentVolumeClaim: claimName: nfs-pvc-mysql-test---apiVersion: v1kind: Servicemetadata: name: mysql-testspec: ports: - port: 3306 targetPort: 3306 nodePort: 32036 type: LoadBalancer selector: name: mysql-test 123# 查看 ConfigMapkubectl get cmkubectl describe cm &lt;ConfigMap Name&gt;]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s数据持久化]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FK8s%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[概述在 Docker 中就有数据卷的概念，当容器删除时，数据也一起会被删除，想要持久化使用数据，需要把主机上的目录挂载到 Docker 中去，在 K8S 中，数据卷是通过 Pod 实现持久化的，如果 Pod 删除，数据卷也会一起删除，k8s 的数据卷是 docker 数据卷的扩展，K8S 适配各种存储系统，包括本地存储 EmptyDir，HostPath， 网络存储（NFS，GlusterFS，PV/PVC）等。 我们以部署 MySQL8 为例，采用 NFS + PV/PVC 网络存储方案实现我们的 Kubernetes 数据持久化。 存储管理与计算管理是两个不同的问题。Persistent Volume 子系统，对存储的供应和使用做了抽象，以 API 形式提供给管理员和用户使用。要完成这一任务，我们引入了两个新的 API 资源：Persistent Volume（持久卷） 和 Persistent Volume Claim（持久卷消费者）。 Persistent Volume（PV）是集群之中的一块网络存储。跟 Node 一样，也是集群的资源。PV 跟 Volume (卷) 类似，不过会有独立于 Pod 的生命周期。这一 API 对象包含了存储的实现细节，例如 NFS、iSCSI 或者其他的云提供商的存储系统。Persistent Volume Claim (PVC) 是用户的一个请求。跟 Pod 类似，Pod 消费 Node 的资源，PVC 消费 PV 的资源。Pod 能够申请特定的资源（CPU 和内存）；Claim 能够请求特定的尺寸和访问模式（例如可以加载一个读写，以及多个只读实例） 什么是 NFSNFS 是 Network File System 的简写，即网络文件系统，NFS 是 FreeBSD 支持的文件系统中的一种。NFS 基于 RPC (Remote Procedure Call) 远程过程调用实现，其允许一个系统在网络上与它人共享目录和文件。通过使用 NFS，用户和程序就可以像访问本地文件一样访问远端系统上的文件。NFS 是一个非常稳定的，可移植的网络文件系统。具备可扩展和高性能等特性，达到了企业级应用质量标准。由于网络速度的增加和延迟的降低，NFS 系统一直是通过网络提供文件系统服务的有竞争力的选择 。 NFS 原理NFS 使用 RPC (Remote Procedure Call) 的机制进行实现，RPC 使得客户端可以调用服务端的函数。同时，由于有 VFS 的存在，客户端可以像使用其它普通文件系统一样使用 NFS 文件系统。经由操作系统的内核，将 NFS 文件系统的调用请求通过 TCP/IP 发送至服务端的 NFS 服务。NFS 服务器执行相关的操作，并将操作结果返回给客户端。 NFS 服务主要进程 rpc.nfsd：最主要的 NFS 进程，管理客户端是否可登录 rpc.mountd：挂载和卸载 NFS 文件系统，包括权限管理 rpc.lockd：非必要，管理文件锁，避免同时写出错 rpc.statd：非必要，检查文件一致性，可修复文件 NFS 的关键工具 主要配置文件：/etc/exports NFS 文件系统维护命令：/usr/bin/exportfs 共享资源的日志文件：/var/lib/nfs/*tab 客户端查询共享资源命令：/usr/sbin/showmount 端口配置：/etc/sysconfig/nfs NFS 服务端配置在 NFS 服务器端的主要配置文件为 /etc/exports 时，通过此配置文件可以设置共享文件目录。每条配置记录由 NFS 共享目录、NFS 客户端地址和参数这 3 部分组成，格式如下： 1[NFS 共享目录] [NFS 客户端地址 1 (参数 1, 参数 2, 参数 3……)] [客户端地址 2 (参数 1, 参数 2, 参数 3……)] NFS 共享目录：服务器上共享出去的文件目录 NFS 客户端地址：允许其访问的 NFS 服务器的客户端地址，可以是客户端 IP 地址，也可以是一个网段 (192.168.141.0/24) 访问参数：括号中逗号分隔项，主要是一些权限选项 访问权限参数 序号 选项 描述 1 ro 客户端对于共享文件目录为只读权限。默认 2 rw 客户端对于共享文件目录具有读写权限 用户映射参数 序号 选项 描述 1 root_squash 使客户端使用 root 账户访冋时，服务器映射为服务器本地的匿名账号 2 no_root_squash 客户端连接服务端时如果使用的是 root，那么也拥有对服务端分享的目录的 root 权限 3 all_squash 将所有客户端用户请求映射到匿名用户或用户组（nfsnobody) 4 no_all_squash 与上相反。默认 5 anonuid=xxx 将远程访问的所有用户都映射为匿名用户，并指定该用户为本地用户(UID=xxx) 6 anongid=xxx 将远程访问的所有用户组都映射为匿名用户组账户，并指定该匿名用户组账户为本地用户组账户(GUI=xxx) 其它配置参数 序号 选项 描述 1 sync 同步写操作，数据写入存储设备后返回成功信息。默认 2 async 异步写提作，数据在未完全写入存储设备前就返回成功信息，实际还在内存， 3 wdelay 延迟写入选项，将多个写提请求合并后写入硬盘，减少 I/O 次数， NFS 非正常关闭数据可能丢失。默认 4 no_wdelay 与上相反，不与 async 同时生效，如果 NFS 服务器主要收到小且不相关的请求，该选项实际会降低性能 5 subtree 若输出目录是一个子目录，则 NFS 服务器将检查其父目录的权限。默认 6 no_subtree 即使输出目录是一个子目录， NFS 服务器也不检查其父目录的权限，这样可以提高效率 7 secure 限制客户端只能从小于 1024 的 TCP/IP 端口连接 NFS 服务器。默认 8 insecure 允许客户端从大于 1024 的 TCP/IP 端口连接服务器 安装 NFS 服务端由于 NFS 是一套分布式文件系统，我们再创建一台独立的虚拟机作为我们 NFS 服务端，配置如下 主机名 IP 系统 CPU/内存 磁盘 kubernetes-volumes 192.168.141.140 Ubuntu Server 18.04 2核2G 20G 创建一个目录作为共享文件目录 1mkdir -p /usr/local/kubernetes/volumes 给目录增加读写权限 1chmod a+rw /usr/local/kubernetes/volumes 安装 NFS 服务端 12apt-get updateapt-get install -y nfs-kernel-server 配置 NFS 服务目录，打开文件 1vi /etc/exports ，在尾部新增一行，内容如下 /usr/local/kubernetes/volumes：作为服务目录向客户端开放 *：表示任何 IP 都可以访问 rw：读写权限 sync：同步权限 no_subtree_check：表示如果输出目录是一个子目录，NFS 服务器不检查其父目录的权限 1/usr/local/kubernetes/volumes *(rw,sync,no_subtree_check) 重启服务，使配置生效 1/etc/init.d/nfs-kernel-server restart 安装 NFS 客户端安装客户端的目的是验证是否可以上传文件到服务端，安装命令如下 1apt-get install -y nfs-common 创建 NFS 客户端挂载目录 1mkdir -p /usr/local/kubernetes/volumes-mount 将 NFS 服务器的 /usr/local/kubernetes/volumes 目录挂载到 NFS 客户端的 /usr/local/kubernetes/volumes-mount 目录 1mount 192.168.141.140:/usr/local/kubernetes/volumes /usr/local/kubernetes/volumes-mount 使用 df 命令查看挂载信息 12345678910111213141516df# 输出如下Filesystem 1K-blocks Used Available Use% Mounted onudev 977556 0 977556 0% /devtmpfs 201732 1252 200480 1% /run/dev/mapper/ubuntu--vg-ubuntu--lv 19475088 5490916 12971848 30% /tmpfs 1008648 0 1008648 0% /dev/shmtmpfs 5120 0 5120 0% /run/locktmpfs 1008648 0 1008648 0% /sys/fs/cgroup/dev/loop0 90624 90624 0 100% /snap/core/6964/dev/loop1 93184 93184 0 100% /snap/core/6350/dev/sda2 999320 214252 716256 24% /boottmpfs 201728 0 201728 0% /run/user/0# 有此输出表示挂载成功193.192.168.141.140:/usr/local/kubernetes/volumes 19475200 5490944 12972032 30% /usr/local/kubernetes/volumes-mount 验证 NFS 服务 测试文件上传 1ip addr &gt; /usr/local/kubernetes/volumes-mount/test.txt 查看 /usr/local/kubernetes/volumes 目录下是否有 test.txt 文件，有则表示成功 取消 NFS 客户端挂载 注意： 不要直接在挂载目录下执行，否则会报错 1umount /usr/local/kubernetes/volumes-mount PV 与 PVCPV 是集群的资源。PVC 是对这一资源的请求，也是对资源的所有权的检验。PV 和 PVC 之间的互动遵循如下的生命周期。 供应： 集群管理员会创建一系列的 PV。这些 PV 包含了为集群用户提供的真实存储资源，它们可利用 Kubernetes API 来消费。 绑定： 用户创建一个包含了容量和访问模式的持久卷申请。Master 会监听 PVC 的产生，并尝试根据请求内容查找匹配的 PV，并把 PV 和 PVC 进行绑定。用户能够获取满足需要的资源，并且在使用过程中可能超出请求数量。如果找不到合适的卷，这一申请就会持续处于非绑定状态，一直到出现合适的 PV。例如一个集群准备了很多的 50G 大小的持久卷，（虽然总量足够）也是无法响应 100G 的申请的，除非把 100G 的 PV 加入集群。 使用： Pod 把申请作为卷来使用。集群会通过 PVC 查找绑定的 PV，并 Mount 给 Pod。对于支持多种访问方式的卷，用户在使用 PVC 作为卷的时候，可以指定需要的访问方式。一旦用户拥有了一个已经绑定的 PVC，被绑定的 PV 就归该用户所有了。用户的 Pods 能够通过在 Pod 的卷中包含的 PVC 来访问他们占有的 PV。 释放： 当用户完成对卷的使用时，就可以利用 API 删除 PVC 对象了，而且他还可以重新申请。删除 PVC 后，对应的卷被视为 “被释放”，但是这时还不能给其他的 PVC 使用。之前的 PVC 数据还保存在卷中，要根据策略来进行后续处理。 回收： PV 的回收策略向集群阐述了在 PVC 释放卷的时候，应如何进行后续工作。目前可以采用三种策略：保留，回收或者删除。保留策略允许重新申请这一资源。在持久卷能够支持的情况下，删除策略会同时删除持久卷以及 AWS EBS/GCE PD 或者 Cinder 卷中的存储内容。如果插件能够支持，回收策略会执行基础的擦除操作（rm -rf /thevolume/*），这一卷就能被重新申请了。 定义 PV持久卷插件持久卷是以插件方式实现的，目前支持的插件如下： GCEPersistentDisk AWSElasticBlockStore NFS（我们采用的是该方案） iSCSI RBD (Ceph Block Device) Glusterfs HostPath (单节点测试使用) 本地持久卷 YAML 配置创建一个名为 nfs-pv-mysql.yml 的配置文件 1234567891011121314151617181920apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv-mysqlspec: # 设置容量 capacity: storage: 5Gi # 访问模式 accessModes: # 该卷能够以读写模式被多个节点同时加载 - ReadWriteMany # 回收策略，这里是基础擦除 `rm-rf/thevolume/*` persistentVolumeReclaimPolicy: Recycle nfs: # NFS 服务端配置的路径 path: "/usr/local/kubernetes/volumes" # NFS 服务端地址 server: 192.168.141.140 readOnly: false 12345678# 部署kubectl create -f nfs-pv-mysql.yml# 删除kubectl delete -f nfs-pv-mysql.yml# 查看kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfs-pv-mysql 5Gi RWX Recycle Available 29m 配置说明Capacity（容量）一般来说，PV 会指定存储容量。这里需要使用 PV 的 capcity 属性。目前存储大小是唯一一个能够被申请的指标，今后会加入更多属性，例如 IOPS，吞吐能力等 AccessModes（访问模式）只要资源提供者支持，持久卷能够被用任何方式加载到主机上。每种存储都会有不同的能力，每个 PV 的访问模式也会被设置成为该卷所支持的特定模式。例如 NFS 能够支持多个读写客户端，但是某个 NFS PV 可能会在服务器上以只读方式使用。每个 PV 都有自己的一系列的访问模式，这些访问模式取决于 PV 的能力。访问模式的可选范围如下： ReadWriteOnce：该卷能够以读写模式被加载到一个节点上 ReadOnlyMany：该卷能够以只读模式加载到多个节点上 ReadWriteMany：该卷能够以读写模式被多个节点同时加载 在 CLI 下，访问模式缩写为： RWO：ReadWriteOnce ROX：ReadOnlyMany RWX：ReadWriteMany 另外，一个卷不论支持多少种访问模式，同时只能以一种访问模式加载。例如一个 GCE Persistent Disk 既能支持 ReadWriteOnce，也能支持 ReadOnlyMany RecyclingPolicy（回收策略）当前的回收策略可选值包括： Retain：人工重新申请 Recycle：基础擦除（rm-rf/thevolume/*） Delete：相关的存储资产例如 AWS EBS，GCE PD 或者 OpenStack Cinder 卷一并删除 目前，只有 NFS 和 HostPath 支持 Recycle 策略，AWS EBS、GCE PD 以及 Cinder 卷支持 Delete 策略 阶段（Phase）一个卷会处于如下阶段之一： Available：可用资源，尚未被绑定到 PVC 上 Bound：该卷已经被绑定 Released：PVC 已经被删除，但该资源尚未被集群回收 Failed：该卷的自动回收过程失败 定义 PVC创建一个名为 nfs-pvc-mysql-test.yml 的配置文件 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-pvc-mysql-testspec: accessModes: # 需要使用和 PV 一致的访问模式 - ReadWriteMany # 按需分配资源 resources: requests: storage: 1Gi 123456# 部署kubectl create -f nfs-pvc-mysql-test.yml# 删除kubectl delete -f nfs-pvc-mysql-test.yml# 查看kubectl get pvc 部署 MySQL8 注意： 要确保每台 Node 都安装了 NFS 客户端，apt-get install -y nfs-common 12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: mysql-testspec: replicas: 1 template: metadata: labels: name: mysql-test spec: containers: - name: mysql-test image: mysql # 只有镜像不存在时，才会进行镜像拉取 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 # 同 Docker 配置中的 environment env: - name: MYSQL_ROOT_PASSWORD value: "123456" # 容器中的挂载目录 volumeMounts: - name: nfs-vol-test mountPath: /var/lib/mysql volumes: # 挂载到数据卷 - name: nfs-vol-test persistentVolumeClaim: claimName: nfs-pvc-mysql-test---apiVersion: v1kind: Servicemetadata: name: mysql-testspec: ports: - port: 3306 targetPort: 3306 type: LoadBalancer selector: name: db-test 解决权限问题当你使用 kubectl create -f &lt;YAML&gt; 部署后，你会发现 Pod 状态为 Error，容器无法正常启动的情况，我们可以使用 kubectl logs &lt;Pod Name&gt; 看到一条日志 1chown: changing ownership of &apos;/var/lib/mysql/&apos;: Operation not permitted 解决方案是在 NFS 服务端配置中增加一个参数 no_root_squash，即将配置修改为：/usr/local/kubernetes/volumes *(rw,sync,no_subtree_check,no_root_squash) 测试运行部署成功后可以使用 kubectl get service 查看我们 MySQL 的运行端口，再使用连接工具连接会报如下错误 意思为无法使用密码的方式登录，在 Docker 部署时我们可以在 YAML 中配置相关参数解决这个问题；下一节我们讲解在 Kubernetes 中采用 ConfigMap 的方式配置 MySQL 附：ImagePullPolicy支持三种 ImagePullPolicy Always： 不管镜像是否存在都会进行一次拉取 Never： 不管镜像是否存在都不会进行拉取 IfNotPresent： 只有镜像不存在时，才会进行镜像拉取 注意 默认为 IfNotPresent，但 :latest 标签的镜像默认为 Always 拉取镜像时 Docker 会进行校验，如果镜像中的 MD5 码没有变，则不会拉取镜像数据 生产环境中应该尽量避免使用 :latest 标签，而开发环境中可以借助 :latest 标签自动拉取最新的镜像]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ingress 统一访问入口]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FIngress%20%E7%BB%9F%E4%B8%80%E8%AE%BF%E9%97%AE%E5%85%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[术语 节点： Kubernetes 集群中的服务器 集群： Kubernetes 管理的一组服务器集合 边界路由器： 为局域网和 Internet 路由数据包的路由器，执行防火墙保护局域网络 集群网络： 遵循 Kubernetes 网络模型实现集群内的通信的具体实现，比如 Flannel 和 Calico 服务： Kubernetes 的服务 (Service) 是使用标签选择器标识的一组 Pod Service (Deployment)。 除非另有说明，否则服务的虚拟 IP 仅可在集群内部访问 内部访问方式 ClusterIPClusterIP 服务是 Kubernetes 的默认服务。它给你一个集群内的服务，集群内的其它应用都可以访问该服务。集群外部无法访问它。在某些场景下我们可以使用 Kubernetes 的 Proxy 模式来访问服务，比如调试服务时。 三种外部访问方式NodePortNodePort 服务是引导外部流量到你的服务的最原始方式。NodePort，正如这个名字所示，在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务。 NodePort 服务特征如下： 每个端口只能是一种服务 端口范围只能是 30000-32767（可调） 不在 YAML 配置文件中指定则会分配一个默认端口 建议： 不要在生产环境中使用这种方式暴露服务，大多数时候我们应该让 Kubernetes 来选择端口 LoadBalancerLoadBalancer 服务是暴露服务到 Internet 的标准方式。所有通往你指定的端口的流量都会被转发到对应的服务。它没有过滤条件，没有路由等。这意味着你几乎可以发送任何种类的流量到该服务，像 HTTP，TCP，UDP，WebSocket，gRPC 或其它任意种类。 IngressIngress 事实上不是一种服务类型。相反，它处于多个服务的前端，扮演着 “智能路由” 或者集群入口的角色。你可以用 Ingress 来做许多不同的事情，各种不同类型的 Ingress 控制器也有不同的能力。它允许你基于路径或者子域名来路由流量到后端服务。 Ingress 可能是暴露服务的最强大方式，但同时也是最复杂的。Ingress 控制器有各种类型，包括 Google Cloud Load Balancer， Nginx，Contour，Istio，等等。它还有各种插件，比如 cert-manager (它可以为你的服务自动提供 SSL 证书)/ 如果你想要使用同一个 IP 暴露多个服务，这些服务都是使用相同的七层协议（典型如 HTTP），你还可以获取各种开箱即用的特性（比如 SSL、认证、路由等等） 什么是 Ingress通常情况下，Service 和 Pod 的 IP 仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到 Service 在 Node 上暴露的 NodePort 上，然后再由 kube-proxy 通过边缘路由器 (edge router) 将其转发给相关的 Pod 或者丢弃。而 Ingress 就是为进入集群的请求提供路由规则的集合 Ingress 可以给 Service 提供集群外部访问的 URL、负载均衡、SSL 终止、HTTP 路由等。为了配置这些 Ingress 规则，集群管理员需要部署一个 Ingress Controller，它监听 Ingress 和 Service 的变化，并根据规则配置负载均衡并提供访问入口。 使用 Nginx Ingress Controller本次实践的主要目的就是将入口统一，不再通过 LoadBalancer 等方式将端口暴露出来，而是使用 Ingress 提供的反向代理负载均衡功能作为我们的唯一入口。通过以下步骤操作仔细体会。 注意： 下面包含资源配置的步骤都是自行创建 YAML 配置文件通过 kubectl create -f &lt;YAML&gt; 和 kubectl delete -f &lt;YAML&gt; 部署和删除 部署 Tomcat部署 Tomcat 但仅允许在内网访问，我们要通过 Ingress 提供的反向代理功能路由到 Tomcat 之上 1234567891011121314151617181920212223242526272829apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: tomcat-appspec: replicas: 2 template: metadata: labels: name: tomcat spec: containers: - name: tomcat image: tomcat ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: tomcat-httpspec: ports: - port: 8080 targetPort: 8080 # ClusterIP, NodePort, LoadBalancer type: LoadBalancer selector: name: tomcat 安装 Nginx Ingress ControllerIngress Controller 有许多种，我们选择最熟悉的 Nginx 来处理请求，其它可以参考 官方文档 下载 Nginx Ingress Controller 配置文件 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml 修改配置文件，找到配置如下位置 (搜索 serviceAccountName) 在下面增加一句 hostNetwork: true 12345678910111213141516171819202122232425262728293031323334353637apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: # 可以部署多个实例 replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: "10254" prometheus.io/scrape: "true" spec: serviceAccountName: nginx-ingress-serviceaccount # 增加 hostNetwork: true，意思是开启主机网络模式，暴露 Nginx 服务端口 80 hostNetwork: true containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.24.1 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx// 以下代码省略... 部署 IngressIngress 翻译过来是入口的意思，说白了就是个 API 网关（想想之前学的 Zuul 和 Spring Cloud Gateway） 1234567891011121314151617181920212223242526272829303132apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: nginx-web annotations: # 指定 Ingress Controller 的类型 kubernetes.io/ingress.class: "nginx" # 指定我们的 rules 的 path 可以使用正则表达式 nginx.ingress.kubernetes.io/use-regex: "true" # 连接超时时间，默认为 5s nginx.ingress.kubernetes.io/proxy-connect-timeout: "600" # 后端服务器回转数据超时时间，默认为 60s nginx.ingress.kubernetes.io/proxy-send-timeout: "600" # 后端服务器响应超时时间，默认为 60s nginx.ingress.kubernetes.io/proxy-read-timeout: "600" # 客户端上传文件，最大大小，默认为 20m nginx.ingress.kubernetes.io/proxy-body-size: "10m" # URL 重写 nginx.ingress.kubernetes.io/rewrite-target: /spec: # 路由规则 rules: # 主机名，只能是域名，修改为你自己的 - host: k8s.test.com http: paths: - path: backend: # 后台部署的 Service Name，与上面部署的 Tomcat 对应 serviceName: tomcat-http # 后台部署的 Service Port，与上面部署的 Tomcat 对应 servicePort: 8080 验证是否成功查看 Tomcat12345kubectl get deployment# 输出如下NAME READY UP-TO-DATE AVAILABLE AGEtomcat-app 2/2 2 2 88m 123456kubectl get service# 输出如下NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 2d5htomcat-http ClusterIP 10.97.222.179 &lt;none&gt; 8080/TCP 89m 查看 Nginx Ingress Controller12345kubectl get pods -n ingress-nginx -o wide# 输出如下，注意下面的 IP 地址，就是我们实际访问地址NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-76f9fddcf8-vzkm5 1/1 Running 0 查看 Ingress12345kubectl get ingress# 输出如下NAME HOSTS ADDRESS PORTS AGEnginx-web k8s.test.com 80 61m 测试访问成功代理到 Tomcat 即表示成功 12# 不设置 Hosts 的方式请求地址，下面的 IP 和 Host 均在上面有配置curl -v http://192.168.141.160 -H 'host: k8s.test.com']]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过资源配置运行容器]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2F%E9%80%9A%E8%BF%87%E8%B5%84%E6%BA%90%E9%85%8D%E7%BD%AE%E8%BF%90%E8%A1%8C%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[概述我们知道通过 run 命令启动容器非常麻烦，Docker 提供了 Compose 为我们解决了这个问题。那 Kubernetes 是如何解决这个问题的呢？其实很简单，使用 kubectl create 命令就可以做到和 Compose 一样的效果了，该命令可以通过配置文件快速创建一个集群资源对象。 创建 YAML 配置文件以部署 Nginx 为例 部署 Deployment创建一个名为 nginx-deployment.yml 的配置文件 123456789101112131415161718192021222324252627# API 版本号apiVersion: extensions/v1beta1# 类型，如：Pod/ReplicationController/Deployment/Service/Ingresskind: Deployment# 元数据metadata: # Kind 的名称 name: nginx-appspec: # 部署的实例数量 replicas: 2 template: metadata: labels: # 容器标签的名字，发布 Service 时，selector 需要和这里对应 name: nginx spec: # 配置容器，数组类型，说明可以配置多个容器 containers: # 容器名称 - name: nginx # 容器镜像 image: nginx # 暴露端口 ports: # Pod 端口 - containerPort: 80 12345# 部署kubectl create -f nginx-deployment.yml# 删除kubectl delete -f nginx-deployment.yml 发布 Service创建一个名为 nginx-service.yml 的配置文件 123456789101112131415161718192021# API 版本号apiVersion: v1# 类型，如：Pod/ReplicationController/Deployment/Service/Ingresskind: Service# 元数据metadata: # Kind 的名称 name: nginx-httpspec: # 暴露端口 ports: ## Service 暴露的端口 - port: 80 ## Pod 上的端口，这里是将 Service 暴露的端口转发到 Pod 端口上 targetPort: 80 # 类型 type: LoadBalancer # 标签选择器 selector: # 需要和上面部署的 Deployment 标签名对应 name: nginx 12345# 部署kubectl create -f nginx-service.yml# 删除kubectl delete -f nginx-service.yml 验证是否生效查看 Pod 列表123456kubectl get pods# 输出如下NAME READY STATUS RESTARTS AGEnginx-app-64bb598779-2pplx 1/1 Running 0 25mnginx-app-64bb598779-824lc 1/1 Running 0 25m 查看 Deployment 列表12345kubectl get deployment# 输出如下NAME READY UP-TO-DATE AVAILABLE AGEnginx-app 2/2 2 2 25m 查看 Service 列表123456kubectl get service# 输出如下NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 20hnginx-http LoadBalancer 10.98.49.142 &lt;pending&gt; 80:31631/TCP 14m 查看 Service 详情1234567891011121314151617kubectl describe service nginx-app# 输出如下Name: nginx-httpNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: name=nginxType: LoadBalancerIP: 10.98.49.142Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPNodePort: &lt;unset&gt; 31631/TCPEndpoints: 10.244.141.205:80,10.244.2.4:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 通过浏览器访问通过浏览器访问 http://192.168.141.150:31631/ ，出现 Nginx 欢迎页即表示成功 集成环境部署也可以不区分配置文件，一次性部署 Deployment 和 Service，创建一个名为 nginx.yml 的配置文件，配置内容如下： 123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-appspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-httpspec: ports: - port: 80 targetPort: 80 # 可以指定 NodePort 端口，默认范围是：30000-32767 # nodePort: 30080 type: LoadBalancer selector: name: nginx 12345# 部署kubectl create -f nginx.yml# 删除kubectl delete -f nginx.yml 附：修改默认的端口范围Kubernetes 服务的 NodePort 默认端口范围是 30000-32767，在某些场合下，这个限制不太适用，我们可以自定义它的端口范围，操作步骤如下： 编辑 vi /etc/kubernetes/manifests/kube-apiserver.yaml 配置文件，增加配置 --service-node-port-range=2-65535 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-systemspec: containers: - command: - kube-apiserver # 在这里增加配置即可 - --service-node-port-range=2-65535 - --advertise-address=192.168.141.150 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.crt - --enable-admission-plugins=NodeRestriction - --enable-bootstrap-token-auth=true - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt// 以下配置省略... 使用 docker ps 命令找到 kube-apiserver 容器，再使用 docker restart &lt;ApiServer 容器 ID&gt; 即可生效。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 Node 无法加入的问题]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2F%E8%A7%A3%E5%86%B3%20Node%20%E6%97%A0%E6%B3%95%E5%8A%A0%E5%85%A5%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述当我们使用 kubeadm join 命令将 Node 节点加入集群时，你会发现所有 kubectl 命令均不可用（呈现阻塞状态，并不会返回响应结果），我们可以在 Node 节点中通过 kubeadm reset 命令将 Node 节点下线，此时回到 Master 节点再使用 watch kubectl get pods --all-namespaces 可以看到下图中报错了，coredns-xxx-xxx 状态为 CrashLoopBackOff 解决方案从上面的错误信息不难看出应该是出现了网络问题，而我们在安装过程中只使用了一个网络插件 Calico ，那么该错误是不是由 Calico 引起的呢？带着这个疑问我们去到 Calico 官网再看一下它的说明，官网地址：https://docs.projectcalico.org/v3.7/getting-started/kubernetes/ 在它的 Quickstart 里有两段话（属于特别提醒），截图如下： 上面这段话的主要意思是：当 kubeadm 安装完成后不要关机，继续完成后续的安装步骤；这也说明了安装 Kubernetes 的过程不要出现中断一口气搞定（不过这不是重点）(*￣rǒ￣) 上面这段话的主要意思是：如果你的网络在 192.168.0.0/16 网段中，则必须选择一个不同的 Pod 网络；恰巧咱们的网络范围（我虚拟机的 IP 范围是 192.168.141.0/24）和该网段重叠 (ノへ￣、)；好吧，当时做单节点集群时因为没啥问题而忽略了 ♪(^∇^*) so，能够遇到这个问题主要是因为虚拟机 IP 范围刚好和 Calico 默认网段重叠导致的，所以想要解决这个问题，咱们就需要修改 Calico 的网段了（当然也可以改虚拟机的），换句话说就是大家重装一下 o (一︿一 +) o 按照以下标准步骤重装即可 重置 Kubernetes12345678910111213141516171819202122kubeadm reset# 输出如下[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.[reset] Are you sure you want to proceed? [y/N]: y[preflight] Running pre-flight checksW0604 01:55:28.517280 22688 reset.go:234] [reset] No kubeadm config, using etcd pod spec to get data directory[reset] No etcd config found. Assuming external etcd[reset] Please manually reset etcd to prevent further issues[reset] Stopping the kubelet service[reset] unmounting mounted directories in "/var/lib/kubelet"[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes][reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki][reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]The reset process does not reset or clean up iptables rules or IPVS tables.If you wish to reset iptables, you must do so manually.For example:iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -XIf your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)to reset your system's IPVS tables. 删除 kubectl 配置1rm -fr ~/.kube/ 启用 IPVS12345modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4 导出并修改配置文件1kubeadm config print init-defaults --kubeconfig ClusterConfiguration &gt; kubeadm.yml 配置文件修改如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647apiVersion: kubeadm.k8s.io/v1beta1bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.141.150 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: kubernetes-master-01 taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta1certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: "192.168.141.200:6444"controllerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: v1.14.2networking: dnsDomain: cluster.local # 主要修改在这里，替换 Calico 网段为我们虚拟机不重叠的网段（这里用的是 Flannel 默认网段） podSubnet: "10.244.0.0/16" serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates: SupportIPVSProxyMode: truemode: ipvs kubeadm 初始化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586kubeadm init --config=kubeadm.yml --experimental-upload-certs | tee kubeadm-init.log# 输出如下[init] Using Kubernetes version: v1.14.2[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "front-proxy-ca" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "etcd/ca" certificate and key[certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [kubernetes-master-01 localhost] and IPs [192.168.141.150 127.0.0.1 ::1][certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "apiserver-etcd-client" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [kubernetes-master-01 localhost] and IPs [192.168.141.150 127.0.0.1 ::1][certs] Generating "ca" certificate and key[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [kubernetes-master-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.141.150 192.168.141.200][certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "sa" key and public key[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[kubeconfig] Writing "admin.conf" kubeconfig file[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[kubeconfig] Writing "kubelet.conf" kubeconfig file[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[kubeconfig] Writing "controller-manager.conf" kubeconfig file[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[apiclient] All control plane components are healthy after 24.507568 seconds[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Storing the certificates in ConfigMap "kubeadm-certs" in the "kube-system" Namespace[upload-certs] Using certificate key:a662b8364666f82c93cc5cd4fb4fabb623bbe9afdb182da353ac40f1752dfa4a[mark-control-plane] Marking the node kubernetes-master-01 as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node kubernetes-master-01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: abcdef.0123456789abcdef[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.141.200:6444 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:2ea8c138021fb1e184a24ed2a81c16c92f9f25c635c73918b1402df98f9c8aad \ --experimental-control-plane --certificate-key a662b8364666f82c93cc5cd4fb4fabb623bbe9afdb182da353ac40f1752dfa4aPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use "kubeadm init phase upload-certs --experimental-upload-certs" to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.141.200:6444 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:2ea8c138021fb1e184a24ed2a81c16c92f9f25c635c73918b1402df98f9c8aad 配置 kubectl1234567# 配置 kubectlmkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config# 验证是否成功kubectl get node 下载 Calico 配置文件并修改1wget https://docs.projectcalico.org/v3.7/manifests/calico.yaml 1vi calico.yaml 修改第 611 行，将 192.168.0.0/16 修改为 10.244.0.0/16，可以通过如下命令快速查找 显示行号：:set number 查找字符：/要查找的字符，输入小写 n 下一个匹配项，输入大写 N 上一个匹配项 安装 Calico1234567891011121314151617181920212223242526kubectl apply -f calico.yaml# 输出如下configmap/calico-config createdcustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org createdclusterrole.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrole.rbac.authorization.k8s.io/calico-node createdclusterrolebinding.rbac.authorization.k8s.io/calico-node createddaemonset.extensions/calico-node createdserviceaccount/calico-node createddeployment.extensions/calico-kube-controllers createdserviceaccount/calico-kube-controllers created 加入 Master 节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 示例如下，别忘记两个备用节点都要加入哦kubeadm join 192.168.141.200:6444 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:2ea8c138021fb1e184a24ed2a81c16c92f9f25c635c73918b1402df98f9c8aad \ --experimental-control-plane --certificate-key a662b8364666f82c93cc5cd4fb4fabb623bbe9afdb182da353ac40f1752dfa4a# 输出如下[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[preflight] Running pre-flight checks before initializing the new control plane instance[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [kubernetes-master-02 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.141.151 192.168.141.200][certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [kubernetes-master-02 localhost] and IPs [192.168.141.151 127.0.0.1 ::1][certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [kubernetes-master-02 localhost] and IPs [192.168.141.151 127.0.0.1 ::1][certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "apiserver-etcd-client" certificate and key[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"[certs] Using the existing "sa" key[kubeconfig] Generating kubeconfig files[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[check-etcd] Checking that the etcd cluster is healthy[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.14" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...[etcd] Announced new etcd member joining to the existing etcd cluster[etcd] Wrote Static Pod manifest for a local etcd member to "/etc/kubernetes/manifests/etcd.yaml"[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s[upload-config] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[mark-control-plane] Marking the node kubernetes-master-02 as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node kubernetes-master-02 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]This node has joined the cluster and a new control plane instance was created:* Certificate signing request was sent to apiserver and approval was received.* The Kubelet was informed of the new secure connection details.* Control plane (master) label and taint were applied to the new node.* The Kubernetes control plane instances scaled up.* A new etcd member was added to the local/stacked etcd cluster.To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configRun 'kubectl get nodes' to see this node join the cluster. 加入 Node 节点123456789101112131415161718192021# 示例如下kubeadm join 192.168.141.200:6444 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:2ea8c138021fb1e184a24ed2a81c16c92f9f25c635c73918b1402df98f9c8aad# 输出如下&gt; --discovery-token-ca-cert-hash sha256:2ea8c138021fb1e184a24ed2a81c16c92f9f25c635c73918b1402df98f9c8aad [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.14" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 验证是否可用12345678kubectl get node# 输出如下，我们可以看到 Node 节点已经成功上线 ━━(￣ー￣*|||━━NAME STATUS ROLES AGE VERSIONkubernetes-master-01 Ready master 19m v1.14.2kubernetes-master-02 Ready master 4m46s v1.14.2kubernetes-master-03 Ready master 3m23s v1.14.2kubernetes-node-01 Ready &lt;none&gt; 74s v1.14.2 1234567891011121314151617181920212223242526272829watch kubectl get pods --all-namespaces# 输出如下，coredns 也正常运行了Every 2.0s: kubectl get pods --all-namespaces kubernetes-master-01: Tue Jun 4 02:31:43 2019NAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-8646dd497f-hz5xp 1/1 Running 0 9m9skube-system calico-node-2z892 1/1 Running 0 9m9skube-system calico-node-fljxv 1/1 Running 0 6m39skube-system calico-node-vprlw 1/1 Running 0 5m16skube-system calico-node-xvqcx 1/1 Running 0 3m7skube-system coredns-8686dcc4fd-5ndjm 1/1 Running 0 21mkube-system coredns-8686dcc4fd-zxtql 1/1 Running 0 21mkube-system etcd-kubernetes-master-01 1/1 Running 0 20mkube-system etcd-kubernetes-master-02 1/1 Running 0 6m37skube-system etcd-kubernetes-master-03 1/1 Running 0 5m14skube-system kube-apiserver-kubernetes-master-01 1/1 Running 0 20mkube-system kube-apiserver-kubernetes-master-02 1/1 Running 0 6m37skube-system kube-apiserver-kubernetes-master-03 1/1 Running 0 5m14skube-system kube-controller-manager-kubernetes-master-01 1/1 Running 1 20mkube-system kube-controller-manager-kubernetes-master-02 1/1 Running 0 6m37skube-system kube-controller-manager-kubernetes-master-03 1/1 Running 0 5m14skube-system kube-proxy-68jqr 1/1 Running 0 3m7skube-system kube-proxy-69bnn 1/1 Running 0 6m39skube-system kube-proxy-vvhp5 1/1 Running 0 5m16skube-system kube-proxy-ws6wx 1/1 Running 0 21mkube-system kube-scheduler-kubernetes-master-01 1/1 Running 1 20mkube-system kube-scheduler-kubernetes-master-02 1/1 Running 0 6m37skube-system kube-scheduler-kubernetes-master-03 1/1 Running 0 5m14s 至此，Kubernetes 高可用集群算是彻底部署成功]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s高可用集群]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FK8s%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[概述在入门课程中我们部署的 Kubernetes 是 集群模式，但在实际生产中我们需要部署 高可用集群 ，本章内容旨在指导大家如何部署 Kubernetes 高可用集群 统一环境配置节点配置 主机名 IP 角色 系统 CPU/内存 磁盘 kubernetes-master-01 192.168.141.150 Master Ubuntu Server 18.04 2核2G 20G kubernetes-master-02 192.168.141.151 Master Ubuntu Server 18.04 2核2G 20G kubernetes-master-03 192.168.141.152 Master Ubuntu Server 18.04 2核2G 20G kubernetes-node-01 192.168.141.160 Node Ubuntu Server 18.04 2核4G 20G kubernetes-node-02 192.168.141.161 Node Ubuntu Server 18.04 2核4G 20G kubernetes-node-03 192.168.141.162 Node Ubuntu Server 18.04 2核4G 20G Kubernetes VIP 192.168.141.200 - - - - 对操作系统的配置 关闭交换空间1swapoff -a 避免开机启动交换空间12# 注释 swap 开头的行vi /etc/fstab 关闭防火墙1ufw disable 配置 DNS12# 取消 DNS 行注释，并增加 DNS 配置如：114.114.114.114，修改后重启下计算机vi /etc/systemd/resolved.conf 安装 Docker123456789101112# 更新软件源sudo apt-get update# 安装所需依赖sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# 安装 GPG 证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# 新增软件源信息sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"# 再次更新软件源sudo apt-get -y update# 安装 Docker CE 版sudo apt-get -y install docker-ce 配置 Docker 加速器在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）12345&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125; 安装 kubeadm，kubelet，kubectl12345678910111213# 安装系统工具apt-get update &amp;&amp; apt-get install -y apt-transport-https# 安装 GPG 证书curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -# 写入软件源；注意：我们用系统代号为 bionic，但目前阿里云不支持，所以沿用 16.04 的 xenialcat &lt;&lt; EOF &gt;/etc/apt/sources.list.d/kubernetes.list&gt; deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main&gt; EOF# 安装apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl 同步时间设置时区 1dpkg-reconfigure tzdata 先选择 Asia（亚洲）-&gt;然后选择 Shanghai（上海） 时间同步 12345678# 安装 ntpdateapt-get install ntpdate# 设置系统时间与网络时间同步（cn.pool.ntp.org 位于中国的公共 NTP 服务器）ntpdate cn.pool.ntp.org# 将系统时间写入硬件时间hwclock --systohc 确认时间 1234date# 输出如下（自行对照与系统时间是否一致）Fri Jun 28 16:04:54 CST 2019 配置 IPVS123456789101112131415161718192021222324252627# 安装系统工具apt-get install -y ipset ipvsadm# 配置并加载 IPVS 模块mkdir -p /etc/sysconfig/modules/vi /etc/sysconfig/modules/ipvs.modules# 输入如下内容#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4# 执行脚本，注意：如果重启则需要重新运行该脚本chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4# 执行脚本输出如下ip_vs_sh 16384 0ip_vs_wrr 16384 0ip_vs_rr 16384 0ip_vs 147456 6 ip_vs_rr,ip_vs_sh,ip_vs_wrrnf_conntrack_ipv4 16384 3nf_defrag_ipv4 16384 1 nf_conntrack_ipv4nf_conntrack 131072 8 xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_ipv4,nf_nat,ipt_MASQUERADE,nf_nat_ipv4,nf_conntrack_netlink,ip_vslibcrc32c 16384 4 nf_conntrack,nf_nat,raid456,ip_vs 配置内核参数123456789101112131415161718192021222324252627282930313233343536373839404142434445# 配置参数vi /etc/sysctl.d/k8s.conf# 输入如下内容net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_nonlocal_bind = 1net.ipv4.ip_forward = 1vm.swappiness=0# 应用参数sysctl --system# 应用参数输出如下（找到 Applying /etc/sysctl.d/k8s.conf 开头的日志）* Applying /etc/sysctl.d/10-console-messages.conf ...kernel.printk = 4 4 1 7* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...* Applying /etc/sysctl.d/10-kernel-hardening.conf ...kernel.kptr_restrict = 1* Applying /etc/sysctl.d/10-link-restrictions.conf ...fs.protected_hardlinks = 1fs.protected_symlinks = 1* Applying /etc/sysctl.d/10-lxd-inotify.conf ...fs.inotify.max_user_instances = 1024* Applying /etc/sysctl.d/10-magic-sysrq.conf ...kernel.sysrq = 176* Applying /etc/sysctl.d/10-network-security.conf ...net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.all.rp_filter = 1net.ipv4.tcp_syncookies = 1* Applying /etc/sysctl.d/10-ptrace.conf ...kernel.yama.ptrace_scope = 1* Applying /etc/sysctl.d/10-zeropage.conf ...vm.mmap_min_addr = 65536* Applying /usr/lib/sysctl.d/50-default.conf ...net.ipv4.conf.all.promote_secondaries = 1net.core.default_qdisc = fq_codel* Applying /etc/sysctl.d/99-sysctl.conf ...* Applying /etc/sysctl.d/k8s.conf ...net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_nonlocal_bind = 1net.ipv4.ip_forward = 1vm.swappiness = 0* Applying /etc/sysctl.conf ... 修改 cloud.cfg1234vi /etc/cloud/cloud.cfg# 该配置默认为 false，修改为 true 即可preserve_hostname: true 单独节点配置 特别注意：为 Master 和 Node 节点单独配置对应的 IP 和 主机名 配置 IP编辑 vi /etc/netplan/50-cloud-init.yaml 配置文件，修改内容如下 123456789network: ethernets: ens33: # 我的 Master 是 150 - 152，Node 是 160 - 162 addresses: [192.168.141.150/24] gateway4: 192.168.141.2 nameservers: addresses: [192.168.141.2] version: 2 配置主机名1234567# 修改主机名hostnamectl set-hostname kubernetes-master-01# 配置 hostscat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.141.150 kubernetes-master-01EOF 安装 HAProxy + Keepalived概述Kubernetes Master 节点运行组件如下： kube-apiserver： 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制 kube-scheduler： 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上 kube-controller-manager： 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等 etcd： CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等） kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。 kube-apiserver 可以运行多个实例，但对其它组件需要提供统一的访问地址，本章节部署 Kubernetes 高可用集群实际就是利用 HAProxy + Keepalived 配置该组件 配置的思路就是利用 HAProxy + Keepalived 实现 kube-apiserver 虚拟 IP 访问从而实现高可用和负载均衡，拆解如下： Keepalived 提供 kube-apiserver 对外服务的虚拟 IP（VIP） HAProxy 监听 Keepalived VIP 运行 Keepalived 和 HAProxy 的节点称为 LB（负载均衡） 节点 Keepalived 是一主多备运行模式，故至少需要两个 LB 节点 Keepalived 在运行过程中周期检查本机的 HAProxy 进程状态，如果检测到 HAProxy 进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP 的高可用 所有组件（如 kubeclt、apiserver、controller-manager、scheduler 等）都通过 VIP +HAProxy 监听的 6444 端口访问 kube-apiserver 服务（注意：kube-apiserver 默认端口为 6443，为了避免冲突我们将 HAProxy 端口设置为 6444，其它组件都是通过该端口统一请求 apiserver） 创建 HAProxy 启动脚本 该步骤在 kubernetes-master-01 执行 12345678910111213141516171819202122mkdir -p /usr/local/kubernetes/lbvi /usr/local/kubernetes/lb/start-haproxy.sh# 输入内容如下#!/bin/bash# 修改为你自己的 Master 地址MasterIP1=192.168.141.150MasterIP2=192.168.141.151MasterIP3=192.168.141.152# 这是 kube-apiserver 默认端口，不用修改MasterPort=6443# 容器将 HAProxy 的 6444 端口暴露出去docker run -d --restart=always --name HAProxy-K8S -p 6444:6444 \ -e MasterIP1=$MasterIP1 \ -e MasterIP2=$MasterIP2 \ -e MasterIP3=$MasterIP3 \ -e MasterPort=$MasterPort \ wise2c/haproxy-k8s# 设置权限chmod +x start-haproxy.sh 创建 Keepalived 启动脚本 该步骤在 kubernetes-master-01 执行 123456789101112131415161718192021222324252627282930313233mkdir -p /usr/local/kubernetes/lbvi /usr/local/kubernetes/lb/start-keepalived.sh# 输入内容如下#!/bin/bash# 修改为你自己的虚拟 IP 地址VIRTUAL_IP=192.168.141.200# 虚拟网卡设备名INTERFACE=ens33# 虚拟网卡的子网掩码NETMASK_BIT=24# HAProxy 暴露端口，内部指向 kube-apiserver 的 6443 端口CHECK_PORT=6444# 路由标识符RID=10# 虚拟路由标识符VRID=160# IPV4 多播地址，默认 224.0.0.18MCAST_GROUP=224.0.0.18docker run -itd --restart=always --name=Keepalived-K8S \ --net=host --cap-add=NET_ADMIN \ -e VIRTUAL_IP=$VIRTUAL_IP \ -e INTERFACE=$INTERFACE \ -e CHECK_PORT=$CHECK_PORT \ -e RID=$RID \ -e VRID=$VRID \ -e NETMASK_BIT=$NETMASK_BIT \ -e MCAST_GROUP=$MCAST_GROUP \ wise2c/keepalived-k8s# 设置权限chmod +x start-keepalived.sh 复制脚本到其它 Master 地址分别在 kubernetes-master-02 和 kubernetes-master-03 执行创建工作目录命令 1mkdir -p /usr/local/kubernetes/lb 将 kubernetes-master-01 中的脚本拷贝至其它 Master 12scp start-haproxy.sh start-keepalived.sh 192.168.141.151:/usr/local/kubernetes/lbscp start-haproxy.sh start-keepalived.sh 192.168.141.152:/usr/local/kubernetes/lb 分别在 3 个 Master 中启动容器（执行脚本） 1sh /usr/local/kubernetes/lb/start-haproxy.sh &amp;&amp; sh /usr/local/kubernetes/lb/start-keepalived.sh 验证是否成功查看容器123456docker ps# 输出如下CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf50df479ecae wise2c/keepalived-k8s "/usr/bin/keepalived…" About an hour ago Up About an hour Keepalived-K8S75066a7ed2fb wise2c/haproxy-k8s "/docker-entrypoint.…" About an hour ago Up About an hour 0.0.0.0:6444-&gt;6444/tcp HAProxy-K8S 查看网卡绑定的虚拟 IP123456ip a | grep ens33# 输出如下2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 inet 192.168.141.151/24 brd 192.168.141.255 scope global ens33 inet 192.168.141.200/24 scope global secondary ens33 特别注意：Keepalived 会对 HAProxy 监听的 6444 端口进行检测，如果检测失败即认定本机 HAProxy 进程异常，会将 VIP 漂移到其他节点，所以无论本机 Keepalived 容器异常或 HAProxy 容器异常都会导致 VIP 漂移到其他节点 部署 Kubernetes 集群初始化 Master 创建工作目录并导出配置文件 12345# 创建工作目录mkdir -p /usr/local/kubernetes/cluster# 导出配置文件到工作目录kubeadm config print init-defaults --kubeconfig ClusterConfiguration &gt; kubeadm.yml 修改配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152apiVersion: kubeadm.k8s.io/v1beta1bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: # 修改为主节点 IP advertiseAddress: 192.168.141.150 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: kubernetes-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta1certificatesDir: /etc/kubernetes/pkiclusterName: kubernetes# 配置 Keepalived 地址和 HAProxy 端口controlPlaneEndpoint: "192.168.141.200:6444"controllerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcd# 国内不能访问 Google，修改为阿里云imageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfiguration# 修改版本号kubernetesVersion: v1.15.0networking: dnsDomain: cluster.local # 配置成 Calico 的默认网段 podSubnet: "192.168.0.0/16" serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---# 开启 IPVS 模式apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates: SupportIPVSProxyMode: truemode: ipvs kubeadm 初始化 12345678910# kubeadm 初始化kubeadm init --config=kubeadm.yml --experimental-upload-certs | tee kubeadm-init.log# 配置 kubectlmkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config# 验证是否成功kubectl get node 安装网络插件 12345# 安装 Calicokubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml# 验证安装是否成功watch kubectl get pods --all-namespaces 加入 Master 节点从 kubeadm-init.log 中获取命令，分别将 kubernetes-master-02 和 kubernetes-master-03 加入 Master 1234# 以下为示例命令kubeadm join 192.168.141.200:6444 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:56d53268517c132ae81c868ce99c44be797148fb2923e59b49d73c99782ff21f \ --experimental-control-plane --certificate-key c4d1525b6cce4b69c11c18919328c826f92e660e040a46f5159431d5ff0545bd 加入 Node 节点从 kubeadm-init.log 中获取命令，分别将 kubernetes-node-01 至 kubernetes-node-03 加入 Node 123# 以下为示例命令kubeadm join 192.168.141.200:6444 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:56d53268517c132ae81c868ce99c44be797148fb2923e59b49d73c99782ff21f 验证集群状态 查看 Node 1kubectl get nodes -o wide 查看 Pod 1kubectl -n kube-system get pod -o wide 查看 Service 1kubectl -n kube-system get svc 验证 IPVS 查看 kube-proxy 日志，server_others.go:176] Using ipvs Proxier. 1kubectl -n kube-system logs -f &lt;kube-proxy 容器名&gt; 查看代理规则 1ipvsadm -ln 查看生效的配置 1kubectl -n kube-system get cm kubeadm-config -oyaml 查看 etcd 集群 1234567891011kubectl -n kube-system exec etcd-kubernetes-master-01 -- etcdctl \ --endpoints=https://192.168.22.150:2379 \ --ca-file=/etc/kubernetes/pki/etcd/ca.crt \ --cert-file=/etc/kubernetes/pki/etcd/server.crt \ --key-file=/etc/kubernetes/pki/etcd/server.key cluster-health# 输出如下member 1dfaf07371bb0cb6 is healthy: got healthy result from https://192.168.141.152:2379member 2da85730b52fbeb2 is healthy: got healthy result from https://192.168.141.150:2379member 6a3153eb4faaaffa is healthy: got healthy result from https://192.168.141.151:2379cluster is healthy 验证高可用 特别注意：Keepalived 要求至少 2 个备用节点，故想测试高可用至少需要 1 主 2 从模式验证，否则可能出现意想不到的问题 对任意一台 Master 机器执行关机操作 1shutdown -h now 在任意一台 Master 节点上查看 Node 状态 1234567kubectl get node# 输出如下，除已关机那台状态为 NotReady 其余正常便表示成功NAME STATUS ROLES AGE VERSIONkubernetes-master-01 NotReady master 18m v1.15.0kubernetes-master-02 Ready master 17m v1.15.0kubernetes-master-03 Ready master 16m v1.15.0 查看 VIP 漂移 123456ip a |grep ens33# 输出如下2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 inet 192.168.141.151/24 brd 192.168.141.255 scope global ens33 inet 192.168.141.200/24 scope global secondary ens33]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用K8s创建容器]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2F%E4%BD%BF%E7%94%A8K8s%E5%88%9B%E5%BB%BA%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[检查组件运行状态12345678910kubectl get cs# 输出如下NAME STATUS MESSAGE ERROR# 调度服务，主要作用是将 POD 调度到 Nodescheduler Healthy ok # 自动化修复服务，主要作用是 Node 宕机后自动修复 Node 回到正常的工作状态controller-manager Healthy ok # 服务注册与发现etcd-0 Healthy &#123;"health":"true"&#125; 检查 Master 状态 123456789kubectl cluster-info# 输出如下# 主节点状态Kubernetes master is running at https://192.168.22.100:6443# DNS 状态KubeDNS is running at https://192.168.22.100:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 检查 Nodes 状态1234567kubectl get nodes# 输出如下，STATUS 为 Ready 即为正常状态NAME STATUS ROLES AGE VERSIONkubernetes-master Ready master 44h v1.15.0kubernetes-slave1 Ready &lt;none&gt; 3h38m v1.15.0kubernetes-slave2 Ready &lt;none&gt; 3h37m v1.15.0 运行第一个容器实例123456# 使用 kubectl 命令创建两个监听 80 端口的 Nginx Pod（Kubernetes 运行容器的最小单元）kubectl run nginx --image=nginx --replicas=2 --port=80# 输出如下kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/nginx created 查看全部 Pods 的状态123456kubectl get pods# 输出如下，需要等待一小段实践，STATUS 为 Running 即为运行成功NAME READY STATUS RESTARTS AGEnginx-755464dd6c-qnmwp 1/1 Running 0 90mnginx-755464dd6c-shqrp 1/1 Running 0 90m 查看已部署的服务12345kubectl get deployment# 输出如下NAME READY UP-TO-DATE AVAILABLE AGEnginx 2/2 2 2 91m 映射服务，让用户可以访问1234kubectl expose deployment nginx --port=80 --type=LoadBalancer# 输出如下service/nginx exposed 查看已发布的服务1234567kubectl get services# 输出如下NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 44h# 由此可见，Nginx 服务已成功发布并将 80 端口映射为 31738nginx LoadBalancer 10.108.121.244 &lt;pending&gt; 80:31738/TCP 88m 查看服务详情1234567891011121314151617kubectl describe service nginx# 输出如下Name: nginxNamespace: defaultLabels: run=nginxAnnotations: &lt;none&gt;Selector: run=nginxType: LoadBalancerIP: 10.108.121.244Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPNodePort: &lt;unset&gt; 31738/TCPEndpoints: 192.168.17.5:80,192.168.8.134:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 验证是否成功通过浏览器访问 Master 服务器 1http://192.168.22.100:31738/ 此时 Kubernetes 会以负载均衡的方式访问部署的 Nginx 服务，能够正常看到 Nginx 的欢迎页即表示成功。容器实际部署在其它 Node 节点上，通过访问 Node 节点的 IP:Port 也是可以的。 停止服务1234kubectl delete deployment nginx# 输出如下deployment.extensions "nginx" deleted 1234kubectl delete service nginx# 输出如下service "nginx" deleted]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置网络]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2F%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[概述容器网络是容器选择连接到其他容器、主机和外部网络的机制。容器的 runtime 提供了各种网络模式，每种模式都会产生不同的体验。例如，Docker 默认情况下可以为容器配置以下网络： none： 将容器添加到一个容器专门的网络堆栈中，没有对外连接。 host： 将容器添加到主机的网络堆栈中，没有隔离。 default bridge： 默认网络模式。每个容器可以通过 IP 地址相互连接。 自定义网桥： 用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。 什么是 CNICNI(Container Network Interface) 是一个标准的，通用的接口。在容器平台，Docker，Kubernetes，Mesos 容器网络解决方案 flannel，calico，weave。只要提供一个标准的接口，就能为同样满足该协议的所有容器平台提供网络功能，而 CNI 正是这样的一个标准接口协议。 Kubernetes 中的 CNI 插件CNI 的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。插件负责为接口配置和管理 IP 地址，并且通常提供与 IP 管理、每个容器的 IP 分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配 IP 地址并配置网络，并在删除容器时再次调用它以清理这些资源。 运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个 veth 对的一侧。接着，它会在主机上进行更改，包括将 veth 的其他部分连接到网桥。再之后，它会通过调用单独的 IPAM（IP地址管理）插件来分配 IP 地址并设置路由。 在 Kubernetes 中，kubelet 可以在适当的时间调用它找到的插件，为通过 kubelet 启动的 pod进行自动的网络配置。 Kubernetes 中可选的 CNI 插件如下： Flannel Calico Canal Weave 什么是 CalicoCalico 为容器和虚拟机提供了安全的网络连接解决方案，并经过了大规模生产验证（在公有云和跨数千个集群节点中），可与 Kubernetes，OpenShift，Docker，Mesos，DC / OS 和 OpenStack 集成。 Calico 还提供网络安全规则的动态实施。使用 Calico 的简单策略语言，您可以实现对容器，虚拟机工作负载和裸机主机端点之间通信的细粒度控制。 安装网络插件 Calico参考官方文档安装：https://docs.projectcalico.org/v3.7/getting-started/kubernetes/ 123456789101112131415161718192021222324252627# 在 Master 节点操作即可kubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml# 安装时显示如下输出configmap/calico-config createdcustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org createdcustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org createdclusterrole.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrole.rbac.authorization.k8s.io/calico-node createdclusterrolebinding.rbac.authorization.k8s.io/calico-node createddaemonset.extensions/calico-node createdserviceaccount/calico-node createddeployment.extensions/calico-kube-controllers createdserviceaccount/calico-kube-controllers created 确认安装是否成功 123456789101112131415watch kubectl get pods --all-namespaces# 需要等待所有状态为 Running，注意时间可能较久，3 - 5 分钟的样子Every 2.0s: kubectl get pods --all-namespaces kubernetes-master: Fri May 10 18:16:51 2019NAMESPACE NAME READY STATUS RESTARTS AGEkube-system calico-kube-controllers-8646dd497f-g2lln 1/1 Running 0 50mkube-system calico-node-8jrtp 1/1 Running 0 50mkube-system coredns-8686dcc4fd-mhwfn 1/1 Running 0 51mkube-system coredns-8686dcc4fd-xsxwk 1/1 Running 0 51mkube-system etcd-kubernetes-master 1/1 Running 0 50mkube-system kube-apiserver-kubernetes-master 1/1 Running 0 51mkube-system kube-controller-manager-kubernetes-master 1/1 Running 0 51mkube-system kube-proxy-p8mdw 1/1 Running 0 51mkube-system kube-scheduler-kubernetes-master 1/1 Running 0 51m 至此基本环境已部署完毕。 解决 ImagePullBackOff在使用 watch kubectl get pods --all-namespaces 命令观察 Pods 状态时如果出现 ImagePullBackOff 无法 Running 的情况，请尝试使用如下步骤处理： Master 中删除 Nodes： kubectl drain node-name –delete-local-data –force –ignore-daemonsets kubectl delete node node-name Slave 中重置配置：kubeadm reset Slave 重启计算机：reboot Slave 重新加入集群：kubeadm join]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用kubeadm配置slave节点]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2F%E4%BD%BF%E7%94%A8kubeadm%E9%85%8D%E7%BD%AEslave%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[将 slave 节点加入到集群中很简单，只需要在 slave 服务器上安装 kubeadm，kubectl，kubelet 三个工具，然后使用 kubeadm join 命令加入即可。准备工作如下： 修改主机名(参考Kubernetes安装装备) 配置软件源(参考安装Kubeadm) 安装三个工具(参考安装Kubeadm 将 slave 加入到集群 123456789101112131415161718kubeadm join 192.168.22.130:6443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:d2872ed87a7b4ae1d9a45e8c193943242071c6859fb79aa1a6a85eed0380cadd# 安装成功将看到如下信息[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.15&quot; ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &apos;kubectl get nodes&apos; on the control-plane to see this node join the cluster. 说明： token 可以通过安装 master 时的日志查看 token 信息 可以通过 kubeadm token list 命令打印出 token 信息 如果 token 过期，可以使用 kubeadm token create 命令创建新的 token discovery-token-ca-cert-hash 可以通过安装 master 时的日志查看 sha256 信息 可以通过 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed ‘s/^.* //‘ 命令查看 sha256 信息 以上方式感谢群友 停 驻 提供。 验证是否成功回到 master 服务器 123456kubectl get nodes# 可以看到 slave 成功加入 masterNAME STATUS ROLES AGE VERSIONkubernetes-master NotReady master 3m37s v1.15.0kubernetes-slave1 NotReady &lt;none&gt; 77s v1.15.0 如果 slave 节点加入 master 时配置有问题可以在 slave 节点上使用 kubeadm reset 重置配置再使用 kubeadm join 命令重新加入即可。希望在 master 节点删除 node ，可以使用 kubeadm delete nodes 删除。 查看 pod 状态1234567891011kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-bccdc95cf-cf54h 0/1 Pending 0 4m7s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;coredns-bccdc95cf-lvkgc 0/1 Pending 0 4m7s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;etcd-kubernetes-master 1/1 Running 0 3m30s 192.168.22.100 kubernetes-master &lt;none&gt; &lt;none&gt;kube-apiserver-kubernetes-master 1/1 Running 0 3m33s 192.168.22.100 kubernetes-master &lt;none&gt; &lt;none&gt;kube-controller-manager-kubernetes-master 1/1 Running 0 3m22s 192.168.22.100 kubernetes-master &lt;none&gt; &lt;none&gt;kube-proxy-t428c 1/1 Running 0 2m8s 192.168.22.101 kubernetes-slave1 &lt;none&gt; &lt;none&gt;kube-proxy-w8r67 1/1 Running 0 4m7s 192.168.22.100 kubernetes-master &lt;none&gt; &lt;none&gt;kube-scheduler-kubernetes-master 1/1 Running 0 3m18s 192.168.22.100 kubernetes-master &lt;none&gt; &lt;none&gt; 由此可以看出 coredns 尚未运行，此时我们还需要安装网络插件。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用kubeadm搭建k8s集群]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2F%E4%BD%BF%E7%94%A8%20kubeadm%20%E6%90%AD%E5%BB%BA%20k8s%20%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[安装 kubernetes 主节点执行以下命令初始化主节点，该命令指定了初始化时需要使用的配置文件，其中添加 --experimental-upload-certs 参数可以在后续执行加入节点时自动分发证书文件。追加的 tee kubeadm-init.log 用以输出日志。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172kubeadm init --config=kubeadm.yml --experimental-upload-certs | tee kubeadm-init.log# 安装成功则会有如下输出Flag --experimental-upload-certs has been deprecated, use --upload-certs instead[init] Using Kubernetes version: v1.15.0[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder "/etc/kubernetes/pki"[certs] Generating "front-proxy-ca" certificate and key[certs] Generating "front-proxy-client" certificate and key[certs] Generating "etcd/ca" certificate and key[certs] Generating "etcd/healthcheck-client" certificate and key[certs] Generating "apiserver-etcd-client" certificate and key[certs] Generating "etcd/server" certificate and key[certs] etcd/server serving cert is signed for DNS names [kubernetes-master localhost] and IPs [192.168.22.130 127.0.0.1 ::1][certs] Generating "etcd/peer" certificate and key[certs] etcd/peer serving cert is signed for DNS names [kubernetes-master localhost] and IPs [192.168.22.130 127.0.0.1 ::1][certs] Generating "ca" certificate and key[certs] Generating "apiserver" certificate and key[certs] apiserver serving cert is signed for DNS names [kubernetes-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.22.130][certs] Generating "apiserver-kubelet-client" certificate and key[certs] Generating "sa" key and public key[kubeconfig] Using kubeconfig folder "/etc/kubernetes"[kubeconfig] Writing "admin.conf" kubeconfig file[kubeconfig] Writing "kubelet.conf" kubeconfig file[kubeconfig] Writing "controller-manager.conf" kubeconfig file[kubeconfig] Writing "scheduler.conf" kubeconfig file[control-plane] Using manifest folder "/etc/kubernetes/manifests"[control-plane] Creating static Pod manifest for "kube-apiserver"[control-plane] Creating static Pod manifest for "kube-controller-manager"[control-plane] Creating static Pod manifest for "kube-scheduler"[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[apiclient] All control plane components are healthy after 25.004958 seconds[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[kubelet] Creating a ConfigMap "kubelet-config-1.15" in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace[upload-certs] Using certificate key:f92c815417afcc03ab258791d3e75bf23d92b41dc1f5a6893a25cd217804dd43[mark-control-plane] Marking the node kubernetes-master as control-plane by adding the label "node-role.kubernetes.io/master=''"[mark-control-plane] Marking the node kubernetes-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: abcdef.0123456789abcdef[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.22.130:6443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:d2872ed87a7b4ae1d9a45e8c193943242071c6859fb79aa1a6a85eed0380cadd 注意：如果安装 kubernetes 版本和下载的镜像版本不统一则会出现 timed out waiting for the condition 错误。中途失败或是想修改配置可以使用 kubeadm reset 命令重置配置，再做初始化操作即可。 配置 kubectl12345mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/config# 非 ROOT 用户执行chown $(id -u):$(id -g) $HOME/.kube/config 验证是否成功12345kubectl get node# 能够打印出节点信息即表示成功NAME STATUS ROLES AGE VERSIONkubernetes-master NotReady master 2m10s v1.15.0 至此主节点配置完成 kubeadm init 的执行过程 init：指定版本进行初始化操作 preflight：初始化前的检查和下载所需要的 Docker 镜像文件 kubelet-start：生成 kubelet 的配置文件 var/lib/kubelet/config.yaml，没有这个文件 kubelet 无法启动，所以初始化之前的 kubelet 实际上启动不会成功 certificates：生成 Kubernetes 使用的证书，存放在 /etc/kubernetes/pki 目录中 kubeconfig：生成 KubeConfig 文件，存放在 /etc/kubernetes 目录中，组件之间通信需要使用对应文件 control-plane：使用 /etc/kubernetes/manifest 目录下的 YAML 文件，安装 Master 组件 etcd：使用 /etc/kubernetes/manifest/etcd.yaml 安装 Etcd 服务 wait-control-plane：等待 control-plan 部署的 Master 组件启动 apiclient：检查 Master 组件服务状态。 uploadconfig：更新配置 kubelet：使用 configMap 配置 kubelet patchnode：更新 CNI 信息到 Node 上，通过注释的方式记录 mark-control-plane：为当前节点打标签，打了角色 Master，和不可调度标签，这样默认就不会使用 Master 节点来运行 Pod bootstrap-token：生成 token 记录下来，后边使用 kubeadm join 往集群中添加节点时会用到 addons：安装附加组件 CoreDNS 和 kube-proxy]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubeadm配置]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubeadm%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装 kubernetes 主要是安装它的各个镜像，而 kubeadm 已经为我们集成好了运行 kubernetes 所需的基本镜像。但由于国内的网络原因，在搭建环境时，无法拉取到这些镜像。此时我们只需要修改为阿里云提供的镜像服务即可解决该问题。 创建并修改配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 导出配置文件kubeadm config print init-defaults --kubeconfig ClusterConfiguration &gt; kubeadm.yml# 修改配置为如下内容apiVersion: kubeadm.k8s.io/v1beta1bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: # 修改为主节点 IP advertiseAddress: 192.168.141.130 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: kubernetes-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta1certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: ""controllerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcd# 国内不能访问 Google，修改为阿里云imageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfiguration# 修改版本号kubernetesVersion: v1.14.1networking: dnsDomain: cluster.local # 配置成 Calico 的默认网段 podSubnet: "192.168.0.0/16" serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---# 开启 IPVS 模式apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates: SupportIPVSProxyMode: truemode: ipvs 查看和拉取镜像1234567891011# 查看所需镜像列表kubeadm config images list --config kubeadm.yml# 拉取镜像kubeadm config images pull --config kubeadm.ymlregistry.aliyuncs.com/google_containers/kube-proxy v1.15.0 d235b23c3570 5 days ago 82.4MBregistry.aliyuncs.com/google_containers/kube-apiserver v1.15.0 201c7a840312 5 days ago 207MBregistry.aliyuncs.com/google_containers/kube-controller-manager v1.15.0 8328bb49b652 5 days ago 159MBregistry.aliyuncs.com/google_containers/kube-scheduler v1.15.0 2d3813851e87 5 days ago 81.1MBregistry.aliyuncs.com/google_containers/coredns 1.3.1 eb516548c180 5 months ago 40.3MBregistry.aliyuncs.com/google_containers/etcd 3.3.10 2c4adeb21b4f 6 months ago 258MBregistry.aliyuncs.com/google_containers/pause 3.1 da86e6ba6ca1 18 months ago 742kB]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubeadm安装]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubeadm%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[kubeadm 是 kubernetes的集群安装工具，能够快速安装 kubernetes 集群。 配置软件源12345678# 安装系统工具apt-get update &amp;&amp; apt-get install -y apt-transport-https# 安装 GPG 证书curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -# 写入软件源；注意：我们用系统代号为 bionic，但目前阿里云不支持，所以沿用 16.04 的 xenialcat &lt;&lt; EOF &gt;/etc/apt/sources.list.d/kubernetes.list&gt; deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main&gt; EOF 安装 kubeadm，kubelet，kubectl12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 安装apt-get update apt-get install -y kubelet kubeadm kubectl# 安装过程如下，注意 kubeadm 的版本号Reading package lists... DoneBuilding dependency tree Reading state information... DoneThe following additional packages will be installed: conntrack cri-tools kubernetes-cni socatThe following NEW packages will be installed: conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni socat0 upgraded, 7 newly installed, 0 to remove and 96 not upgraded.Need to get 50.6 MB of archives.After this operation, 290 MB of additional disk space will be used.Get:1 http://mirrors.aliyun.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]Get:2 http://mirrors.aliyun.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]Get:3 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 cri-tools amd64 1.12.0-00 [5,343 kB]Get:4 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 kubernetes-cni amd64 0.7.5-00 [6,473 kB]Get:5 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 kubelet amd64 1.14.1-00 [21.5 MB]Get:6 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 kubectl amd64 1.14.1-00 [8,806 kB]Get:7 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 kubeadm amd64 1.14.1-00 [8,150 kB]Fetched 50.6 MB in 5s (9,912 kB/s) Selecting previously unselected package conntrack.(Reading database ... 67205 files and directories currently installed.)Preparing to unpack .../0-conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...Selecting previously unselected package cri-tools.Preparing to unpack .../1-cri-tools_1.12.0-00_amd64.deb ...Unpacking cri-tools (1.12.0-00) ...Selecting previously unselected package kubernetes-cni.Preparing to unpack .../2-kubernetes-cni_0.7.5-00_amd64.deb ...Unpacking kubernetes-cni (0.7.5-00) ...Selecting previously unselected package socat.Preparing to unpack .../3-socat_1.7.3.2-2ubuntu2_amd64.deb ...Unpacking socat (1.7.3.2-2ubuntu2) ...Selecting previously unselected package kubelet.Preparing to unpack .../4-kubelet_1.14.1-00_amd64.deb ...Unpacking kubelet (1.14.1-00) ...Selecting previously unselected package kubectl.Preparing to unpack .../5-kubectl_1.14.1-00_amd64.deb ...Unpacking kubectl (1.14.1-00) ...Selecting previously unselected package kubeadm.Preparing to unpack .../6-kubeadm_1.14.1-00_amd64.deb ...Unpacking kubeadm (1.14.1-00) ...Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...Setting up kubernetes-cni (0.7.5-00) ...Setting up cri-tools (1.12.0-00) ...Setting up socat (1.7.3.2-2ubuntu2) ...Setting up kubelet (1.14.1-00) ...Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /lib/systemd/system/kubelet.service.Setting up kubectl (1.14.1-00) ...Processing triggers for man-db (2.8.3-2ubuntu0.1) ...# 注意这里的版本号，我们使用的是 kubernetes v1.14.1Setting up kubeadm (1.14.1-00) ...# 设置 kubelet 自启动，并启动 kubeletsystemctl enable kubelet &amp;&amp; systemctl start kubelet kubeadm：用于初始化 Kubernetes 集群 kubectl：Kubernetes 的命令行工具，主要作用是部署和管理应用，查看各种资源，创建，删除和更新组件 kubelet：主要负责启动 Pod 和容器]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s安装准备]]></title>
    <url>%2F2019%2F06%2F28%2Fk8s%2FKubernetes%E5%AE%89%E8%A3%85%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[Docker安装 (APT 方式)安装123456789101112# 更新软件源sudo apt-get update# 安装所需依赖sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# 安装 GPG 证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# 新增软件源信息sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"# 再次更新软件源sudo apt-get -y update# 安装 Docker CE 版sudo apt-get -y install docker-ce 验证 12345678910111213141516171819docker versionClient: Version: 18.09.6 API version: 1.39 Go version: go1.10.8 Git commit: 481bc77 Built: Sat May 4 02:35:57 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.6 API version: 1.39 (minimum version 1.12) Go version: go1.10.8 Git commit: 481bc77 Built: Sat May 4 01:59:36 2019 OS/Arch: linux/amd64 Experimental: false 配置加速器对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） 12345&#123; "registry-mirrors": [ "https://registry.docker-cn.com" ]&#125; 注意，一定要保证该文件符合 JSON 规范，否则 Docker 将不能启动。 验证加速器是否配置成功： 1234567sudo systemctl restart dockerdocker info...# 出现如下语句即表示配置成功Registry Mirrors: https://registry.docker-cn.com/... 修改主机名在同一局域网中主机名不应该相同，所以我们需要做修改，下列操作步骤为修改 18.04 版本的 Hostname，如果是 16.04 或以下版本则直接修改 /etc/hostname 里的名称即可 查看当前 Hostname 123456789101112# 查看当前主机名hostnamectl# 显示如下内容 Static hostname: ubuntu Icon name: computer-vm Chassis: vm Machine ID: 0b45435beb6e4bf5a50cf7aad366d781 Boot ID: 3343415c079f4cffad055f6eb1f75b8a Virtualization: vmware Operating System: Ubuntu 18.04.2 LTS Kernel: Linux 4.15.0-52-generic Architecture: x86-64 修改 Hostname 12# 使用 hostnamectl 命令修改，其中 kubernetes-master 为新的主机名hostnamectl set-hostname kubernetes-master 修改 cloud.cfg 如果 cloud-init package 安装了，需要修改 cloud.cfg 文件。该软件包通常缺省安装用于处理 cloud 12345# 如果有该文件vi /etc/cloud/cloud.cfg# 该配置默认为 false，修改为 true 即可preserve_hostname: true 验证 12345678910root@kubernetes-master:~# hostnamectl Static hostname: kubernetes-master Icon name: computer-vm Chassis: vm Machine ID: 0b45435beb6e4bf5a50cf7aad366d781 Boot ID: 3343415c079f4cffad055f6eb1f75b8a Virtualization: vmware Operating System: Ubuntu 18.04.2 LTS Kernel: Linux 4.15.0-52-generic Architecture: x86-64]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置browser-sync浏览器同步测试工具]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%89%8D%E7%AB%AF%2F%E6%B5%8F%E8%A7%88%E5%99%A8%E5%90%8C%E6%AD%A5%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[简介 Time-saving synchronised browser testing. It’s wicked-fast and totally free. 官方网站 文档 起步 安装依赖12# 也可以 npm i -D browser-syncnpm install --save-dev browser-sync 配置package.json中script123456&#123; "scripts": &#123; "dev": "browser-sync start --server --files \"*.html, css/*.css, js/*.js\"", "start": "npm run dev" &#125;,&#125; 启动开发服务12# 或者 npm startnpm run dev]]></content>
      <categories>
        <category>browser-sync</category>
      </categories>
      <tags>
        <tag>browser-sync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue父子组件通信]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%89%8D%E7%AB%AF%2FVue%E7%88%B6%E5%AD%90%E7%BB%84%E4%BB%B6%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[父组件向子组件传值 组件实例定义方式，注意：一定要使用props属性来定义父组件传递过来的数据 123456789101112131415&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; msg: '这是父组件中的消息' &#125;, components: &#123; son: &#123; template: '&lt;h1&gt;这是子组件 --- &#123;&#123;finfo&#125;&#125;&lt;/h1&gt;', props: ['finfo'] &#125; &#125; &#125;); &lt;/script&gt; 使用v-bind或简化指令，将数据传递到子组件中： 123&lt;div id="app"&gt; &lt;son :finfo="msg"&gt;&lt;/son&gt; &lt;/div&gt; 子组件向父组件传值 原理：父组件将方法的引用，传递到子组件内部，子组件在内部调用父组件传递过来的方法，同时把要发送给父组件的数据，在调用方法的时候当作参数传递进去； 父组件将方法的引用传递给子组件，其中，getMsg是父组件中methods中定义的方法名称，func是子组件调用传递过来方法时候的方法名称 1&lt;son @func=&quot;getMsg&quot;&gt;&lt;/son&gt; 子组件内部通过this.$emit(&#39;方法名&#39;, 要传递的数据)方式，来调用父组件中的方法，同时把数据传递给父组件使用 12345678910111213141516171819202122232425262728293031323334&lt;div id="app"&gt; &lt;!-- 引用父组件 --&gt; &lt;son @func="getMsg"&gt;&lt;/son&gt; &lt;!-- 组件模板定义 --&gt; &lt;script type="x-template" id="son"&gt; &lt;div&gt; &lt;input type="button" value="向父组件传值" @click="sendMsg" /&gt; &lt;/div&gt; &lt;/script&gt; &lt;/div&gt; &lt;script&gt; // 子组件的定义方式 Vue.component('son', &#123; template: '#son', // 组件模板Id methods: &#123; sendMsg() &#123; // 按钮的点击事件 this.$emit('func', 'OK'); // 调用父组件传递过来的方法，同时把数据传递出去 &#125; &#125; &#125;); // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123; getMsg(val)&#123; // 子组件中，通过 this.$emit() 实际调用的方法，在此进行定义 alert(val); &#125; &#125; &#125;); &lt;/script&gt;]]></content>
      <categories>
        <category>Vuejs</category>
      </categories>
      <tags>
        <tag>Vuejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue-cli 快速上手]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%89%8D%E7%AB%AF%2Fvue-cli%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%2F</url>
    <content type="text"><![CDATA[介绍和安装在开发中，需要打包的东西不止是js、css、html。还有更多的东西要处理，这些插件和加载器如果我们一一去添加就会比较麻烦。 幸好，vue官方提供了一个快速搭建vue项目的脚手架：vue-cli 使用它能快速的构建一个web工程模板。 官网：https://github.com/vuejs/vue-cli 安装命令： 1npm install -g vue-cli 快速上手我们新建一个文件夹hello-vue-cli,打开终端并进入目录. 用vue-cli命令，快速搭建一个webpack的项目： 1vue init webpack 前面几项都走默认或yes 下面这些我们选no 最后，再选yes，使用 npm安装 项目结构安装好的项目结构： 入口文件： 单文件组件： 每一个.vue文件，就是一个独立的vue组件。类似于我们刚才写的loginForm.js和registerForm.js 只不过，我们在js中编写 html模板和样式非常的不友好，而且没有语法提示和高亮。 而单文件组件中包含三部分内容： template：模板，支持html语法高亮和提示 script：js脚本，这里编写的就是vue的组件对象，看到上面的data(){}了吧 style：样式，支持CSS语法高亮和提示 每个组件都有自己独立的html、JS、CSS，互不干扰，真正做到可独立复用 运行看生成的package.json： 可以看到这引入了非常多的依赖，绝大多数都是开发期依赖，比如大量的加载器。 运行时依赖只有vue和vue-router 脚本有三个： dev：使用了webpack-dev-server命令，开发时热部署使用 start：使用了npm run dev命令，与上面的dev效果完全一样 build：等同于webpack的打包功能，会打包到dist目录下。 我们执行npm run dev 或者 npm start 都可以启动项目： 页面：]]></content>
      <categories>
        <category>vue-cli</category>
      </categories>
      <tags>
        <tag>vue-cli</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6 语法指南]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%89%8D%E7%AB%AF%2FES6%20%E8%AF%AD%E6%B3%95%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[什么是ECMAScript来看下前端的发展历程： web1.0时代： 最初的网页以HTML为主，是纯静态的网页。网页是只读的，信息流只能从服务的到客户端单向流通。开发人员也只关心页面的样式和内容即可。 web2.0时代： 1995年，网景工程师Brendan Eich 花了10天时间设计了JavaScript语言。 1996年，微软发布了JScript，其实是JavaScript的逆向工程实现。 1997年，为了统一各种不同script脚本语言，ECMA（欧洲计算机制造商协会）以JavaScript为基础，制定了ECMAscript标准规范。JavaScript和JScript都是ECMAScript的标准实现者，随后各大浏览器厂商纷纷实现了ECMAScript标准。 所以，ECMAScript是浏览器脚本语言的规范，而各种我们熟知的js语言，如JavaScript则是规范的具体实现。 ECMAScript的快速发展而后，ECMAScript就进入了快速发展期。 1998年6月，ECMAScript 2.0 发布。 1999年12月，ECMAScript 3.0 发布。这时，ECMAScript 规范本身也相对比较完善和稳定了，但是接下来的事情，就比较悲剧了。 2007年10月。。。。ECMAScript 4.0 草案发布。 这次的新规范，历时颇久，规范的新内容也有了很多争议。在制定ES4的时候，是分成了两个工作组同时工作的。 一边是以 Adobe, Mozilla, Opera 和 Google为主的 ECMAScript 4 工作组。 一边是以 Microsoft 和 Yahoo 为主的 ECMAScript 3.1 工作组。 ECMAScript 4 的很多主张比较激进，改动较大。而 ECMAScript 3.1 则主张小幅更新。最终经过 TC39 的会议，决定将一部分不那么激进的改动保留发布为 ECMAScript 3.1，而ES4的内容，则延续到了后来的ECMAScript5和6版本中 2009年12月，ECMAScript 5 发布。 2011年6月，ECMAScript 5.1 发布。 2015年6月，ECMAScript 6，也就是 ECMAScript 2015 发布了。 并且从 ECMAScript 6 开始，开始采用年号来做版本。即 ECMAScript 2015，就是ECMAScript6。 ES5和6的一些新特性我们这里只把一些常用的进行学习，更详细的大家参考：阮一峰的ES6教程 let 和 const 命令 var 之前，js定义变量只有一个关键字：var var有一个问题，就是定义的变量有时会莫名奇妙的成为全局变量。 例如这样的一段代码： 1234for(var i = 0; i &lt; 5; i++)&#123; console.log(i);&#125;console.log("循环外：" + i) 你猜下打印的结果是什么？ let let所声明的变量，只在let命令所在的代码块内有效。 我们把刚才的var改成let试试： 1234for(let i = 0; i &lt; 5; i++)&#123; console.log(i);&#125;console.log("循环外：" + i) 结果： const const声明的变量是常量，不能被修改 字符串扩展 新的API ES6为字符串扩展了几个新的API： includes()：返回布尔值，表示是否找到了参数字符串。 startsWith()：返回布尔值，表示参数字符串是否在原字符串的头部。 endsWith()：返回布尔值，表示参数字符串是否在原字符串的尾部。 实验一下： 字符串模板 ES6中提供了`来作为字符串模板标记。我们可以这么玩： 在两个`之间的部分都会被作为字符串的值，不管你任意换行，甚至加入js脚本 键盘是的1的左侧，tab的上侧，esc的正下方 解构表达式 数组解构 比如有一个数组： 1let arr = [1,2,3] 我想获取其中的值，只能通过角标。ES6可以这样： 123const [x,y,z] = arr;// x，y，z将与arr中的每个位置对应来取值// 然后打印console.log(x,y,z); 结果： 对象解构 例如有个person对象： 12345const person = &#123; name:"jack", age:21, language: ['java','js','css']&#125; 我们可以这么做： 123456// 解构表达式获取值const &#123;name,age,language&#125; = person;// 打印console.log(name);console.log(age);console.log(language); 结果： 如过想要用其它变量接收，需要额外指定别名： {name:n}：name是person中的属性名，冒号后面的n是解构后要赋值给的变量。 函数优化 函数参数默认值 在ES6以前，我们无法给一个函数参数设置默认值，只能采用变通写法： 1234567function add(a , b) &#123; // 判断b是否为空，为空就给默认值1 b = b || 1; return a + b;&#125;// 传一个参数console.log(add(10)); 现在可以这么写： 12345function add(a , b = 1) &#123; return a + b;&#125;// 传一个参数console.log(add(10)); 箭头函数 ES6中定义函数的简写方式： 一个参数时： 12345var print = function (obj) &#123; console.log(obj);&#125;// 简写为：var print2 = obj =&gt; console.log(obj); 多个参数： 123456// 两个参数的情况：var sum = function (a , b) &#123; return a + b;&#125;// 简写为：var sum2 = (a,b) =&gt; a+b; 代码不止一行，可以用{}括起来 123var sum3 = (a,b) =&gt; &#123; return a + b;&#125; 对象的函数属性简写 比如一个Person对象，里面有eat方法： 12345678910111213let person = &#123; name: "jack", // 以前： eat: function (food) &#123; console.log(this.name + "在吃" + food); &#125;, // 箭头函数版： eat2: food =&gt; console.log(person.name + "在吃" + food),// 这里拿不到this // 简写版： eat3(food)&#123; console.log(this.name + "在吃" + food); &#125;&#125; 箭头函数结合解构表达式 比如有一个函数： 123456789const person = &#123; name:"jack", age:21, language: ['java','js','css']&#125;function hello(person) &#123; console.log("hello," + person.name)&#125; 如果用箭头函数和解构表达式 1var hi = (&#123;name&#125;) =&gt; console.log("hello," + name); map和reducemap()`：接收一个函数，将原数组中的所有元素用这个函数处理后放入新数组返回。 举例：有一个字符串数组，我们希望转为int数组 123456let arr = ['1','20','-5','3'];console.log(arr)arr = arr.map(s =&gt; parseInt(s));console.log(arr) reduce reduce()：接收一个函数（必须）和一个初始值（可选），该函数接收两个参数： 第一个参数是上一次reduce处理的结果 第二个参数是数组中要处理的下一个元素 reduce()会从左到右依次把数组中的元素用reduce处理，并把处理的结果作为下次reduce的第一个参数。如果是第一次，会把前两个元素作为计算参数，或者把用户指定的初始值作为起始参数 举例： 1const arr = [1,20,-5,3] 没有初始值： 指定初始值： promise所谓Promise，简单说就是一个容器，里面保存着某个未来才会结束的事件（通常是一个异步操作）的结果。从语法上说，Promise 是一个对象，从它可以获取异步操作的消息。Promise 提供统一的 API，各种异步操作都可以用同样的方法进行处理。 感觉跟java的Future类很像啊，有木有！ 我们可以通过Promise的构造函数来创建Promise对象，并在内部封装一个异步执行的结果。 语法： 123456789const promise = new Promise(function(resolve, reject) &#123; // ... 执行异步操作 if (/* 异步操作成功 */)&#123; resolve(value);// 调用resolve，代表Promise将返回成功的结果 &#125; else &#123; reject(error);// 调用reject，代表Promise会返回失败结果 &#125;&#125;); 这样，在promise中就封装了一段异步执行的结果。 如果我们想要等待异步执行完成，做一些事情，我们可以通过promise的then方法来实现,语法： 123promise.then(function(value)&#123; // 异步执行成功后的回调&#125;); 如果想要处理promise异步执行失败的事件，还可以跟上catch： 12345promise.then(function(value)&#123; // 异步执行成功后的回调&#125;).catch(function(error)&#123; // 异步执行失败后的回调&#125;) 示例： 12345678910111213141516171819const p = new Promise(function (resolve, reject) &#123; // 这里我们用定时任务模拟异步 setTimeout(() =&gt; &#123; const num = Math.random(); // 随机返回成功或失败 if (num &lt; 0.5) &#123; resolve(&quot;成功！num:&quot; + num) &#125; else &#123; reject(&quot;出错了！num:&quot; + num) &#125; &#125;, 300)&#125;)// 调用promisep.then(function (msg) &#123; console.log(msg);&#125;).catch(function (msg) &#123; console.log(msg);&#125;) 结果： set和map（了解）ES6提供了Set和Map的数据结构。 Set，本质与数组类似。不同在于Set中只能保存不同元素，如果元素相同会被忽略。跟java很像吧。 构造函数： 12345// Set构造函数可以接收一个数组或空let set = new Set();set.add(1);// [1]// 接收数组let set2 = new Set([2,3,4,5,5]);// 得到[2,3,4,5] 普通方法： 123456789set.add(1);// 添加set.clear();// 清空set.delete(2);// 删除指定元素set.has(2); // 判断是否存在set.keys();// 返回所有keyset.values();// 返回所有值set.entries();// 返回键值对集合// 因为set没有键值对，所有其keys、values、entries方法返回值一样的。set.size; // 元素个数。是属性，不是方法。 map，本质是与Object类似的结构。不同在于，Object强制规定key只能是字符串。而Map结构的key可以是任意对象。即： object是 &lt;string,object&gt;集合 map是&lt;object,object&gt;集合 构造函数： 12345678910111213// map接收一个数组，数组中的元素是键值对数组const map = new Map([ ['key1','value1'], ['key2','value2'],])// 或者接收一个setconst set = new Set([ ['key1','value1'], ['key2','value2'],])const map2 = new Map(set)// 或者其它mapconst map3 = new Map(map); 方法： 模块化什么是模块化模块化就是把代码进行拆分，方便重复利用。类似java中的导包：要使用一个包，必须先导包。 而JS中没有包的概念，换来的是 模块。 模块功能主要由两个命令构成：export和import。 export命令用于规定模块的对外接口， import命令用于导入其他模块提供的功能。 export比如我定义一个js文件:hello.js，里面有一个对象： 12345const util = &#123; sum(a,b)&#123; return a + b; &#125;&#125; 我可以使用export将这个对象导出： 123456const util = &#123; sum(a,b)&#123; return a + b; &#125;&#125;export util; 当然，也可以简写为： 12345export const util = &#123; sum(a,b)&#123; return a + b; &#125;&#125; export不仅可以导出对象，一切JS变量都可以导出。比如：基本类型变量、函数、数组、对象。 当要导出多个值时，还可以简写。比如我有一个文件：user.js： 123var name = "jack"var age = 21export &#123;name,age&#125; 省略名称 上面的导出代码中，都明确指定了导出的变量名，这样其它人在导入使用时就必须准确写出变量名，否则就会出错。 因此js提供了default关键字，可以对导出的变量名进行省略 例如： 123456// 无需声明对象的名字export default &#123; sum(a,b)&#123; return a + b; &#125;&#125; 这样，当使用者导入时，可以任意起名字 import使用export命令定义了模块的对外接口以后，其他 JS 文件就可以通过import命令加载这个模块。 例如我要使用上面导出的util： 1234// 导入utilimport util from 'hello.js'// 调用util中的属性util.sum(1,2) 要批量导入前面导出的name和age： 123import &#123;name, age&#125; from 'user.js'console.log(name + " , 今年"+ age +"岁了") 但是上面的代码暂时无法测试，因为浏览器目前还不支持ES6 的导入和导出功能。除非借助于工具，把ES6 的语法进行编译降级到ES5，比如Babel-cli工具 我们暂时不做测试，大家了解即可。 对象扩展ES6给Object拓展了许多新的方法，如： keys(obj)：获取对象的所有key形成的数组 values(obj)：获取对象的所有value形成的数组 entries(obj)：获取对象的所有key和value形成的二维数组。格式：[[k1,v1],[k2,v2],...] assian(dest, …src) ：将多个src对象的值 拷贝到 dest中（浅拷贝）。 数组扩展ES6给数组新增了许多方法： find(callback)：把数组中的元素逐个传递给函数callback执行，如果返回true，则返回该元素 findIndex(callback)：与find类似，不过返回的是品牌到的元素的索引 includes（callback）：与find类似，如果匹配到元素，则返回true，代表找到了。]]></content>
      <categories>
        <category>ES6</category>
      </categories>
      <tags>
        <tag>ES6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运用Swagger编写API文档]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%89%8D%E7%AB%AF%2FSwagger%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是Swagger随着互联网技术的发展，现在的网站架构基本都由原来的后端渲染，变成了：前端渲染、先后端分离的形态，而且前端技术和后端技术在各自的道路上越走越远。 前端和后端的唯一联系，变成了API接口；API文档变成了前后端开发人员联系的纽带，变得越来越重要，`swagger`就是一款让你更好的书写API文档的框架。 SwaggerEditor安装与启动（1） 下载 （2）解压swagger-editor, （3）全局安装http-server(http-server是一个简单的零配置命令行http服务器) 1npm install -g http-server （4）启动swagger-editor 1http-server swagger-editor （5）浏览器打开： http://localhost:8080 语法规则（1）固定字段 字段名 类型 描述 swagger string 必需的。使用指定的规范版本。 info Info Object 必需的。提供元数据API。 host string 主机名或ip服务API。 basePath string API的基本路径 schemes [string] API的传输协议。 值必须从列表中:”http”,”https”,”ws”,”wss”。 consumes [string] 一个MIME类型的api可以使用列表。值必须是所描述的Mime类型。 produces [string] MIME类型的api可以产生的列表。 值必须是所描述的Mime类型。 paths 路径对象 必需的。可用的路径和操作的API。 definitions 定义对象 一个对象数据类型生产和使用操作。 parameters 参数定义对象 一个对象来保存参数,可以使用在操作。 这个属性不为所有操作定义全局参数。 responses 反应定义对象 一个对象响应,可以跨操作使用。 这个属性不为所有操作定义全球响应。 externalDocs 外部文档对象 额外的外部文档。 summary string 什么操作的一个简短的总结。 最大swagger-ui可读性,这一领域应小于120个字符。 description string 详细解释操作的行为。GFM语法可用于富文本表示。 operationId string 独特的字符串用于识别操作。 id必须是唯一的在所有业务中所描述的API。 工具和库可以使用operationId来唯一地标识一个操作,因此,建议遵循通用的编程的命名约定。 deprecated boolean 声明该操作被弃用。 使用声明的操作应该没有。 默认值是false。 （2）字段类型与格式定义 普通的名字 type format 说明 integer integer int32 签署了32位 long integer int64 签署了64位 float number float double number double string string byte string byte base64编码的字符 binary string binary 任何的八位字节序列 boolean boolean date string date 所定义的full-date- - - - - -RFC3339 dateTime string date-time 所定义的date-time- - - - - -RFC3339 password string password 用来提示用户界面输入需要模糊。 基础模块-城市API文档新增城市编写新增城市的API , post提交城市实体 URL： /city Method: post 编写后的文档内容如下： 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748swagger: '2.0'info: version: "1.0.0" title: 基础模块-城市APIbasePath: /basehost: api.tensquare.compaths: /city: post: summary: 新增城市 parameters: - name: "body" in: "body" description: 城市实体类 required: true schema: $ref: '#/definitions/City' responses: 200: description: 成功 schema: $ref: '#/definitions/ApiResponse'definitions: City: type: object properties: id: type: string description: "ID" name: type: string description: "名称" ishot: type: string description: 是否热门 ApiResponse: type: object properties: flag: type: boolean description: 是否成功 code: type: integer format: int32 description: 返回码 message: type: string description: 返回信息 编辑后可以在右侧窗口看到显示的效果 修改城市URL： /city/{cityId} Method: put 编写后的文档内容如下： 代码如下： 12345678910111213141516171819/city/&#123;cityId&#125;: put: summary: 修改城市 parameters: - name: cityId in: path description: 城市ID required: true type: string - name: body in: body description: 城市 schema: $ref: '#/definitions/City' responses: 200: description: 成功响应 schema: $ref: '#/definitions/ApiResponse' 删除城市删除城市地址为/city/{cityId} ，与修改城市的地址相同，区别在于使用delete方法提交请求 代码如下： （/city/{cityId} 下增加delete） 1234567891011121314delete: summary: 根据ID删除 description: 返回是否成功 parameters: - name: cityId in: path description: 城市ID required: true type: string responses: '200': description: 成功 schema: $ref: '#/definitions/ApiResponse' 根据ID查询城市URL: /city/{cityId} Method: get 返回的内容结构为： {flag:true,code:20000, message:”查询成功”,data: {…..} } data属性返回的是city的实体类型 代码实现如下： （1）在definitions下定义城市对象的响应对象 123456789101112ApiCityResponse: type: "object" properties: code: type: "integer" format: "int32" flag: type: "boolean" message: type: "string" data: $ref: '#/definitions/City' （2）/city/{cityId} 下新增get方法API 1234567891011121314get: summary: 根据ID查询 description: 返回一个城市 parameters: - name: cityId in: path description: 城市ID required: true type: string responses: '200': description: 操作成功 schema: $ref: '#/definitions/ApiCityResponse' 城市列表URL: /city Method: get 返回的内容结构为： {flag:true,code:20000, message:”查询成功”,data:[{…..},{…..},{…..}] } data属性返回的是city的实体数组 实现步骤如下： （1）在definitions下定义城市列表对象以及相应对象 12345678910111213141516CityList: type: "array" items: $ref: '#/definitions/City'ApiCityListResponse: type: "object" properties: code: type: "integer" format: "int32" flag: type: "boolean" message: type: "string" data: $ref: '#/definitions/CityList' （2）在/city增加get 12345678get: summary: "城市全部列表" description: "返回城市全部列表" responses: 200: description: "成功查询到数据" schema: $ref: '#/definitions/ApiCityListResponse' 根据条件查询城市列表实现API效果如下: 代码如下： 123456789101112131415/city/search: post: summary: 城市列表(条件查询) parameters: - name: body in: body description: 查询条件 required: true schema: $ref: "#/definitions/City" responses: 200: description: 查询成功 schema: $ref: '#/definitions/ApiCityListResponse' 城市分页列表实现API效果如下： 实现如下： （1）在definitions下定义城市分页列表响应对象 1234567891011121314151617ApiCityPageResponse: type: "object" properties: code: type: "integer" format: "int32" flag: type: "boolean" message: type: "string" data: properties: total: type: "integer" format: "int32" rows: $ref: '#/definitions/CityList' （2）新增节点 123456789101112131415161718192021222324252627/city/search/&#123;page&#125;/&#123;size&#125;: post: summary: 城市分页列表 parameters: - name: page in: path description: 页码 required: true type: integer format: int32 - name: size in: path description: 页大小 required: true type: integer format: int32 - name: body in: body description: 查询条件 required: true schema: $ref: "#/definitions/City" responses: 200: description: 查询成功 schema: $ref: '#/definitions/ApiCityPageResponse' SwaggerUISwaggerUI是用来展示Swagger文档的界面，以下为安装步骤 （1）在本地安装nginx （2）下载SwaggerUI源码 https://swagger.io/download-swagger-ui/ （3）解压，将dist文件夹下的全部文件拷贝至 nginx的html目录 （4）启动nginx （5）浏览器打开页面 http://localhost即可看到文档页面 （6）我们将编写好的yml文件也拷贝至nginx的html目录，这样我们就可以加载我们的swagger文档了]]></content>
      <categories>
        <category>Swagger</category>
      </categories>
      <tags>
        <tag>Swagger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 Zookeeper安装]]></title>
    <url>%2F2019%2F05%2F23%2FZookeeper%2FZookeeper%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Zookeeper 部署有三种方式，单机模式、集群模式、伪集群模式，以下采用手动安装的方式部署 注意： 集群为大于等于3个奇数，如 3、5、7,不宜太多，集群机器多了选举和数据同步耗时长，不稳定。 单机模式下载进入要下载的版本的目录，选择 .tar.gz 文件下载，下载链接：http://archive.apache.org/dist/zookeeper/ 安装注意： 需要先安装 Java 使用 tar 解压要安装的目录即可，以 3.4.13 版本为例，解压到 /usr/local/zookeeper-3.4.13 1tar -zxvf zookeeper-3.4.13.tar.gz -C /usr/local 配置在根目录下创建 data 和 logs 两个目录用于存储数据和日志 123cd /usr/local/zookeeper-3.4.13mkdir datamkdir logs 在 conf 目录下新建 zoo.cfg 文件，写入以下内容保存 1234tickTime=2000dataDir=/usr/local/zookeeper-3.4.13/datadataLogDir=/usr/local/zookeeper-3.4.13/logsclientPort=2181 启动和停止进入 bin 目录，启动、停止、重启和查看当前节点状态 1234./zkServer.sh start./zkServer.sh stop./zkServer.sh restart./zkServer.sh status 伪集群模式伪集群模式就是在同一主机启动多个 zookeeper 并组成集群，下边以在 192.168.10.134 主机上创 3 个 zookeeper 组集群为例。 将通过单机模式安装的 zookeeper，复制成 zookeeper1/zookeeper2/zookeeper3 三份 zookeeper1 修改配置文件 123456789tickTime=2000dataDir=/usr/local/zookeeper1/datadataLogDir=/usr/local/zookeeper1/logsclientPort=2181initLimit=5syncLimit=2server.1=192.168.10.134:2888:3888server.2=192.168.10.134:4888:5888server.3=192.168.10.134:6888:7888 设置服务器 ID 1echo '1' &gt; data/myid zookeeper2 修改配置文件 123456789tickTime=2000dataDir=/usr/local/zookeeper2/datadataLogDir=/usr/local/zookeeper2/logsclientPort=2181initLimit=5syncLimit=2server.1=192.168.10.134:2888:3888server.2=192.168.10.134:4888:5888server.3=192.168.10.134:6888:7888 设置服务器 ID 1echo '2' &gt; data/myid zookeeper3 修改配置文件 123456789tickTime=2000dataDir=/usr/local/zookeeper3/datadataLogDir=/usr/local/zookeeper3/logsclientPort=2181initLimit=5syncLimit=2server.1=192.168.10.134:2888:3888server.2=192.168.10.134:4888:5888server.3=192.168.10.134:6888:7888 设置服务器 ID 1echo '3' &gt; data/myid 启动和停止分别启动服务器，顺序无所谓 1234./zkServer.sh start./zkServer.sh stop./zkServer.sh restart./zkServer.sh status 集群模式集群模式就是在不同主机上安装 zookeeper 然后组成集群的模式，操作步骤同上，此处不再赘述。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba路由网关全局过滤功能]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E8%B7%AF%E7%94%B1%E7%BD%91%E5%85%B3%E7%9A%84%E5%85%A8%E5%B1%80%E8%BF%87%E6%BB%A4%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[概述全局过滤器作用于所有的路由，不需要单独配置，我们可以用它来实现很多统一化处理的业务需求，比如权限认证，IP 访问限制等等。 注意：截止博客发表时间 2019 年 01 月 10 日，Spring Cloud Gateway 正式版为 2.0.2 其文档并不完善，并且有些地方还要重新设计，这里仅提供一个基本的案例 详见：Spring Cloud Gateway Documentation 生命周期 Spring Cloud Gateway 基于 Project Reactor 和 WebFlux，采用响应式编程风格，打开它的 Filter 的接口 GlobalFilter 你会发现它只有一个方法 filter 创建全局过滤器实现 GlobalFilter, Ordered 接口并在类上增加 @Component 注解就可以使用过滤功能了，非常简单方便 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package club.codeopen.spring.cloud.gateway.filters;/** * @author by cheng * @Classname AuthFilter * @Description * @Date 2019/1/13 11:36 */import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import com.google.common.collect.Maps;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.Ordered;import org.springframework.core.io.buffer.DataBuffer;import org.springframework.http.HttpStatus;import org.springframework.http.server.reactive.ServerHttpResponse;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;import java.util.Map;/** * 鉴权过滤器 */@Componentpublic class AuthFilter implements GlobalFilter, Ordered &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; String token = exchange.getRequest().getQueryParams().getFirst("token"); if (token == null || token.isEmpty()) &#123; ServerHttpResponse response = exchange.getResponse(); // 封装错误信息 Map&lt;String, Object&gt; responseData = Maps.newHashMap(); responseData.put("code", 401); responseData.put("message", "非法请求"); responseData.put("cause", "Token is empty"); try &#123; // 将信息转换为 JSON ObjectMapper objectMapper = new ObjectMapper(); byte[] data = objectMapper.writeValueAsBytes(responseData); // 输出错误信息到页面 DataBuffer buffer = response.bufferFactory().wrap(data); response.setStatusCode(HttpStatus.UNAUTHORIZED); response.getHeaders().add("Content-Type", "application/json;charset=UTF-8"); return response.writeWith(Mono.just(buffer)); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; &#125; return chain.filter(exchange); &#125; /** * 设置过滤器的执行顺序 * @return */ @Override public int getOrder() &#123; return Ordered.LOWEST_PRECEDENCE; &#125;&#125; 测试过滤器浏览器访问：http://localhost:9000/nacos-consumer/echo/app/name 网页显示 浏览器访问：http://localhost:9000/nacos-consumer/echo/app/name?token=123456 网页显示 1Hello Nacos Discovery nacos-consumer i am from port 8082 附：Spring Cloud Gateway BenchmarkSpring 官方人员提供的网关基准测试报告 GitHub Proxy Avg Latency Avg Req/Sec/Thread gateway 6.61ms 3.24k linkered 7.62ms 2.82k zuul 12.56ms 2.09k none 2.09ms 11.77k 说明 这里的 Zuul 为 1.x 版本，是一个基于阻塞 IO 的 API Gateway Zuul 已经发布了 Zuul 2.x，基于 Netty，非阻塞的，支持长连接，但 Spring Cloud 暂时还没有整合计划 Linkerd 基于 Scala 实现的、目前市面上仅有的生产级别的 Service Mesh（其他诸如 Istio、Conduit 暂时还不能用于生产）。]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba路由网关]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E8%B7%AF%E7%94%B1%E7%BD%91%E5%85%B3%2F</url>
    <content type="text"><![CDATA[什么是 Spring Cloud GatewaySpring Cloud Gateway 是 Spring 官方基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，Spring Cloud Gateway 旨在为微服务架构提供一种简单而有效的统一的 API 路由管理方式。Spring Cloud Gateway 作为 Spring Cloud 生态系中的网关，目标是替代 Netflix ZUUL，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。 Spring Cloud Gateway 功能特征 基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0 动态路由 Predicates 和 Filters 作用于特定路由 集成 Hystrix 断路器 集成 Spring Cloud DiscoveryClient 易于编写的 Predicates 和 Filters 限流 路径重写 Spring Cloud Gateway 工程流程 客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。 过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（pre）或之后（post）执行业务逻辑 POM1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-alibaba-demo-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-gateway&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-cloud-alibaba-demo-gateway&lt;/name&gt; &lt;url&gt;https://www.codeopen.club&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons Begin --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.spring.cloud.gateway.GatewayApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354spring: application: # 应用名称 name: spring-gateway cloud: # 使用 Naoos 作为服务注册发现 nacos: discovery: server-addr: 127.0.0.1:8848 # 使用 Sentinel 作为熔断器 sentinel: transport: port: 8721 dashboard: localhost:8080 # 路由网关配置 gateway: # 设置与服务注册发现组件结合，这样可以采用服务名的路由策略 discovery: locator: enabled: true # 配置路由规则 routes: # 采用自定义路由 ID（有固定用法，不同的 id 有不同的功能，详见：https://cloud.spring.io/spring-cloud-gateway/2.0.x/single/spring-cloud-gateway.html#gateway-route-filters） - id: NACOS-CONSUMER # 采用 LoadBalanceClient 方式请求，以 lb:// 开头，后面的是注册在 Nacos 上的服务名 uri: lb://nacos-consumer # Predicate 翻译过来是“谓词”的意思，必须，主要作用是匹配用户的请求，有很多种用法 predicates: # Method 方法谓词，这里是匹配 GET 和 POST 请求 - Method=GET,POST - id: NACOS-CONSUMER-FEIGN uri: lb://nacos-consumer-feign predicates: - Method=GET,POSTserver: port: 9000# 目前无效feign: sentinel: enabled: true# 目前无效management: endpoints: web: exposure: include: "*"# 配置日志级别，方别调试logging: level: org.springframework.cloud.gateway: debug 注意：请仔细阅读注释 测试访问依次运行 Nacos 服务、NacosProviderApplication、NacosConsumerApplication、NacosConsumerFeignApplication、GatewayApplication 打开浏览器访问：http://localhost:9000/nacos-consumer/echo/app/name 浏览器显示 1Hello Nacos Discovery nacos-consumer i am from port 8082 1 打开浏览器访问：http://localhost:9000/nacos-consumer-feign/echo/hi 浏览器显示 1Hello Nacos Discovery Hi Feign i am from port 8082 1 注意：请求方式是 http://路由网关IP:路由网关Port/服务名/** 至此说明 Spring Cloud Gateway 的路由功能配置成功]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba熔断器仪表盘]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E7%86%94%E6%96%AD%E5%99%A8%E4%BB%AA%E8%A1%A8%E7%9B%98%2F</url>
    <content type="text"><![CDATA[Sentinel 控制台Sentinel 控制台提供一个轻量级的控制台，它提供机器发现、单机资源实时监控、集群资源汇总，以及规则管理的功能。您只需要对应用进行简单的配置，就可以使用这些功能。 注意: 集群资源汇总仅支持 500 台以下的应用集群，有大概 1 - 2 秒的延时。 下载并打包 可以从最新版本的源码自行构建 Sentinel 控制台 12345# 下载源码git clone https://github.com/alibaba/Sentinel.git# 编译打包mvn clean package 注：下载依赖时间较长，请耐心等待… 也可以从 release 页面 下载最新版本的控制台 jar 包 启动控制台Sentinel 控制台是一个标准的 SpringBoot 应用，以 SpringBoot 的方式运行 jar 包即可。 12345# 自行构建Sentinel控制台，控制台jar包位置cd sentinel-dashboard\targe # 运行jar包java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar 如若 8080 端口冲突，可使用 -Dserver.port=新端口 进行设置。 访问服务打开浏览器访问：http://localhost:8080/#/dashboard/home 测试 Sentinel使用之前的 Feign 客户端，application.yml 完整配置如下： 123456789101112131415161718192021222324spring: application: name: nacos-consumer-feign cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: port: 8720 dashboard: localhost:8080server: port: 9092feign: sentinel: enabled: truemanagement: endpoints: web: exposure: include: "*" 注：由于 8719 端口已被 sentinel-dashboard 占用，故这里修改端口号为 8720；不修改也能注册，会自动帮你在端口号上 + 1； 打开浏览器访问：http://localhost:8080/#/dashboard/home 此时会多一个名为 nacos-consumer-feign 的服务]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloud alibaba统一的依赖管理]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E7%BB%9F%E4%B8%80%E7%9A%84%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Spring Cloud Alibaba 项目都是基于 Spring Cloud，而 Spring Cloud 项目又是基于 Spring Boot 进行开发，并且都是使用 Maven 做项目管理工具。在实际开发中，我们一般都会创建一个依赖管理项目作为 Maven 的 Parent 项目使用，这样做可以极大的方便我们对 Jar 包版本的统一管理。 创建依赖管理项目创建一个工程名为 spring-cloud-alibaba-demo-dependencies 的项目，pom.xml 配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;spring-cloud-alibaba-demo-dependencies&lt;/name&gt; &lt;url&gt;https://www.codeopen.club&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;properties&gt; &lt;!-- Environment Settings --&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- Spring Settings --&gt; &lt;spring-cloud.version&gt;Finchley.SR2&lt;/spring-cloud.version&gt; &lt;spring-cloud-alibaba.version&gt;0.2.1.RELEASE&lt;/spring-cloud-alibaba.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- Compiler 插件, 设定 JDK 版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包 jar 文件时，配置 manifest 文件，加入 lib 包的 jar 依赖 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Add directory entries --&gt; &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt; &lt;addDefaultSpecificationEntries&gt;true&lt;/addDefaultSpecificationEntries&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- resource --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- install --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- clean --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- ant --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- dependency --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- Java Document Generate --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- YUI Compressor (CSS/JS压缩) --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;yuicompressor-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;compress&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;jswarn&gt;false&lt;/jswarn&gt; &lt;nosuffix&gt;true&lt;/nosuffix&gt; &lt;linebreakpos&gt;30000&lt;/linebreakpos&gt; &lt;force&gt;true&lt;/force&gt; &lt;includes&gt; &lt;include&gt;**/*.js&lt;/include&gt; &lt;include&gt;**/*.css&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.min.js&lt;/exclude&gt; &lt;exclude&gt;**/*.min.css&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!-- 资源文件配置 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos-s&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/project&gt; parent：继承了 Spring Boot 的 Parent，表示我们是一个 Spring Boot 工程 package：pom，表示该项目仅当做依赖项目，没有具体的实现代码 spring-cloud-alibaba-dependencies：在 properties 配置中预定义了版本号为 0.2.1.RELEASE ，表示我们的 Spring Cloud Alibaba 对应的是 Spring Cloud Finchley 版本 build：配置了项目所需的各种插件 repositories：配置项目下载依赖时的第三方库 依赖版本说明项目的最新版本是 0.2.1.RELEASE 和 0.1.1.RELEASE，版本 0.2.1.RELEASE 对应的是 Spring Cloud Finchley 版本，版本 0.1.1.RELEASE 对应的是 Spring Cloud Edgware 版本]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba熔断器]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E7%86%94%E6%96%AD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[概述在微服务架构中，根据业务来拆分成一个个的服务，服务与服务之间可以通过 RPC 相互调用，在 Spring Cloud 中可以用 RestTemplate + LoadBalanceClient 和 Feign 来调用。为了保证其高可用，单个服务通常会集群部署。由于网络原因或者自身的原因，服务并不能保证 100% 可用，如果单个服务出现问题，调用这个服务就会出现线程阻塞，此时若有大量的请求涌入，Servlet 容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的 “雪崩” 效应。 为了解决这个问题，业界提出了熔断器模型。 阿里巴巴开源了 Sentinel 组件，实现了熔断器模式，Spring Cloud 对这一组件进行了整合。在微服务架构中，一个请求需要调用多个服务是非常常见的，如下图： 较底层的服务如果出现故障，会导致连锁故障。当对特定的服务的调用的不可用达到一个阀值熔断器将会被打开。 熔断器打开后，为了避免连锁故障，通过 fallback 方法可以直接返回一个固定值。 什么是 Sentinel随着微服务的流行，服务和服务之间的稳定性变得越来越重要。 Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性Sentinel 的特征 丰富的应用场景： Sentinel 承接了阿里巴巴近 10 年的 双十一大促流量 的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、实时熔断下游不可用应用等。 完备的实时监控： Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。 广泛的开源生态： Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。 完善的 SPI 扩展点： Sentinel 提供简单易用、完善的 SPI 扩展点。您可以通过实现扩展点，快速的定制逻辑。例如定制规则管理、适配数据源等 Feign 中使用 Sentinel如果要在您的项目中引入 Sentinel，使用 group ID 为 org.springframework.cloud 和 artifact ID 为 spring-cloud-starter-alibaba-sentinel 的 starter。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; Sentinel 适配了 Feign 组件。但默认是关闭的。需要在配置文件中配置打开它，在配置文件增加以下代码： 123feign: sentinel: enabled: true 在 Service 中增加 fallback 指定类12345678910111213141516171819package club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.service;import club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.service.fallback.EchoServiceFallback;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;/** * @author by cheng * @Classname EchoService * @Description * @Date 2019/1/13 9:52 */@FeignClient(value = "nacos-provider", fallback = EchoServiceFallback.class)public interface EchoService &#123; @GetMapping(value = "/echo/&#123;message&#125;") String echo(@PathVariable("message") String message);&#125; 创建熔断器类并实现对应的 Feign 接口123456789101112131415161718package club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.service.fallback;import club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.service.EchoService;import org.springframework.stereotype.Component;/** * @author by cheng * @Classname EchoServiceFallback * @Description * @Date 2019/1/13 10:08 */@Componentpublic class EchoServiceFallback implements EchoService &#123; @Override public String echo(String message) &#123; return "echo fallback"; &#125;&#125; 测试熔断器此时我们关闭服务提供者，再次请求 http://localhost:9092/echo/hi 浏览器会显示： 1echo fallback]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba服务消费者（Feign）]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%EF%BC%88Feign%EF%BC%89%2F</url>
    <content type="text"><![CDATA[概述Feign 是一个声明式的伪 Http 客户端，它使得写 Http 客户端变得更简单。使用 Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用 Feign 注解和 JAX-RS 注解。Feign 支持可插拔的编码器和解码器。Feign 默认集成了 Ribbon，Nacos 也很好的兼容了 Feign，默认实现了负载均衡的效果 Feign 采用的是基于接口的注解 Feign 整合了 ribbon POM创建一个工程名为 spring-cloud-alibaba-demo-nacos-consumer-feign 的服务消费者项目，pom.xml 配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-alibaba-demo-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-nacos-consumer-feign&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-cloud-alibaba-demo-nacos-consumer-feign&lt;/name&gt; &lt;url&gt;https://www.codeopen.club&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.NacosConsumerFeignApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 主要增加了 org.springframework.cloud:spring-cloud-starter-openfeign 依赖 application.yml12345678910111213141516spring: application: name: nacos-consumer-feign cloud: nacos: discovery: server-addr: 127.0.0.1:8848server: port: 9092management: endpoints: web: exposure: include: "*" Application通过 @EnableFeignClients 注解开启 Feign 功能 123456789101112131415161718192021package club.codeopen.spring.cloud.alibaba.nacos.consumer.feign;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.openfeign.EnableFeignClients;/** * @author by cheng * @Classname NacosConsumerFeignApplication * @Description * @Date 2019/1/13 9:51 */@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class NacosConsumerFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosConsumerFeignApplication.class, args); &#125;&#125; 创建 Feign 接口通过 @FeignClient(&quot;服务名&quot;) 注解来指定调用哪个服务。代码如下： 123456789101112131415161718package club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.service;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;/** * @author by cheng * @Classname EchoService * @Description * @Date 2019/1/13 9:52 */@FeignClient(value = "nacos-provider")public interface EchoService &#123; @GetMapping(value = "/echo/&#123;message&#125;") String echo(@PathVariable("message") String message);&#125; Controller123456789101112131415161718192021222324package club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.controller;import club.codeopen.spring.cloud.alibaba.nacos.consumer.feign.service.EchoService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;/** * @author by cheng * @Classname NacosConsumerFeignController * @Description * @Date 2019/1/13 9:52 */@RestControllerpublic class NacosConsumerFeignController &#123; @Autowired private EchoService echoService; @GetMapping(value = "/echo/hi") public String echo() &#123; return echoService.echo("Hi Feign"); &#125;&#125; 启动工程通过浏览器访问 http://localhost:8848/nacos，即 Nacos Server 网址 你会发现多了一个名为 nacos-consumer-feign 的服务 这时打开 http://localhost:9092/echo/hi ，你会在浏览器上看到： 1Hello Nacos Discovery Hi Feign 测试负载均衡 启动多个 consumer-provider 实例，效果图如下： 修改 consumer-provider 项目中的 Controller 代码，用于确定负载均衡生效 12345678910111213141516171819202122232425262728293031323334package club.codeopen.spring.cloud.alibaba.nacos.provider;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;/** * @author by cheng * @Classname NacosProviderApplication * @Description * @Date 2019/1/13 9:24 */@SpringBootApplication@EnableDiscoveryClientpublic class NacosProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosProviderApplication.class, args); &#125; @Value("$&#123;server.port&#125;") private String port; @RestController public class EchoController &#123; @GetMapping(value = "/echo/&#123;message&#125;") public String echo(@PathVariable String message) &#123; return "Hello Nacos Discovery " + message + " i am from port " + port; &#125; &#125;&#125; 在浏览器上多次访问 http://localhost:9092/echo/hi ，浏览器交替显示： 12Hello Nacos Discovery Hi Feign i am from port 8081Hello Nacos Discovery Hi Feign i am from port 8082]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
        <category>Feign</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
        <tag>Feign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba服务消费者]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%2F</url>
    <content type="text"><![CDATA[概述服务消费者的创建与服务提供者大同小异，这里采用最原始的一种方式，即显示的使用 LoadBalanceClient 和 RestTemplate 结合的方式来访问 POM创建一个工程名为 spring-cloud-alibaba-demo-nacos-consumer 的服务消费者项目，pom.xml 配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-alibaba-demo-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-nacos-consumer&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-cloud-alibaba-demo-nacos-consumer&lt;/name&gt; &lt;url&gt;https://www.codeopen.club&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.spring.cloud.alibaba.nacos.consumer.NacosConsumerApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml12345678910111213141516spring: application: name: nacos-consumer cloud: nacos: discovery: server-addr: 127.0.0.1:8848server: port: 9091management: endpoints: web: exposure: include: "*" Application12345678910111213141516171819package club.codeopen.spring.cloud.alibaba.nacos.consumer;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;/** * @author by cheng * @Classname NacosConsumerApplication * @Description * @Date 2019/1/13 9:38 */@SpringBootApplication@EnableDiscoveryClientpublic class NacosConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosConsumerApplication.class, args); &#125;&#125; Configuration创建一个名为 NacosConsumerConfiguration 的 Java 配置类，主要作用是为了注入 RestTemplate 1234567891011121314151617181920package club.codeopen.spring.cloud.alibaba.nacos.consumer.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.client.RestTemplate;/** * @author by cheng * @Classname NacosConsumerConfiguration * @Description * @Date 2019/1/13 9:39 */@Configurationpublic class NacosConsumerConfiguration &#123; @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; Controller创建一个名为 NacosConsumerController 测试用的 Controller 123456789101112131415161718192021222324252627282930313233343536package club.codeopen.spring.cloud.alibaba.nacos.consumer.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;/** * @author by cheng * @Classname NacosConsumerController * @Description * @Date 2019/1/13 9:39 */@RestControllerpublic class NacosConsumerController &#123; @Autowired private LoadBalancerClient loadBalancerClient; @Autowired private RestTemplate restTemplate; @Value("$&#123;spring.application.name&#125;") private String appName; @GetMapping(value = "/echo/app/name") public String echo() &#123; //使用 LoadBalanceClient 和 RestTemplate 结合的方式来访问 ServiceInstance serviceInstance = loadBalancerClient.choose("nacos-provider"); String url = String.format("http://%s:%s/echo/%s", serviceInstance.getHost(), serviceInstance.getPort(), appName); return restTemplate.getForObject(url, String.class); &#125;&#125; 启动工程通过浏览器访问 http://localhost:8848/nacos，即 Nacos Server 网址 你会发现多了一个名为 nacos-consumer 的服务 这时打开 http://localhost:9091/echo/app/name ，你会在浏览器上看到： 1Hello Nacos Discovery nacos-consumer 服务的端点检查通过浏览器访问 http://localhost:9091/actuator/nacos-discovery 你会在浏览器上看到：]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务注册与发现之Nacos]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0%E4%B9%8BNacos%2F</url>
    <content type="text"><![CDATA[什么是 NacosNacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。 Nacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。 Nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。 基本架构及概念 服务 (Service)服务是指一个或一组软件功能（例如特定信息的检索或一组操作的执行），其目的是不同的客户端可以为不同的目的重用（例如通过跨进程的网络调用）。Nacos 支持主流的服务生态，如 Kubernetes Service、gRPC|Dubbo RPC Service 或者 Spring Cloud RESTful Service 服务注册中心 (Service Registry)服务注册中心，它是服务，其实例及元数据的数据库。服务实例在启动时注册到服务注册表，并在关闭时注销。服务和路由器的客户端查询服务注册表以查找服务的可用实例。服务注册中心可能会调用服务实例的健康检查 API 来验证它是否能够处理请求 服务元数据 (Service Metadata)服务元数据是指包括服务端点(endpoints)、服务标签、服务版本号、服务实例权重、路由规则、安全策略等描述服务的数据 服务提供方 (Service Provider)是指提供可复用和可调用服务的应用方 服务消费方 (Service Consumer)是指会发起对某个服务调用的应用方 配置 (Configuration)在系统开发过程中通常会将一些需要变更的参数、变量等从代码中分离出来独立管理，以独立的配置文件的形式存在。目的是让静态的系统工件或者交付物（如 WAR，JAR 包等）更好地和实际的物理运行环境进行适配。配置管理一般包含在系统部署的过程中，由系统管理员或者运维人员完成这个步骤。配置变更是调整系统运行时的行为的有效手段之一 配置管理 (Configuration Management)在数据中心中，系统中所有配置的编辑、存储、分发、变更管理、历史版本管理、变更审计等所有与配置相关的活动统称为配置管理 名字服务 (Naming Service)提供分布式系统中所有对象(Object)、实体(Entity)的“名字”到关联的元数据之间的映射管理服务，例如 ServiceName -&gt; Endpoints Info, Distributed Lock Name -&gt; Lock Owner/Status Info, DNS Domain Name -&gt; IP List, 服务发现和 DNS 就是名字服务的2大场景 配置服务 (Configuration Service)在服务或者应用运行过程中，提供动态配置或者元数据以及配置管理的服务提供者 下载安装准备环境Nacos 依赖 Java 环境来运行。如果您是从代码开始构建并运行 Nacos，还需要为此配置 Maven 环境，请确保是在以下版本环境中安装使用: 64 bit OS，支持 Linux/Unix/Mac/Windows，推荐选用 Linux/Unix/Mac。 64 bit JDK 1.8+ Maven 3.2.x+ 下载并安装123456# 下载源码git clone https://github.com/alibaba/nacos.git# 安装到本地仓库cd nacos/mvn -Prelease-nacos clean install -U 注：下载依赖时间较长，请耐心等待… 启动服务1234567cd distribution/target/nacos-server-0.7.0/nacos/bin# Linux./startup.sh -m standalone# Windowsstartup.cmd 访问服务打开浏览器访问：http://localhost:8848/nacos/index.html]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
        <category>Nacos</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
        <tag>Nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba服务提供者]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%E6%9C%8D%E5%8A%A1%E6%8F%90%E4%BE%9B%E8%80%85%2F</url>
    <content type="text"><![CDATA[概述通过一个简单的示例来感受一下如何将服务注册到 Nacos，其实和 Eureka 没有太大差别 POM创建一个工程名为 spring-cloud-alibaba-demo-nacos-provider 的服务提供者项目，pom.xml 配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-alibaba-demo-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-nacos-provider&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-cloud-alibaba-demo-nacos-provider&lt;/name&gt; &lt;url&gt;https://www.codeopen.club&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.spring.cloud.alibaba.nacos.provider.NacosProviderApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml12345678910111213141516spring: application: name: nacos-provider cloud: nacos: discovery: server-addr: 127.0.0.1:8848server: port: 8081management: endpoints: web: exposure: include: "*" Application通过 @EnableDiscoveryClient 注解表明是一个 Nacos 客户端，该注解是 Spring Cloud 提供的原生注解 123456789101112131415161718192021222324252627282930package club.codeopen.spring.cloud.alibaba.nacos.provider;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;/** * @author by cheng * @Classname NacosProviderApplication * @Description * @Date 2019/1/13 9:24 */@SpringBootApplication@EnableDiscoveryClientpublic class NacosProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(NacosProviderApplication.class, args); &#125; @RestController public class EchoController &#123; @GetMapping(value = "/echo/&#123;message&#125;") public String echo(@PathVariable String message) &#123; return "Hello Nacos Discovery " + message; &#125; &#125;&#125; 启动工程通过浏览器访问 http://localhost:8848/nacos，即 Nacos Server 网址 你会发现一个服务已经注册在服务中了，服务名为 nacos-provider 这时打开 http://localhost:8081/echo/hi ，你会在浏览器上看到： 1Hello Nacos Discovery hi 服务的端点检查spring-cloud-starter-alibaba-nacos-discovery 在实现的时候提供了一个 EndPoint, EndPoint 的访问地址为 http://ip:port/actuator/nacos-discovery。 EndPoint 的信息主要提供了两类: 121、subscribe: 显示了当前有哪些服务订阅者2、NacosDiscoveryProperties: 显示了当前服务实例关于 Nacos 的基础配置 12 通过浏览器访问 http://localhost:8081/actuator/nacos-discovery 你会在浏览器上看到： 附：Nacos Starter 更多配置项信息 配置项 Key 默认值 说明 服务端地址 spring.cloud.nacos.discovery.server-addr 无 Nacos Server 启动监听的ip地址和端口 服务名 spring.cloud.nacos.discovery.service ${spring.application.name} 给当前的服务命名 权重 spring.cloud.nacos.discovery.weight 1 取值范围 1 到 100，数值越大，权重越大 网卡名 spring.cloud.nacos.discovery.network-interface 无 当IP未配置时，注册的IP为此网卡所对应的IP地址，如果此项也未配置，则默认取第一块网卡的地址 注册的IP地址 spring.cloud.nacos.discovery.ip 无 优先级最高 注册的端口 spring.cloud.nacos.discovery.port -1 默认情况下不用配置，会自动探测 命名空间 spring.cloud.nacos.discovery.namespace 无 常用场景之一是不同环境的注册的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。 AccessKey spring.cloud.nacos.discovery.access-key 无 当要上阿里云时，阿里云上面的一个云账号名 SecretKey spring.cloud.nacos.discovery.secret-key 无 当要上阿里云时，阿里云上面的一个云账号密码 Metadata spring.cloud.nacos.discovery.metadata 无 使用 Map 格式配置，用户可以根据自己的需要自定义一些和服务相关的元数据信息 日志文件名 spring.cloud.nacos.discovery.log-name 无 接入点 spring.cloud.nacos.discovery.enpoint UTF-8 地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址 是否集成 Ribbon ribbon.nacos.enabled true]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%2F</url>
    <content type="text"><![CDATA[Spring Cloud Alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务。 依托 Spring Cloud Alibaba，您只需要添加一些注解和少量配置，就可以将 Spring Cloud 应用接入阿里微服务解决方案，通过阿里中间件来迅速搭建分布式应用系统。 参考文档 请查看 WIKI 主要功能 服务限流降级：默认支持 Servlet、Feign、RestTemplate、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。 服务注册与发现：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持。 分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新。 消息驱动能力：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。 阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。 分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。 更多功能请参考 Roadmap。 组件Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。 Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。 RocketMQ：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。 Alibaba Cloud ACM：一款在分布式架构环境中对应用配置进行集中管理和推送的应用配置中心产品。 Alibaba Cloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。 Alibaba Cloud SchedulerX: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。 更多组件请参考 Roadmap。 如何构建 master 分支对应的是 Spring Cloud Finchley，最低支持 JDK 1.8。 1.x 分支对应的是 Spring Cloud Edgware，最低支持 JDK 1.7。 Spring Cloud 使用 Maven 来构建，最快的使用方式是将本项目clone到本地，然后执行以下命令： 1./mvnw install 执行完毕后，项目将被安装到本地 Maven 仓库。 如何使用如何引入依赖项目的最新版本是 0.2.1.RELEASE 和 0.1.1.RELEASE，版本 0.2.1.RELEASE 对应的是 Spring Cloud Finchley 版本，版本 0.1.1.RELEASE 对应的是 Spring Cloud Edgware 版本。 如果需要使用已发布的版本，在 dependencyManagement 中添加如下配置。 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;0.2.1.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 然后再 dependencies 中添加自己所需使用的依赖即可使用。 如果您想体验最新的 BUILD-SNAPSHOT 的新功能，则可以将版本换成最新的版本，但是需要在 pom.xml 中配置 Spring BUILDSNAPSHOT 仓库，注意: SNAPSHOT 版本随时可能更新 12345678910&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-snapshot&lt;/id&gt; &lt;name&gt;Spring Snapshot Repository&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 演示 Demo为了演示如何使用，Spring Cloud Alibaba 项目包含了一个子模块spring-cloud-alibaba-examples。此模块中提供了演示用的 example ，您可以阅读对应的 example 工程下的 readme 文档，根据里面的步骤来体验。 Example 列表： Sentinel Example Nacos Config Example Nacos Discovery Example RocketMQ Example Alibaba Cloud OSS Example Alibaba Cloud ANS Example Alibaba Cloud ACM Example Alibaba Cloud SchedulerX Example 版本管理规范项目的版本号格式为 x.x.x 的形式，其中 x 的数值类型为数字，从0开始取值，且不限于 0~9 这个范围。项目处于孵化器阶段时，第一位版本号固定使用0，即版本号为 0.x.x 的格式。 由于 Spring Boot 1 和 Spring Boot 2 在 Actuator 模块的接口和注解有很大的变更，且 spring-cloud-commons 从 1.x.x 版本升级到 2.0.0 版本也有较大的变更，因此我们使用了两个不同分支来分别支持 Spring Boot 1 和 Spring Boot 2: 0.1.x 版本适用于 Spring Boot 1 0.2.x 版本适用于 Spring Boot 2 项目孵化阶段，项目版本升级机制如下： 功能改动的升级会增加第三位版本号的数值，例如 0.1.0 的下一个版本为0.1.1。]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba 合集]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%20%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[Spring Cloud AlibabaSpring Cloud Alibaba 致力于提供微服务开发的一站式解决方案。此项目包含开发分布式应用微服务的必需组件，方便开发者通过 Spring Cloud 编程模型轻松使用这些组件来开发分布式应用服务。 依托 Spring Cloud Alibaba，您只需要添加一些注解和少量配置，就可以将 Spring Cloud 应用接入阿里微服务解决方案，通过阿里中间件来迅速搭建分布式应用系统。 参考文档 请查看 WIKI 。 创建统一的依赖管理概述 温馨提示 项目的最新版本是 0.2.1.RELEASE 和 0.1.1.RELEASE，版本 0.2.1.RELEASE 对应的是 Spring Cloud Finchley 版本，版本 0.1.1.RELEASE 对应的是 Spring Cloud Edgware 版本。 故在选择 Spring Boot 版本时不要使用 2.1.0 及以上版本（因为 2.1.x 版本必须使用 Spring Cloud Greenwich，俗称 G 版），请使用官方 Demo 中使用的 2.0.5.RELEASE，以免发生意想不到的问题（比如服务无法注册到服务器） Spring Cloud Alibaba 项目都是基于 Spring Cloud，而 Spring Cloud 项目又是基于 Spring Boot 进行开发，并且都是使用 Maven 做项目管理工具。在实际开发中，我们一般都会创建一个依赖管理项目作为 Maven 的 Parent 项目使用，这样做可以极大的方便我们对 Jar 包版本的统一管理。 创建依赖管理项目创建一个工程名为 cloud-alibaba-dependencies 的项目，pom.xml 配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;properties&gt; &lt;!-- Environment Settings --&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;!-- Spring Settings --&gt; &lt;spring-cloud.version&gt;Finchley.SR2&lt;/spring-cloud.version&gt; &lt;spring-cloud-alibaba.version&gt;0.2.1.RELEASE&lt;/spring-cloud-alibaba.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud-alibaba.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- Compiler 插件, 设定 JDK 版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 打包 jar 文件时，配置 manifest 文件，加入 lib 包的 jar 依赖 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!-- Add directory entries --&gt; &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt; &lt;addDefaultSpecificationEntries&gt;true&lt;/addDefaultSpecificationEntries&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- resource --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- install --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- clean --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- ant --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!-- dependency --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;!-- 资源文件配置 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;sonatype-repos-s&lt;/id&gt; &lt;name&gt;Sonatype Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;aliyun-repos&lt;/id&gt; &lt;name&gt;Aliyun Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/project&gt; parent：继承了 Spring Boot 的 Parent，表示我们是一个 Spring Boot 工程 package：pom，表示该项目仅当做依赖项目，没有具体的实现代码 spring-cloud-alibaba-dependencies：在 properties 配置中预定义了版本号为 0.2.1.RELEASE ，表示我们的 Spring Cloud Alibaba 对应的是 Spring Cloud Finchley 版本 build：配置了项目所需的各种插件 repositories：配置项目下载依赖时的第三方库 服务注册与发现概述在 Spring Cloud Netflix 阶段我们采用 Eureka 做作为我们的服务注册与发现服务器，现利用 Spring Cloud Alibaba 提供的 Nacos 组件替代该方案。 Nacos 官网 什么是 NacosNacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。 Nacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。 Nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。 基本架构及概念 服务 (Service)服务是指一个或一组软件功能（例如特定信息的检索或一组操作的执行），其目的是不同的客户端可以为不同的目的重用（例如通过跨进程的网络调用）。Nacos 支持主流的服务生态，如 Kubernetes Service、gRPC|Dubbo RPC Service 或者 Spring Cloud RESTful Service. 服务注册中心 (Service Registry)服务注册中心，它是服务，其实例及元数据的数据库。服务实例在启动时注册到服务注册表，并在关闭时注销。服务和路由器的客户端查询服务注册表以查找服务的可用实例。服务注册中心可能会调用服务实例的健康检查 API 来验证它是否能够处理请求。 服务元数据 (Service Metadata)服务元数据是指包括服务端点(endpoints)、服务标签、服务版本号、服务实例权重、路由规则、安全策略等描述服务的数据 服务提供方(Service Provider)是指提供可复用和可调用服务的应用方 服务消费方 (Service Consumer)是指会发起对某个服务调用的应用方 配置 (Configuration)在系统开发过程中通常会将一些需要变更的参数、变量等从代码中分离出来独立管理，以独立的配置文件的形式存在。目的是让静态的系统工件或者交付物（如 WAR，JAR 包等）更好地和实际的物理运行环境进行适配。配置管理一般包含在系统部署的过程中，由系统管理员或者运维人员完成这个步骤。配置变更是调整系统运行时的行为的有效手段之一 配置管理 (Configuration Management)在数据中心中，系统中所有配置的编辑、存储、分发、变更管理、历史版本管理、变更审计等所有与配置相关的活动统称为配置管理 名字服务 (Naming Service)提供分布式系统中所有对象(Object)、实体(Entity)的“名字”到关联的元数据之间的映射管理服务，例如 ServiceName -&gt; Endpoints Info, Distributed Lock Name -&gt; Lock Owner/Status Info, DNS Domain Name -&gt; IP List, 服务发现和 DNS 就是名字服务的2大场景 配置服务 (Configuration Service)在服务或者应用运行过程中，提供动态配置或者元数据以及配置管理的服务提供者 下载安装预备环境准备Nacos 依赖 Java 环境来运行。如果您是从代码开始构建并运行Nacos，还需要为此配置 Maven环境，请确保是在以下版本环境中安装使用: 64 bit OS，支持 Linux/Unix/Mac/Windows，推荐选用 Linux/Unix/Mac。 64 bit JDK 1.8+；下载 &amp; 配置。 Maven 3.2.x+；下载 &amp; 配置。 下载源码或者安装包 从 Github 上下载源码方式 1234567git clone https://github.com/alibaba/nacos.gitcd nacos/mvn -Prelease-nacos clean install -U ls -al distribution/target/// change the $version to your actual pathcd distribution/target/nacos-server-$version/nacos/bin 下载编译后压缩包方式 您可以从 最新稳定版本 下载 nacos-server-$version.zip 包。 12unzip nacos-server-$version.zip 或者 tar -xvf nacos-server-$version.tar.gzcd nacos/bin 启动服务器 Linux/Unix/Mac 启动命令(standalone代表着单机模式运行，非集群模式): 1sh startup.sh -m standalone Windows 启动命令： 1cmd startup.cmd 或者双击startup.cmd运行文件。 访问服务打开浏览器访问：http://132.232.137.183:8848/nacos 默认用户名：nacos 密码：nacos 创建服务提供者概述通过一个简单的示例来感受一下如何将服务注册到 Nacos，其实和 Eureka 没有太大差别。 POM创建一个工程名为 cloud-alibaba-service-provider 的服务提供者项目，pom.xml 配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-service-provider&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; Application通过 @EnableDiscoveryClient 注解表明是一个 Nacos 客户端，该注解是 Spring Cloud 提供的原生注解 1234567@SpringBootApplication@EnableDiscoveryClientpublic class ServiceProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceProviderApplication.class,args); &#125;&#125; Controller1234567891011@RestControllerpublic class ProviderController &#123; @Value("$&#123;server.port&#125;") private int port; @GetMapping("/hello/&#123;message&#125;") public String hello(@PathVariable("message")String message)&#123; return String.format("Hello " + message + ",I am provider .I am from "+ port); &#125;&#125; application.yml1234567891011121314server: port: 8081spring: application: name: service-provider cloud: nacos: discovery: server-addr: 132.232.137.183:8848management: endpoints: web: exposure: include: "*" 启动工程通过浏览器访问 http://132.232.137.183:8848/nacos，即 Nacos Server 网址 你会发现一个服务已经注册在服务中了，服务名为service-provider 这时打开 http://localhost:8081/hello/nacos ，你会在浏览器上看到： 服务的端点检查spring-cloud-starter-alibaba-nacos-discovery 在实现的时候提供了一个 EndPoint, EndPoint 的访问地址为 http://ip:port/actuator/nacos-discovery。 EndPoint 的信息主要提供了两类: 121、subscribe: 显示了当前有哪些服务订阅者2、NacosDiscoveryProperties: 显示了当前服务实例关于 Nacos 的基础配置 通过浏览器访问 http://localhost:8081/actuator/nacos-discovery 你会在浏览器上看到： 附：Nacos Starter 更多配置项信息 配置项 Key 默认值 说明 服务端地址 spring.cloud.nacos.discovery.server-addr 无 Nacos Server 启动监听的ip地址和端口 服务名 spring.cloud.nacos.discovery.service ${spring.application.name} 给当前的服务命名 权重 spring.cloud.nacos.discovery.weight 1 取值范围 1 到 100，数值越大，权重越大 网卡名 spring.cloud.nacos.discovery.network-interface 无 当IP未配置时，注册的IP为此网卡所对应的IP地址，如果此项也未配置，则默认取第一块网卡的地址 注册的IP地址 spring.cloud.nacos.discovery.ip 无 优先级最高 注册的端口 spring.cloud.nacos.discovery.port -1 默认情况下不用配置，会自动探测 命名空间 spring.cloud.nacos.discovery.namespace 无 常用场景之一是不同环境的注册的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。 AccessKey spring.cloud.nacos.discovery.access-key 无 当要上阿里云时，阿里云上面的一个云账号名 SecretKey spring.cloud.nacos.discovery.secret-key 无 当要上阿里云时，阿里云上面的一个云账号密码 Metadata spring.cloud.nacos.discovery.metadata 无 使用 Map 格式配置，用户可以根据自己的需要自定义一些和服务相关的元数据信息 日志文件名 spring.cloud.nacos.discovery.log-name 无 接入点 spring.cloud.nacos.discovery.enpoint UTF-8 地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址 是否集成 Ribbon ribbon.nacos.enabled true 创建服务消费者概述服务消费者的创建与服务提供者大同小异，这里采用最原始的一种方式，即显示的使用 LoadBalanceClient 和 RestTemplate 结合的方式来访问 POM创建一个工程名为 cloud-alibaba-service-consumer 的服务消费者项目，pom.xml 配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-service-consumer&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; Application1234567@SpringBootApplication@EnableDiscoveryClientpublic class ServiceConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceConsumerApplication.class,args); &#125;&#125; Configuration创建一个名为 ConsumerConfiguration 的 Java 配置类，主要作用是为了注入 RestTemplate 12345678@Configurationpublic class ConsumerConfiguration &#123; @Bean public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125; Controller创建一个名为 ConsumerController 测试用的 Controller 123456789101112131415161718192021222324@RestControllerpublic class ConsumerController &#123; private final LoadBalancerClient loadBalancerClient; private final RestTemplate restTemplate; @Autowired public ConsumerController(LoadBalancerClient loadBalancerClient, RestTemplate restTemplate) &#123; this.loadBalancerClient = loadBalancerClient; this.restTemplate = restTemplate; &#125; @Value("$&#123;spring.application.name&#125;") private String appName; @GetMapping(value = "/echo/app/name") public String echo() &#123; //使用 LoadBalanceClient 和 RestTemplate 结合的方式来访问 ServiceInstance serviceInstance = loadBalancerClient.choose("service-provider"); String url = String.format("http://%s:%s/hello/%s", serviceInstance.getHost(), serviceInstance.getPort(), appName); return restTemplate.getForObject(url, String.class); &#125;&#125; application.yml1234567891011121314server: port: 9091spring: application: name: service-consumer cloud: nacos: discovery: server-addr: 132.232.137.183:8848management: endpoints: web: exposure: include: "*" 启动工程通过浏览器访问 http://132.232.137.183:8848/nacos，即 Nacos Server 网址 你会发现多了一个名为 service-consumer 的服务 这时打开 http://localhost:9091/echo/app/name ，你会在浏览器上看到： 服务的端点检查通过浏览器访问 http://localhost:9091/actuator/nacos-discovery 你会在浏览器上看到： 创建服务消费者（Feign）概述Feign 是一个声明式的伪 Http 客户端，它使得写 Http 客户端变得更简单。使用 Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用 Feign 注解和 JAX-RS 注解。Feign 支持可插拔的编码器和解码器。Feign 默认集成了 Ribbon，Nacos 也很好的兼容了 Feign，默认实现了负载均衡的效果 Feign 采用的是基于接口的注解 Feign 整合了 ribbon POM创建一个工程名为 cloud-alibaba-service-consumer-feign 的服务消费者项目，pom.xml 配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-service-consumer-feign&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 主要增加了 org.springframework.cloud:spring-cloud-starter-openfeign 依赖 Application通过 @EnableFeignClients 注解开启 Feign 功能 12345678@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class ServiceConsumerFeignApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServiceConsumerFeignApplication.class,args); &#125;&#125; 创建 Feign 接口通过 @FeignClient(&quot;服务名&quot;) 注解来指定调用哪个服务。代码如下： 12345@FeignClient(value = "service-provider")public interface ConsumerService &#123; @GetMapping("/hello/&#123;message&#125;") String hello(@PathVariable("message")String message);&#125; Controller1234567891011121314@RestControllerpublic class ConsumerFeignController &#123; private final ConsumerService consumerService; @Autowired public ConsumerFeignController(ConsumerService consumerService) &#123; this.consumerService = consumerService; &#125; @GetMapping(value = "/hello/&#123;message&#125;") public String hello(@PathVariable("message")String message) &#123; return consumerService.hello(message); &#125;&#125; application.yml1234567891011121314server: port: 9092spring: application: name: service-consumer-feign cloud: nacos: discovery: server-addr: 132.232.137.183:8848management: endpoints: web: exposure: include: "*" 启动工程通过浏览器访问 http://132.232.137.183:8848/nacos，即 Nacos Server 网址 你会发现多了一个名为 service-consumer-feign 的服务 这时打开 http://localhost:9092/hello/feign ，你会在浏览器上看到： 测试负载均衡 启动多个 service-provider 实例，效果图如下： 在浏览器上多次访问 http://localhost:9092/hello/feign ，浏览器交替显示 使用熔断器防止服务雪崩概述在微服务架构中，根据业务来拆分成一个个的服务，服务与服务之间可以通过 RPC相互调用，在 Spring Cloud 中可以用 RestTemplate + LoadBalanceClient 和 Feign来调用。为了保证其高可用，单个服务通常会集群部署。由于网络原因或者自身的原因，服务并不能保证 100% 可用，如果单个服务出现问题，调用这个服务就会出现线程阻塞，此时若有大量的请求涌入，Servlet 容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的 “雪崩” 效应。 为了解决这个问题，业界提出了熔断器模型。 阿里巴巴开源了 Sentinel 组件，实现了熔断器模式，Spring Cloud 对这一组件进行了整合。在微服务架构中，一个请求需要调用多个服务是非常常见的，如下图： 较底层的服务如果出现故障，会导致连锁故障。当对特定的服务的调用的不可用达到一个阀值熔断器将会被打开。 熔断器打开后，为了避免连锁故障，通过 fallback 方法可以直接返回一个固定值 什么是 Sentinel随着微服务的流行，服务和服务之间的稳定性变得越来越重要。 Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性 Sentinel 的特征 丰富的应用场景： Sentinel 承接了阿里巴巴近 10 年的 双十一大促流量 的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、实时熔断下游不可用应用等。 完备的实时监控： Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。 广泛的开源生态： Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。 完善的 SPI 扩展点： Sentinel 提供简单易用、完善的 SPI 扩展点。您可以通过实现扩展点，快速的定制逻辑。例如定制规则管理、适配数据源等 Feign 中使用 Sentinel如果要在您的项目中引入 Sentinel，使用 group ID 为 org.springframework.cloud 和 artifact ID 为 spring-cloud-starter-alibaba-sentinel 的 starter。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; Sentinel 适配了 Feign 组件。但默认是关闭的。需要在配置文件中配置打开它，在配置文件增加以下代码： 123feign: sentinel: enabled: true 在 Service 中增加 fallback 指定类12345@FeignClient(value = "service-provider", fallback = ConsumerServiceFallback.class)public interface ConsumerService &#123; @GetMapping("/hello/&#123;message&#125;") String hello(@PathVariable("message")String message);&#125; 创建熔断器类并实现对应的 Feign 接口1234567@Componentpublic class ConsumerServiceFallback implements ConsumerService &#123; @Override public String hello(String message) &#123; return "服务器繁忙，请稍后重试"; &#125;&#125; 注意：一定要加上@component注解 测试熔断器此时我们关闭服务提供者 再次请求 http://localhost:9092/hello/feign 浏览器会显示： 使用熔断器仪表盘监控Sentinel 控制台Sentinel 控制台提供一个轻量级的控制台，它提供机器发现、单机资源实时监控、集群资源汇总，以及规则管理的功能。您只需要对应用进行简单的配置，就可以使用这些功能。 注意: 集群资源汇总仅支持 500 台以下的应用集群，有大概 1 - 2 秒的延时。 获取控制台 您可以从 release 页面 下载最新版本的控制台 jar 包。 您也可以从最新版本的源码自行构建 Sentinel 控制台： 下载 控制台 工程 使用以下命令将代码打包成一个 fat jar: mvn clean package 启动控制台Sentinel 控制台是一个标准的 SpringBoot 应用，以 SpringBoot 的方式运行 jar 包即可。 12# 使用源码自行构建jar包位置 sentinel-dashboard\targetjava -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar 如若8080端口冲突，可使用 -Dserver.port=新端口 进行设置。 访问服务打开浏览器访问：http://127.0.0.1:8080/#/dashboard/home 配置控制台信息application.yml 配置文件中增加如下配置： 123456spring: cloud: sentinel: transport: port: 8719 dashboard: 127.0.0.1:8080 这里的 spring.cloud.sentinel.transport.port 端口配置会在应用对应的机器上启动一个 Http Server，该 Server 会与 Sentinel 控制台做交互。比如 Sentinel 控制台添加了 1 个限流规则，会把规则数据 push 给这个 Http Server 接收，Http Server 再将规则注册到 Sentinel 中 测试 Sentinel使用之前的 Feign 客户端，application.yml 完整配置如下： 123456789101112131415161718192021server: port: 9092spring: application: name: service-consumer-feign cloud: nacos: discovery: server-addr: 132.232.137.183:8848 sentinel: transport: port: 8719 dashboard: 127.0.0.1:8080management: endpoints: web: exposure: include: "*"feign: sentinel: enabled: true 注：由于 8719 端口已被 sentinel-dashboard 占用，不修改也能注册，会自动帮你在端口号上 + 1； 打开浏览器访问：http://127.0.0.1:8080/#/dashboard/home 此时会多一个名为 service-consumer-feign 的服务 使用路由网关统一访问接口什么是 Spring Cloud GatewaySpring Cloud Gateway 是 Spring 官方基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，Spring Cloud Gateway 旨在为微服务架构提供一种简单而有效的统一的 API 路由管理方式。Spring Cloud Gateway 作为 Spring Cloud 生态系中的网关，目标是替代 Netflix ZUUL，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。 Spring Cloud Gateway 功能特征 基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0 动态路由 Predicates 和 Filters 作用于特定路由 集成 Hystrix 断路器 集成 Spring Cloud DiscoveryClient 易于编写的 Predicates 和 Filters 限流 路径重写 Spring Cloud Gateway 工程流程 客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。 过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（pre）或之后（post）执行业务逻辑。 POM创建一个工程名为 cloud-alibaba-gateway 的服务提供者项目，pom.xml 配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-gateway&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;!-- Commons Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Commons Begin --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 主要增加了 org.springframework.cloud:spring-cloud-starter-gateway 依赖 特别注意 Spring Cloud Gateway 不使用 Web 作为服务器，而是 使用 WebFlux 作为服务器，Gateway 项目已经依赖了 starter-webflux，所以这里 千万不要依赖 starter-web 由于过滤器等功能依然需要 Servlet 支持，故这里还需要依赖 javax.servlet:javax.servlet-api Application12345678@SpringBootApplication@EnableDiscoveryClient@EnableFeignClientspublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class,args); &#125;&#125; application.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253spring: application: # 应用名称 name: cloud-gateway cloud: # 使用 Naoos 作为服务注册发现 nacos: discovery: server-addr: 132.232.137.183:8848 # 使用 Sentinel 作为熔断器 sentinel: transport: port: 8721 dashboard: 127.0.0.1:8080 # 路由网关配置 gateway: # 设置与服务注册发现组件结合，这样可以采用服务名的路由策略 discovery: locator: enabled: true # 配置路由规则 routes: # 采用自定义路由 ID（有固定用法，不同的 id 有不同的功能，详见：https://cloud.spring.io/spring-cloud-static/spring-cloud-gateway/2.0.2.RELEASE/single/spring-cloud-gateway.html#gateway-route-filters） - id: SERVICE-CONSUMER # 采用 LoadBalanceClient 方式请求，以 lb:// 开头，后面的是注册在 Nacos 上的服务名 uri: lb://service-consumer # Predicate 翻译过来是“谓词”的意思，必须，主要作用是匹配用户的请求，有很多种用法 predicates: # Method 方法谓词，这里是匹配 GET 和 POST 请求 - Method=GET,POST - id: SERVICE-CONSUMER-FEIGN uri: lb://service-consumer-feign predicates: - Method=GET,POSTserver: port: 9000# 目前无效feign: sentinel: enabled: truemanagement: endpoints: web: exposure: include: "*"# 配置日志级别，方别调试logging: level: org.springframework.cloud.gateway: debug 注意：请仔细阅读注释 测试访问依次运行 Nacos 服务、ServiceProviderApplication、ServiceConsumerApplication、ServiceConsumerFeignApplication、GatewayApplication 打开浏览器访问：http://localhost:9000/nacos-consumer/echo/app/name 浏览器显示 打开浏览器访问：http://localhost:9000/nacos-consumer-feign/hello/gateway 浏览器显示 注意：请求方式是 http://路由网关IP:路由网关Port/服务名/** 至此说明 Spring Cloud Gateway 的路由功能配置成功 使用路由网关的全局过滤功能概述全局过滤器作用于所有的路由，不需要单独配置，我们可以用它来实现很多统一化处理的业务需求，比如权限认证，IP 访问限制等等。 注意：Spring Cloud Finchley.SR2版默认引入 Spring Cloud Gateway 2.0.2.RELEASE 其文档并不完善，并且有些地方还要重新设计，这里仅提供一个基本的案例 详见：Spring Cloud Gateway Documentation 生命周期 Spring Cloud Gateway 基于 Project Reactor 和 WebFlux，采用响应式编程风格，打开它的 Filter 的接口 GlobalFilter 你会发现它只有一个方法 filter 创建全局过滤器实现 GlobalFilter, Ordered 接口并在类上增加 @Component 注解就可以使用过滤功能了，非常简单方便 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import com.google.common.collect.Maps;import org.apache.commons.lang3.StringUtils;import org.springframework.cloud.gateway.filter.GatewayFilter;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.core.Ordered;import org.springframework.core.io.buffer.DataBuffer;import org.springframework.http.HttpStatus;import org.springframework.http.server.reactive.ServerHttpResponse;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;import java.util.HashMap;/** * @author by cheng * @description: 全局拦截器 * @data 2019/2/19 */@Componentpublic class ComsumerFilter implements GatewayFilter,Ordered&#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; String token = exchange.getRequest().getQueryParams().getFirst("token"); if (StringUtils.isBlank(token))&#123; ServerHttpResponse response = exchange.getResponse(); // 封装错误信息 HashMap&lt;String, Object&gt; respData = Maps.newHashMap(); respData.put("code",401); respData.put("message","非法请求"); respData.put("cause","Token is empty"); try &#123; // 将信息转换为 JSON ObjectMapper objectMapper = new ObjectMapper(); byte[] data = objectMapper.writeValueAsBytes(respData); // 输出错误信息到页面 DataBuffer buffer = response.bufferFactory().wrap(data); response.setStatusCode(HttpStatus.UNAUTHORIZED); response.getHeaders().add("Content-Type", "application/json;charset=UTF-8"); return response.writeWith(Mono.just(buffer)); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; &#125; return chain.filter(exchange); &#125; /** * 设置过滤器的执行顺序 * @return */ @Override public int getOrder() &#123; return Ordered.LOWEST_PRECEDENCE; &#125;&#125; 测试过滤器浏览器访问：http://localhost:9000/service-consumer-feign/hello/globalFilter 网页显示 浏览器访问：http://localhost:9000/service-consumer-feign/hello/globalFilter?token=123456网页显示 附：Spring Cloud Gateway BenchmarkSpring 官方人员提供的网关基准测试报告 GitHub Proxy Avg Latency Avg Req/Sec/Thread gateway 6.61ms 3.24k linkered 7.62ms 2.82k zuul 12.56ms 2.09k none 2.09ms 11.77k 说明 这里的 Zuul 为 1.x 版本，是一个基于阻塞 IO 的 API Gateway Zuul 已经发布了 Zuul 2.x，基于 Netty，非阻塞的，支持长连接，但 Spring Cloud 暂时还没有整合计划 Linkerd 基于 Scala 实现的、目前市面上仅有的生产级别的 Service Mesh（其他诸如 Istio、Conduit 暂时还不能用于生产）。 Nacos Config 服务端初始化分布式配置中心在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件 Nacos ConfigNacos 提供用于存储配置和其他元数据的 key/value 存储，为分布式系统中的外部化配置提供服务器端和客户端支持。使用 Spring Cloud Alibaba Nacos Config，您可以在 Nacos Server 集中管理你 Spring Cloud 应用的外部属性配置。 Spring Cloud Alibaba Nacos Config 是 Spring Cloud Config Server 和 Client 的替代方案，客户端和服务器上的概念与 Spring Environment 和 PropertySource 有着一致的抽象，在特殊的 bootstrap 阶段，配置被加载到 Spring 环境中。当应用程序通过部署管道从开发到测试再到生产时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时需要运行的所有内容 创建配置文件需要在 Nacos Server 中创建配置文件，我们依然采用 YAML 的方式部署配置文件，操作流程如下： 浏览器打开 http://132.232.137.183:8848/nacos ，访问 Nacos Server 新建配置文件，此处我们以之前创建的服务提供者项目为例 注意：Data ID 的默认扩展名为 .properties ，希望使用 YAML 配置，此处必须指明是 .yaml 发布成功后在 “配置列表” 一栏即可看到刚才创建的配置项 Nacos Config 客户端的使用POM此处我们以之前创建的服务提供者项目为例 在 pom.xml 中增加 org.springframework.cloud:spring-cloud-starter-alibaba-nacos-config 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 完整的 pom.xml 如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-service-provider&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; bootstrap.properties创建名为 bootstrap.properties 的配置文件并删除之前创建的 application.yml 配置文件，由于已经在服务端配置，此处不再赘述 123456# 这里的应用名对应 Nacos Config 中的 Data ID，实际应用名称以配置中心的配置为准spring.application.name=service-provider-config# 指定查找名为 service-provider-config.yaml 的配置文件spring.cloud.nacos.config.file-extension=yaml# Nacos Server 的地址spring.cloud.nacos.config.server-addr=132.232.137:8848 注意：在之前的 Spring Cloud Netflix 课程中有提到过 Spring Boot 配置文件的加载顺序，依次为 bootstrap.properties -&gt; bootstrap.yml -&gt; application.properties -&gt; application.yml ，其中 bootstrap.properties 配置为最高优先级 启动应用程序启动应用后我们可以通过日志看到，已经成功加载到了配置文件 配置的动态更新Nacos Config 也支持配置的动态更新，操作流程如下： 修改服务端配置，增加一个 user.name 的属性 修改 Controller ，增加一个请求方法，测试配置更新效果 12345678// 注入配置文件上下文@Autowiredprivate ConfigurableApplicationContext applicationContext;// 从上下文中读取配置@GetMapping("getname") public String getName()&#123; return "Hello " + applicationContext.getEnvironment().getProperty("user.name"); &#125; 通过浏览器访问该接口http://localhost:8081/getname，浏览器显示 修改服务端配置 此时观察控制台日志，你会发现我们已经成功刷新了配置 刷新浏览器，浏览器显示 注意：你可以使用 spring.cloud.nacos.config.refresh.enabled=false 来关闭动态刷新 Nacos Config 多环境的配置Spring Boot Profile我们在做项目开发的时候，生产环境和测试环境的一些配置可能会不一样，有时候一些功能也可能会不一样，所以我们可能会在上线的时候手工修改这些配置信息。但是 Spring 中为我们提供了 Profile 这个功能。我们只需要在启动的时候添加一个虚拟机参数，激活自己环境所要用的 Profile 就可以了。 操作起来很简单，只需要为不同的环境编写专门的配置文件，如：application-dev.yml、application-prod.yml， 启动项目时只需要增加一个命令参数 --spring.profiles.active=环境配置 即可，启动命令如下： 1java -jar cloud-alibaba-service-provider-1.0.0-SNAPSHOT.jar --spring.profiles.active=prod Nacos Config Profilespring-cloud-starter-alibaba-nacos-config 在加载配置的时候，不仅仅加载了以 dataid 为 ${spring.application.name}.${file-extension:properties} 为前缀的基础配置，还加载了 dataid 为 ${spring.application.name}-${profile}.${file-extension:properties} 的基础配置。在日常开发中如果遇到多套环境下的不同配置，可以通过 Spring 提供的 ${spring.profiles.active} 这个配置项来配置。 此处我们以之前创建的服务提供者项目为例 在 Nacos Server 中增加配置增加一个名为 service-provider-config-prod.yaml 的配置 注意：此时，我将配置文件中的端口由 8081 -&gt; 8082 在项目中增加配置增加一个名为 bootstrap-prod.properties 的配置文件，内容如下： 1234spring.profiles.active=prodspring.application.name=service-provider-configspring.cloud.nacos.config.file-extension=yamlspring.cloud.nacos.config.server-addr=127.0.0.1:8848 主要增加了 spring.profiles.active=prod 配置，用于指定访问 Nacos Server 中的 nacos-provider-config-prod.yaml 配置 启动应用程序此时我们有两个配置文件，分别为 bootstrap.properties 和 bootstrap-prod.properties ，我们需要指定启动时加载哪一个配置文件，操作流程如下： Run -&gt; Edit Configurations.. 设置需要激活的配置 观察日志，判断是否成功加载配置 Spring Cloud Alibaba 链路追踪什么是链路追踪微服务架构是通过业务来划分服务的，使用 REST 调用。对外暴露的一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者网络超时，都会形成导致接口调用失败。随着业务的不断扩张，服务之间互相调用会越来越复杂。 随着服务的越来越多，对调用链的分析会越来越复杂。它们之间的调用关系也许如下： 面对以上情况，我们就需要一些可以帮助理解系统行为、用于分析性能问题的工具，以便发生故障的时候，能够快速定位和解决问题，这就是所谓的 APM（应用性能管理） 什么是 SkyWalking目前主要的一些 APM 工具有: Cat、Zipkin、Pinpoint、SkyWalking；Apache SkyWalking 是观察性分析平台和应用性能管理系统。提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。 Skywalking Agent： 使用 JavaAgent 做字节码植入，无侵入式的收集，并通过 HTTP 或者 gRPC 方式发送数据到 SkyWalking Collector。 SkyWalking Collector： 链路数据收集器，对 agent 传过来的数据进行整合分析处理并落入相关的数据存储中。 Storage： SkyWalking 的存储，时间更迭，SW 已经开发迭代到了 6.x 版本，在 6.x 版本中支持以 ElasticSearch(支持 6.x)、Mysql、TiDB、H2、作为存储介质进行数据存储。 UI： Web 可视化平台，用来展示落地的数据。 SkyWalking 功能特性 多种监控手段，语言探针和服务网格(Service Mesh) 多语言自动探针，Java，.NET Core 和 Node.JS 轻量高效，不需要大数据 模块化，UI、存储、集群管理多种机制可选 支持告警 优秀的可视化方案 SkyWalking 服务端配置基于 Docker 安装 ElasticSearch在 为什么需要链路追踪 章节中介绍过 SkyWalking 存储方案有多种，官方推荐的方案是 ElasticSearch ，所以我们需要先安装 ElasticSearch。 docker-compose.yml 1234567891011version: '3'services: elasticsearch: image: wutang/elasticsearch-shanghai-zone:6.3.2 container_name: elasticsearch restart: always ports: - 9200:9200 - 9300:9300 environment: cluster.name: elasticsearch 其中，9200 端口号为 SkyWalking 配置 ElasticSearch 所需端口号，cluster.name为 SkyWalking 配置 ElasticSearch 集群的名称 测试是否启动成功浏览器访问 http://elasticsearchIP:9200/ ，浏览器返回如下信息即表示成功启动 下载并启动 SkyWalking官方已经为我们准备好了编译过的服务端版本，下载地址为 http://skywalking.apache.org/downloads/，这里我们需要下载 6.x releases 版本 配置 SkyWalking下载完成后解压缩，进入 apache-skywalking-apm-incubating/config 目录并修改 application.yml 配置文件 这里需要做三件事： 注释 H2 存储方案 启用 ElasticSearch 存储方案 修改 ElasticSearch 服务器地址 启动 SkyWalking修改完配置后，进入 apache-skywalking-apm-incubating\bin 目录，运行 startup.bat 启动服务端 通过浏览器访问 http://localhost:8080 出现如下界面即表示启动成功 默认的用户名密码为：admin/admin，登录成功后，效果如下图 SkyWalking 客户端配置Java Agent 服务器探针参考官网给出的帮助 Setup java agent，我们需要使用官方提供的探针为我们达到监控的目的，按照实际情况我们需要实现三种部署方式 IDEA 部署探针 Java 启动方式部署探针（我们是 Spring Boot 应用程序，需要使用 java -jar 的方式启动应用） Docker 启动方式部署探针（需要做到一次构建到处运行的持续集成效果，本章节暂不提供解决方案，到后面的实战环节再实现） 探针文件在 apache-skywalking-apm-incubating/agent 目录下 IDEA 部署探针继续之前的案例项目，创建一个名为 cloud-external-skywalking 的目录，并将 agent 整个目录拷贝进来 修改项目的 VM 运行参数，点击菜单栏中的 Run -&gt; EditConfigurations...，此处我们以 nacos-provider 项目为例，修改参数如下 123-javaagent:H:\code\cloud-alibaba\cloud-external-skywalking\agent\skywalking-agent.jar-Dskywalking.agent.service_name=service-provider-Dskywalking.collector.backend_service=localhost:11800 -javaagent：用于指定探针路径 -Dskywalking.agent.service_name：用于重写 agent/config/agent.config 配置文件中的服务名 -Dskywalking.collector.backend_service：用于重写 agent/config/agent.config 配置文件中的服务地址 Java 启动方式1java -javaagent:H:\code\cloud-alibaba\cloud-external-skywalking\agent\skywalking-agent.jar -Dskywalking.agent.service_name=service-provider -Dskywalking.collector.backend_service=localhost:11800 -jar yourApp.jar 测试监控启动 service-provider 项目，通过观察日志可以发现，已经成功加载探针 访问之前写好的接口 http://localhost:8081/hello/skywalking 之后再刷新 SkyWalking Web UI，你会发现 Service 与 Endpoint 已经成功检测到了 至此，表示 SkyWalking 链路追踪配置成功 SkyWalking Trace 监控SkyWalking 通过业务调用监控进行依赖分析，提供给我们了服务之间的服务调用拓扑关系、以及针对每个 Endpoint 的 Trace 记录 调用链路监控 点击 Trace 菜单，进入追踪页 点击 Trace ID 展开详细信息 上图展示了一次正常的响应，总响应时间为 444ms 共有一个 Span（基本工作单元，表示一次完整的请求，包含响应，即请求并响应） Span /hello/{message} 说明如下： Duration：响应时间 444 毫秒 component：组件类型为 SpringMVC url：请求地址 http.method：请求类型 服务性能指标监控 点击 Service 菜单，进入服务性能指标监控页 选择希望监控的服务 Avg SLA： 服务可用性（主要是通过请求成功与失败次数来计算） CPM： 每分钟调用次数 Avg Response Time： 平均响应时间 点击 More Server Details... 还可以查看详细信息 上图中展示了服务在一定时间范围内的相关数据，包括： 服务可用性指标 SLA 每分钟平均响应数 平均响应时间 服务进程 PID 服务所在物理机的 IP、Host、OS 运行时 CPU 使用率 运行时堆内存使用率 运行时非堆内存使用率 GC 情况 附：Maven Assembly 插件 什么是 Assembly Plugin Assembly 插件目的是提供一个把工程依赖元素、模块、网站文档等其他文件存放到单个归档文件里 Assembly 支持的归档文件类型 zip tar.gz tar.bz2 jar dir war 使用步骤 此处以将 SkyWalking 探针打包为 tar.gz 为例，为后期持续集成时构建 Docker 镜像做好准备 POM 在 pom.xml 中增加插件配置 123456789101112131415161718192021222324252627&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;!-- 配置执行器 --&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;!-- 绑定到 package 生命周期阶段上 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;!-- 只运行一次 --&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;finalName&gt;skywalking&lt;/finalName&gt; &lt;descriptors&gt; &lt;!-- 配置描述文件路径 --&gt; &lt;descriptor&gt;src/main/resources/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; assembly.xml 创建 src/main/resources/assembly.xml 配置文件 1234567891011121314151617181920212223242526&lt;assembly&gt; &lt;id&gt;6.0.0-Beta&lt;/id&gt; &lt;formats&gt; &lt;!-- 打包的文件格式，支持 zip、tar.gz、tar.bz2、jar、dir、war --&gt; &lt;format&gt;tar.gz&lt;/format&gt; &lt;/formats&gt; &lt;!-- tar.gz 压缩包下是否生成和项目名相同的根目录，有需要请设置成 true --&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;!-- 是否把本项目添加到依赖文件夹下，有需要请设置成 true --&gt; &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt; &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt; &lt;!-- 将 scope 为 runtime 的依赖包打包 --&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;!-- 设置需要打包的文件路径 --&gt; &lt;directory&gt;agent&lt;/directory&gt; &lt;!-- 打包后的输出路径 --&gt; &lt;outputDirectory&gt;&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;/fileSets&gt;&lt;/assembly&gt; 打包 12mvn clean packagemvn clean install package：会在 target 目录下创建名为 skywalking-6.0.0-Beta.tar.gz 的压缩包 install：会在本地仓库目录下创建名为 hello-spring-cloud-external-skywalking-1.0.0-SNAPSHOT-6.0.0-Beta.tar.gz 的压缩包 Spring Cloud Alibaba 异步通信消息队列的流派什么是 MQMessage Queue（MQ），消息队列中间件。很多人都说：MQ 通过将消息的发送和接收分离来实现应用程序的异步和解偶，这个给人的直觉是——MQ 是异步的，用来解耦的，但是这个只是 MQ 的效果而不是目的。MQ 真正的目的是为了通讯，屏蔽底层复杂的通讯协议，定义了一套应用层的、更加简单的通讯协议。一个分布式系统中两个模块之间通讯要么是 HTTP，要么是自己开发的 TCP，但是这两种协议其实都是原始的协议。HTTP 协议很难实现两端通讯——模块 A 可以调用 B，B 也可以主动调用 A，如果要做到这个两端都要背上 WebServer，而且还不支持长连接（HTTP 2.0 的库根本找不到）。TCP 就更加原始了，粘包、心跳、私有的协议，想一想头皮就发麻。MQ 所要做的就是在这些协议之上构建一个简单的“协议”——生产者/消费者模型。MQ 带给我的“协议”不是具体的通讯协议，而是更高层次通讯模型。它定义了两个对象——发送数据的叫生产者；接收数据的叫消费者， 提供一个 SDK 让我们可以定义自己的生产者和消费者实现消息通讯而无视底层通讯协议 有 Broker 的 MQ这个流派通常有一台服务器作为 Broker，所有的消息都通过它中转。生产者把消息发送给它就结束自己的任务了，Broker 则把消息主动推送给消费者（或者消费者主动轮询） 重 Topickafka、JMS（ActiveMQ）就属于这个流派，生产者会发送 key 和数据到 Broker，由 Broker 比较 key 之后决定给哪个消费者。这种模式是我们最常见的模式，是我们对 MQ 最多的印象。在这种模式下一个 topic 往往是一个比较大的概念，甚至一个系统中就可能只有一个 topic，topic 某种意义上就是 queue，生产者发送 key 相当于说：“hi，把数据放到 key 的队列中” 如上图所示，Broker 定义了三个队列，key1，key2，key3，生产者发送数据的时候会发送 key1 和 data，Broker 在推送数据的时候则推送 data（也可能把 key 带上）。 虽然架构一样但是 kafka 的性能要比 jms 的性能不知道高到多少倍，所以基本这种类型的 MQ 只有 kafka 一种备选方案。如果你需要一条暴力的数据流（在乎性能而非灵活性）那么 kafka 是最好的选择 轻 Topic这种的代表是 RabbitMQ（或者说是 AMQP）。生产者发送 key 和数据，消费者定义订阅的队列，Broker 收到数据之后会通过一定的逻辑计算出 key 对应的队列，然后把数据交给队列 这种模式下解耦了 key 和 queue，在这种架构中 queue 是非常轻量级的（在 RabbitMQ 中它的上限取决于你的内存），消费者关心的只是自己的 queue；生产者不必关心数据最终给谁只要指定 key 就行了，中间的那层映射在 AMQP 中叫 exchange（交换机）。 AMQP 中有四种 exchange Direct exchange：key 就等于 queue Fanout exchange：无视 key，给所有的 queue 都来一份 Topic exchange：key 可以用“宽字符”模糊匹配 queue Headers exchange：无视 key，通过查看消息的头部元数据来决定发给那个 queue（AMQP 头部元数据非常丰富而且可以自定义） 这种结构的架构给通讯带来了很大的灵活性，我们能想到的通讯方式都可以用这四种 exchange 表达出来。如果你需要一个企业数据总线（在乎灵活性）那么 RabbitMQ 绝对的值得一用 无 Broker 的 MQ无 Broker 的 MQ 的代表是 ZeroMQ。该作者非常睿智，他非常敏锐的意识到——MQ 是更高级的 Socket，它是解决通讯问题的。所以 ZeroMQ 被设计成了一个“库”而不是一个中间件，这种实现也可以达到——没有 Broker 的目的 节点之间通讯的消息都是发送到彼此的队列中，每个节点都既是生产者又是消费者。ZeroMQ 做的事情就是封装出一套类似于 Socket 的 API 可以完成发送数据，读取数据 ZeroMQ 其实就是一个跨语言的、重量级的 Actor 模型邮箱库。你可以把自己的程序想象成一个 Actor，ZeroMQ 就是提供邮箱功能的库；ZeroMQ 可以实现同一台机器的 RPC 通讯也可以实现不同机器的 TCP、UDP 通讯，如果你需要一个强大的、灵活、野蛮的通讯能力，别犹豫 ZeroMQ 附：Queue 和 Topic 的区别 Queue： 一个发布者发布消息，下面的接收者按队列顺序接收，比如发布了 10 个消息，两个接收者 A,B 那就是 A,B 总共 会收到 10 条消息，不重复。 Topic： 一个发布者发布消息，有两个接收者 A,B 来订阅，那么发布了 10 条消息，A,B 各收到 10 条消息。 类型 Topic Queue 概要 Publish Subscribe Messaging 发布订阅消息 Point-to-Point 点对点 有无状态 Topic 数据默认不落地，是无状态的。 Queue 数据默认会在 MQ 服务器上以文件形式保存，比如 ActiveMQ 一般保存在 $AMQ_HOME\data\kr-store\data下面。也可以配置成 DB 存储。 完整性保障 并不保证 Publisher 发布的每条数据，Subscriber 都能接受到。 Queue 保证每条数据都能被 Receiver 接收。 消息是否会丢失 一般来说 Publisher 发布消息到某一个 Topic 时，只有正在监听该 Topic 地址的 Sub 能够接收到消息；如果没有 Sub 在监听，该 Topic 就丢失了。 Sender 发送消息到目标 Queue，Receiver 可以异步接收这个 Queue 上的消息。Queue 上的消息如果暂时没有 Receiver 来取，也不会丢失。 消息发布接收策略 一对多的消息发布接收策略，监听同一个 Topic 地址的多个 Sub 都能收到 Publisher 发送的消息。Sub 接收完通知 MQ 服务器 一对一的消息发布接收策略，一个 Sender 发送的消息，只能有一个 Receiver 接收。Receiver 接收完后，通知 MQ 服务器已接收，MQ 服务器对 Queue 里的消息采取删除或其他操作。 RocketMQ 简介概述消息队列作为高并发系统的核心组件之一，能够帮助业务系统解构提升开发效率和系统稳定性。主要具有以下优势： 削峰填谷： 主要解决瞬时写压力大于应用服务能力导致消息丢失、系统奔溃等问题 系统解耦： 解决不同重要程度、不同能力级别系统之间依赖导致一死全死 提升性能： 当存在一对多调用时，可以发一条消息给消息系统，让消息系统通知相关系统 蓄流压测： 线上有些链路不好压测，可以通过堆积一定量消息再放开来压测 RocketMQApache Alibaba RocketMQ 是一个消息中间件。消息中间件中有两个角色：消息生产者和消息消费者。RocketMQ 里同样有这两个概念，消息生产者负责创建消息并发送到 RocketMQ 服务器，RocketMQ 服务器会将消息持久化到磁盘，消息消费者从 RocketMQ 服务器拉取消息并提交给应用消费 RocketMQ 特点RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点： 支持严格的消息顺序 支持 Topic 与 Queue 两种模式 亿级消息堆积能力 比较友好的分布式特性 同时支持 Push 与 Pull 方式消费消息 历经多次天猫双十一海量消息考验 RocketMQ 优势目前主流的 MQ 主要是 RocketMQ、kafka、RabbitMQ，其主要优势有： 支持事务型消息（消息发送和 DB 操作保持两方的最终一致性，RabbitMQ 和 Kafka 不支持） 支持结合 RocketMQ 的多个系统之间数据最终一致性（多方事务，二方事务是前提） 支持 18 个级别的延迟消息（RabbitMQ 和 Kafka 不支持） 支持指定次数和时间间隔的失败消息重发（Kafka 不支持，RabbitMQ 需要手动确认） 支持 Consumer 端 Tag 过滤，减少不必要的网络传输（RabbitMQ 和 Kafka 不支持） 支持重复消费（RabbitMQ 不支持，Kafka 支持） 消息队列对比参照表 基于 Docker 安装 RocketMQdocker-compose.yml注意：启动 RocketMQ Server + Broker + Console 至少需要 2G 内存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455version: '3.5'services: rmqnamesrv: image: foxiswho/rocketmq:server container_name: rmqnamesrv ports: - 9876:9876 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store networks: rmq: aliases: - rmqnamesrv rmqbroker: image: foxiswho/rocketmq:broker container_name: rmqbroker ports: - 10909:10909 - 10911:10911 volumes: - ./data/logs:/opt/logs - ./data/store:/opt/store - ./data/brokerconf/broker.conf:/etc/rocketmq/broker.conf environment: NAMESRV_ADDR: "rmqnamesrv:9876" JAVA_OPTS: " -Duser.home=/opt" JAVA_OPT_EXT: "-server -Xms128m -Xmx128m -Xmn128m" command: mqbroker -c /etc/rocketmq/broker.conf depends_on: - rmqnamesrv networks: rmq: aliases: - rmqbroker rmqconsole: image: styletang/rocketmq-console-ng container_name: rmqconsole ports: - 8080:8080 environment: JAVA_OPTS: "-Drocketmq.namesrv.addr=rmqnamesrv:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false" depends_on: - rmqnamesrv networks: rmq: aliases: - rmqconsolenetworks: rmq: name: rmq driver: bridge broker.confRocketMQ Broker 需要一个配置文件，按照上面的 Compose 配置，我们需要在 ./data/brokerconf/ 目录下创建一个名为 broker.conf 的配置文件，内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# 所属集群名字brokerClusterName=DefaultCluster# broker 名字，注意此处不同的配置文件填写的不一样，如果在 broker-a.properties 使用: broker-a,# 在 broker-b.properties 使用: broker-bbrokerName=broker-a# 0 表示 Master，&gt; 0 表示 SlavebrokerId=0# nameServer地址，分号分割# namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876# 启动IP,如果 docker 报 com.alibaba.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;192.168.0.120:10909&gt; failed# 解决方式1 加上一句 producer.setVipChannelEnabled(false);，解决方式2 brokerIP1 设置宿主机IP，不要使用docker 内部IP# brokerIP1=192.168.0.253# 在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4# 是否允许 Broker 自动创建 Topic，建议线下开启，线上关闭 ！！！这里仔细看是 false，false，falseautoCreateTopicEnable=true# 是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true# Broker 对外服务的监听端口listenPort=10911# 删除文件时间点，默认凌晨4点deleteWhen=04# 文件保留时间，默认48小时fileReservedTime=120# commitLog 每个文件的大小默认1GmapedFileSizeCommitLog=1073741824# ConsumeQueue 每个文件默认存 30W 条，根据业务情况调整mapedFileSizeConsumeQueue=300000# destroyMapedFileIntervalForcibly=120000# redeleteHangedFileInterval=120000# 检测物理文件磁盘空间diskMaxUsedSpaceRatio=88# 存储路径# storePathRootDir=/home/ztztdata/rocketmq-all-4.1.0-incubating/store# commitLog 存储路径# storePathCommitLog=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/commitlog# 消费队列存储# storePathConsumeQueue=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/consumequeue# 消息索引存储路径# storePathIndex=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/index# checkpoint 文件存储路径# storeCheckpoint=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/checkpoint# abort 文件存储路径# abortFile=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/abort# 限制的消息大小maxMessageSize=65536# flushCommitLogLeastPages=4# flushConsumeQueueLeastPages=2# flushCommitLogThoroughInterval=10000# flushConsumeQueueThoroughInterval=60000# Broker 的角色# - ASYNC_MASTER 异步复制Master# - SYNC_MASTER 同步双写Master# - SLAVEbrokerRole=ASYNC_MASTER# 刷盘方式# - ASYNC_FLUSH 异步刷盘# - SYNC_FLUSH 同步刷盘flushDiskType=ASYNC_FLUSH# 发消息线程池数量# sendMessageThreadPoolNums=128# 拉消息线程池数量# pullMessageThreadPoolNums=128 RocketMQ 控制台访问 http://rmqIP:8080 登入控制台 RocketMQ 生产者概述RocketMQ 是一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。 由于本教程整个案例基于 Spring Cloud，故我们采用 Spring Cloud Stream 完成一次发布和订阅 官方教程 Spring Cloud StreamSpring Cloud Stream 是一个用于构建基于消息的微服务应用框架。它基于 Spring Boot 来创建具有生产级别的单机 Spring 应用，并且使用 Spring Integration 与 Broker 进行连接。 Spring Cloud Stream 提供了消息中间件配置的统一抽象，推出了 publish-subscribe、consumer groups、partition 这些统一的概念。 Spring Cloud Stream 内部有两个概念： Binder： 跟外部消息中间件集成的组件，用来创建 Binding，各消息中间件都有自己的 Binder 实现。 Binding： 包括 Input Binding 和 Output Binding。 Binding 在消息中间件与应用程序提供的 Provider 和 Consumer 之间提供了一个桥梁，实现了开发者只需使用应用程序的 Provider 或 Consumer 生产或消费数据即可，屏蔽了开发者与底层消息中间件的接触。 解决连接超时问题我们采用 Docker 部署了 RocketMQ 服务，此时 RocketMQ Broker 暴露的地址和端口(10909，10911)是基于容器的，会导致我们开发机无法连接，从而引发 org.apache.rocketmq.remoting.exception.RemotingTooMuchRequestException: sendDefaultImpl call timeout 异常 注意下图中的 IP 地址，这个是容器的 IP，开发机与容器不在一个局域网所以无法连接。 解决方案是在 broker.conf 配置文件中增加 brokerIP1=宿主机IP 即可 POM创建一个工程名为 cloud-alibaba-rocketmq-provider RocketMQ 生产者项目,主要增加了 org.springframework.cloud:spring-cloud-starter-stream-rocketmq 依赖 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-rocketmq-provider&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 消息生产者服务12345678910111213141516package club.codeopen.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.messaging.MessageChannel;import org.springframework.messaging.support.MessageBuilder;import org.springframework.stereotype.Service;@Servicepublic class ProviderService &#123; @Autowired private MessageChannel output; public void send(String message) &#123; output.send(MessageBuilder.withPayload(message).build()); &#125;&#125; Application配置 Output(Source.class) 的 Binding 信息并配合 @EnableBinding 注解使其生效 1234567891011121314151617181920212223242526272829package club.codeopen;import club.codeopen.service.ProviderService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.CommandLineRunner;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.stream.annotation.EnableBinding;import org.springframework.cloud.stream.messaging.Source;@SpringBootApplication@EnableBinding(&#123;Source.class&#125;)public class RocketMQProviderApplication implements CommandLineRunner &#123; @Autowired private ProviderService providerService; public static void main(String[] args) &#123; SpringApplication.run(RocketMQProviderApplication.class, args); &#125; /** * 实现了 CommandLineRunner 接口，只是为了 Spring Boot 启动时执行任务，不必特别在意 * @param args * @throws Exception */ @Override public void run(String... args) throws Exception &#123; providerService.send("Hello RocketMQ"); &#125;&#125; application.yml123456789101112131415161718192021spring: application: name: rocketmq-provider cloud: stream: rocketmq: binder: # RocketMQ 服务器地址 namesrv-addr: 192.168.129.149:9876 bindings: # 这里是个 Map 类型参数，&#123;&#125; 为 YAML 中 Map 的行内写法 output: &#123;destination: test-topic, content-type: application/json&#125;server: port: 9093management: endpoints: web: exposure: include: '*' 运行成功后即可在 RocketMQ 控制台的 消息 列表中选择 test-topic 主题即可看到发送的消息 RocketMQ 消费者POM创建一个工程名为 cloud-alibaba-rocketmq-consumer RocketMQ 消费者项目,主要增加了 org.springframework.cloud:spring-cloud-starter-stream-rocketmq 依赖 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../cloud-alibaba-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;cloud-alibaba-rocketmq-consumer&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rocketmq&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 消息消费者服务主要使用 @StreamListener(&quot;input&quot;) 注解来订阅从名为 input 的 Binding 中接收的消息 12345678910111213package club.codeopen.receive;import org.springframework.cloud.stream.annotation.StreamListener;import org.springframework.stereotype.Service;@Servicepublic class ConsumerReceive &#123; @StreamListener("input") public void receiveInput(String message) &#123; System.out.println("Receive input: " + message); &#125;&#125; Application配置 Input(Sink.class) 的 Binding 信息并配合 @EnableBinding 注解使其生效 1234567891011121314package club.codeopen;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.stream.annotation.EnableBinding;import org.springframework.cloud.stream.messaging.Sink;@SpringBootApplication@EnableBinding(&#123;Sink.class&#125;)public class RocketMQConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RocketMQConsumerApplication.class, args); &#125;&#125; application.yml123456789101112131415161718192021spring: application: name: rocketmq-consumer cloud: stream: rocketmq: binder: namesrv-addr: 192.168.129.149:9876 bindings: input: &#123;consumer.orderly: true&#125; bindings: input: &#123;destination: test-topic, content-type: text/plain, group: test-group, consumer.maxAttempts: 1&#125;server: port: 9094management: endpoints: web: exposure: include: '*' 运行成功后即可在控制台接收到消息：Receive input: Hello RocketMQ RocketMQ 自定义 Binding概述在实际生产中，我们需要发布和订阅的消息可能不止一种 Topic ，故此时就需要使用自定义 Binding 来帮我们实现多 Topic 的发布和订阅功能 生产者自定义 Output 接口，代码如下： 1234567public interface MySource &#123; @Output("output1") MessageChannel output1(); @Output("output2") MessageChannel output2();&#125; 发布消息的案例代码如下： 123456@Autowiredprivate MySource source;public void send(String msg) throws Exception &#123; source.output1().send(MessageBuilder.withPayload(msg).build());&#125; 消费者自定义 Input 接口，代码如下： 12345678910111213public interface MySink &#123; @Input("input1") SubscribableChannel input1(); @Input("input2") SubscribableChannel input2(); @Input("input3") SubscribableChannel input3(); @Input("input4") SubscribableChannel input4();&#125; 接收消息的案例代码如下： 1234@StreamListener("input1")public void receiveInput1(String receiveMsg) &#123; System.out.println("input1 receive: " + receiveMsg);&#125; Application配置 Input 和 Output 的 Binding 信息并配合 @EnableBinding 注解使其生效，代码如下： 1234567@SpringBootApplication@EnableBinding(&#123; MySource.class, MySink.class &#125;)public class RocketMQApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RocketMQApplication.class, args); &#125;&#125; application.yml 生产者 1234567891011spring: application: name: rocketmq-provider cloud: stream: rocketmq: binder: namesrv-addr: 192.168.129.149:9876 bindings: output1: &#123;destination: test-topic1, content-type: application/json&#125; output2: &#123;destination: test-topic2, content-type: application/json&#125; 消费者 123456789101112131415spring: application: name: rocketmq-consumer cloud: stream: rocketmq: binder: namesrv-addr: 192.168.129.149:9876 bindings: input: &#123;consumer.orderly: true&#125; bindings: input1: &#123;destination: test-topic1, content-type: text/plain, group: test-group, consumer.maxAttempts: 1&#125; input2: &#123;destination: test-topic1, content-type: text/plain, group: test-group, consumer.maxAttempts: 1&#125; input3: &#123;destination: test-topic2, content-type: text/plain, group: test-group, consumer.maxAttempts: 1&#125; input4: &#123;destination: test-topic2, content-type: text/plain, group: test-group, consumer.maxAttempts: 1&#125;]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Alibaba 服务配置]]></title>
    <url>%2F2019%2F05%2F23%2Fspring-cloud-alibaba%2FSpring%20Cloud%20Alibaba%20%E6%9C%8D%E5%8A%A1%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Nacos Config 服务端初始化分布式配置中心在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件。 Nacos ConfigNacos 提供用于存储配置和其他元数据的 key/value 存储，为分布式系统中的外部化配置提供服务器端和客户端支持。使用 Spring Cloud Alibaba Nacos Config，您可以在 Nacos Server 集中管理你 Spring Cloud 应用的外部属性配置。 Spring Cloud Alibaba Nacos Config 是 Spring Cloud Config Server 和 Client 的替代方案，客户端和服务器上的概念与 Spring Environment 和 PropertySource 有着一致的抽象，在特殊的 bootstrap 阶段，配置被加载到 Spring 环境中。当应用程序通过部署管道从开发到测试再到生产时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时需要运行的所有内容 创建配置文件需要在 Nacos Server 中创建配置文件，我们依然采用 YAML 的方式部署配置文件，操作流程如下： 浏览器打开 http://localhost:8848/nacos ，访问 Nacos Server 新建配置文件，此处我们以之前创建的 服务提供者 项目为例 注意：Data ID 的默认扩展名为 .properties ，希望使用 YAML 配置，此处必须指明是 .yaml 发布成功后在 “配置列表” 一栏即可看到刚才创建的配置项 Nacos Config 客户端的使用POM此处我们以之前创建的 服务提供者 项目为例 在 pom.xml 中增加 org.springframework.cloud:spring-cloud-starter-alibaba-nacos-config 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 完整的 pom.xml 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-alibaba-demo-dependencies/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-cloud-alibaba-demo-nacos-provider&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-cloud-alibaba-demo-nacos-provider&lt;/name&gt; &lt;url&gt;https://www.codeopen.club&lt;/url&gt; &lt;inceptionYear&gt;2019-Now&lt;/inceptionYear&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- Spring Boot End --&gt; &lt;!-- Spring Cloud Begin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud End --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.spring.cloud.alibaba.nacos.provider.NacosProviderApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; bootstrap.properties创建名为 bootstrap.properties 的配置文件并删除之前创建的 application.yml 配置文件，由于已经在服务端配置，此处不再赘述 123456# 这里的应用名对应 Nacos Config 中的 Data ID，实际应用名称以配置中心的配置为准spring.application.name=nacos-provider-config# 指定查找名为 nacos-provider-config.yaml 的配置文件spring.cloud.nacos.config.file-extension=yaml# Nacos Server 的地址spring.cloud.nacos.config.server-addr=127.0.0.1:8848 注意：在之前的 Spring Cloud Netflix 课程中有提到过 Spring Boot 配置文件的加载顺序，依次为 bootstrap.properties -&gt; bootstrap.yml -&gt; application.properties -&gt; application.yml ，其中 bootstrap.properties 配置为最高优先级 启动应用程序启动应用后我们可以通过日志看到，已经成功加载到了配置文件 配置的动态更新Nacos Config 也支持配置的动态更新，操作流程如下： 修改服务端配置，增加一个 user.name 的属性 修改 Controller ，增加一个请求方法，测试配置更新效果 123456789// 注入配置文件上下文@Autowiredprivate ConfigurableApplicationContext applicationContext;// 从上下文中读取配置@GetMapping(value = "/hi")public String sayHi() &#123; return "Hello " + applicationContext.getEnvironment().getProperty("user.name");&#125; 通过浏览器访问该接口，浏览器显示 1Hello cheng 修改服务端配置 此时观察控制台日志，你会发现我们已经成功刷新了配置 刷新浏览器，浏览器显示 1Hello chengChange 注意：你可以使用 spring.cloud.nacos.config.refresh.enabled=false 来关闭动态刷新 Nacos Config 多环境的配置Spring Boot Profile我们在做项目开发的时候，生产环境和测试环境的一些配置可能会不一样，有时候一些功能也可能会不一样，所以我们可能会在上线的时候手工修改这些配置信息。但是 Spring 中为我们提供了 Profile 这个功能。我们只需要在启动的时候添加一个虚拟机参数，激活自己环境所要用的 Profile 就可以了。 操作起来很简单，只需要为不同的环境编写专门的配置文件，如：application-dev.yml、application-prod.yml， 启动项目时只需要增加一个命令参数 --spring.profiles.active=环境配置 即可，启动命令如下： 1java -jar spring-cloud-alibaba-demo-nacos-provider-1.0.0-SNAPSHOT.jar --spring.profiles.active=prod Nacos Config Profilespring-cloud-starter-alibaba-nacos-config 在加载配置的时候，不仅仅加载了以 dataid 为 ${spring.application.name}.${file-extension:properties} 为前缀的基础配置，还加载了 dataid 为 ${spring.application.name}-${profile}.${file-extension:properties} 的基础配置。在日常开发中如果遇到多套环境下的不同配置，可以通过 Spring 提供的 ${spring.profiles.active} 这个配置项来配置。 此处我们以之前创建的 服务提供者 项目为例 在 Nacos Server 中增加配置增加一个名为 nacos-provider-config-prod.yaml 的配置 注意：此时，我将配置文件中的端口由 8081 -&gt; 8082 在项目中增加配置增加一个名为 bootstrap-prod.properties 的配置文件，内容如下： 1234spring.profiles.active=prodspring.application.name=nacos-provider-configspring.cloud.nacos.config.file-extension=yamlspring.cloud.nacos.config.server-addr=127.0.0.1:8848 主要增加了 spring.profiles.active=prod 配置，用于指定访问 Nacos Server 中的 nacos-provider-config-prod.yaml 配置 启动应用程序此时我们有两个配置文件，分别为 bootstrap.properties 和 bootstrap-prod.properties ，我们需要指定启动时加载哪一个配置文件，操作流程如下： Run -&gt; Edit Configurations.. 设置需要激活的配置 观察日志，判断是否成功加载配置]]></content>
      <categories>
        <category>Spring Cloud Alibaba</category>
        <category>Nacos</category>
      </categories>
      <tags>
        <tag>Spring Cloud Alibaba</tag>
        <tag>Nacos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[引入thymeleaf并支持非严格的HTML]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2F%E5%BC%95%E5%85%A5thymeleaf%E5%B9%B6%E6%94%AF%E6%8C%81%E9%9D%9E%E4%B8%A5%E6%A0%BC%E7%9A%84HTML%2F</url>
    <content type="text"><![CDATA[引入依赖主要增加 spring-boot-starter-thymeleaf 和 nekohtml 这两个依赖 spring-boot-starter-thymeleaf：Thymeleaf 自动配置 nekohtml：允许使用非严格的 HTML 语法 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;version&gt;1.9.22&lt;/version&gt;&lt;/dependency&gt; 在 application.yml 中配置 Thymeleaf1234567spring: thymeleaf: cache: false # 开发时关闭缓存,不然没法看到实时页面 mode: LEGACYHTML5 # 用非严格的 HTML encoding: UTF-8 servlet: content-type: text/html 修改 html 标签用于引入 thymeleaf 引擎，这样才可以在其他标签里使用 th:* 语法，声明如下： 12&lt;!DOCTYPE html SYSTEM &quot;http://www.thymeleaf.org/dtd/xhtml1-strict-thymeleaf-spring4-4.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;]]></content>
      <categories>
        <category>thymeleaf</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>thymeleaf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成MyBatis的Maven插件生成代码]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2F%E9%9B%86%E6%88%90MyBatis%E7%9A%84Maven%E6%8F%92%E4%BB%B6%E7%94%9F%E6%88%90%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[我们无需手动编写 实体类、DAO、XML 配置文件，只需要使用 MyBatis 提供的一个 Maven 插件就可以自动生成所需的各种文件便能够满足基本的业务需求，如果业务比较复杂只需要修改相关文件即可。 配置插件在 pom.xml 文件中增加 mybatis-generator-maven-plugin 插件 1234567891011121314151617181920212223242526&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;$&#123;basedir&#125;/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;3.4.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; configurationFile：自动生成所需的配置文件路径 自动生成的配置在 src/main/resources/generator/ 目录下创建 generatorConfig.xml 配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;generatorConfiguration&gt; &lt;!-- 引入数据库连接配置 --&gt; &lt;properties resource="jdbc.properties"/&gt; &lt;context id="Mysql" targetRuntime="MyBatis3Simple" defaultModelType="flat"&gt; &lt;property name="beginningDelimiter" value="`"/&gt; &lt;property name="endingDelimiter" value="`"/&gt; &lt;!-- 配置 tk.mybatis 插件 --&gt; &lt;plugin type="tk.mybatis.mapper.generator.MapperPlugin"&gt; &lt;property name="mappers" value="com.funtl.utils.MyMapper"/&gt; &lt;/plugin&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass="$&#123;jdbc.driverClass&#125;" connectionURL="$&#123;jdbc.connectionURL&#125;" userId="$&#123;jdbc.username&#125;" password="$&#123;jdbc.password&#125;"&gt; &lt;/jdbcConnection&gt; &lt;!-- 配置实体类存放路径 --&gt; &lt;javaModelGenerator targetPackage="com.funtl.hello.spring.boot.entity" targetProject="src/main/java"/&gt; &lt;!-- 配置 XML 存放路径 --&gt; &lt;sqlMapGenerator targetPackage="mapper" targetProject="src/main/resources"/&gt; &lt;!-- 配置 DAO 存放路径 --&gt; &lt;javaClientGenerator targetPackage="com.funtl.hello.spring.boot.mapper" targetProject="src/main/java" type="XMLMAPPER"/&gt; &lt;!-- 配置需要生成的表，% 代表所有 --&gt; &lt;table tableName="%"&gt; &lt;!-- mysql 配置 --&gt; &lt;generatedKey column="id" sqlStatement="Mysql" identity="true"/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 配置数据源在 src/main/resources 目录下创建 jdbc.properties 数据源配置： 1234jdbc.driverClass=com.mysql.jdbc.Driverjdbc.connectionURL=jdbc:mysql://ip:port/dbname?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsejdbc.username=rootjdbc.password=123456 插件自动生成命令1mvn mybatis-generator:generate 完整配置案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN""http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;!-- 配置生成器 --&gt;&lt;generatorConfiguration&gt;&lt;!-- 可以用于加载配置项或者配置文件，在整个配置文件中就可以使用$&#123;propertyKey&#125;的方式来引用配置项 resource：配置资源加载地址，使用resource，MBG从classpath开始找，比如com/myproject/generatorConfig.properties url：配置资源加载地质，使用URL的方式，比如file:///C:/myfolder/generatorConfig.properties. 注意，两个属性只能选址一个; 另外，如果使用了mybatis-generator-maven-plugin，那么在pom.xml中定义的properties都可以直接在generatorConfig.xml中使用&lt;properties resource="" url="" /&gt; --&gt; &lt;!-- 在MBG工作的时候，需要额外加载的依赖包 location属性指明加载jar/zip包的全路径&lt;classPathEntry location="/Program Files/IBM/SQLLIB/java/db2java.zip" /&gt; --&gt;&lt;!-- context:生成一组对象的环境 id:必选，上下文id，用于在生成错误时提示 defaultModelType:指定生成对象的样式 1，conditional：类似hierarchical； 2，flat：所有内容（主键，blob）等全部生成在一个对象中； 3，hierarchical：主键生成一个XXKey对象(key class)，Blob等单独生成一个对象，其他简单属性在一个对象中(record class) targetRuntime: 1，MyBatis3：默认的值，生成基于MyBatis3.x以上版本的内容，包括XXXBySample； 2，MyBatis3Simple：类似MyBatis3，只是不生成XXXBySample； introspectedColumnImpl：类全限定名，用于扩展MBG--&gt;&lt;context id="mysql" defaultModelType="hierarchical" targetRuntime="MyBatis3Simple" &gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name="autoDelimitKeywords" value="false"/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name="javaFileEncoding" value="UTF-8"/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name="javaFormatter" value="org.mybatis.generator.api.dom.DefaultJavaFormatter"/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name="xmlFormatter" value="org.mybatis.generator.api.dom.DefaultXmlFormatter"/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name="beginningDelimiter" value="`"/&gt; &lt;property name="endingDelimiter" value="`"/&gt; &lt;!-- 必须要有的，使用这个配置链接数据库 @TODO:是否可以扩展 --&gt; &lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql:///pss" userId="root" password="admin"&gt; &lt;!-- 这里面可以设置property属性，每一个property属性都设置到配置的Driver上 --&gt; &lt;/jdbcConnection&gt; &lt;!-- java类型处理器 用于处理DB中的类型到Java中的类型，默认使用JavaTypeResolverDefaultImpl； 注意一点，默认会先尝试使用Integer，Long，Short等来对应DECIMAL和 NUMERIC数据类型； --&gt; &lt;javaTypeResolver type="org.mybatis.generator.internal.types.JavaTypeResolverDefaultImpl"&gt; &lt;!-- true：使用BigDecimal对应DECIMAL和 NUMERIC数据类型 false：默认, scale&gt;0;length&gt;18：使用BigDecimal; scale=0;length[10,18]：使用Long； scale=0;length[5,9]：使用Integer； scale=0;length&lt;5：使用Short； --&gt; &lt;property name="forceBigDecimals" value="false"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- java模型创建器，是必须要的元素 负责：1，key类（见context的defaultModelType）；2，java类；3，查询类 targetPackage：生成的类要放的包，真实的包受enableSubPackages属性控制； targetProject：目标项目，指定一个存在的目录下，生成的内容会放到指定目录中，如果目录不存在，MBG不会自动建目录 --&gt; &lt;javaModelGenerator targetPackage="com._520it.mybatis.domain" targetProject="src/main/java"&gt; &lt;!-- for MyBatis3/MyBatis3Simple 自动为每一个生成的类创建一个构造方法，构造方法包含了所有的field；而不是使用setter； --&gt; &lt;property name="constructorBased" value="false"/&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;!-- for MyBatis3 / MyBatis3Simple 是否创建一个不可变的类，如果为true， 那么MBG会创建一个没有setter方法的类，取而代之的是类似constructorBased的类 --&gt; &lt;property name="immutable" value="false"/&gt; &lt;!-- 设置一个根对象， 如果设置了这个根对象，那么生成的keyClass或者recordClass会继承这个类；在Table的rootClass属性中可以覆盖该选项 注意：如果在key class或者record class中有root class相同的属性，MBG就不会重新生成这些属性了，包括： 1，属性名相同，类型相同，有相同的getter/setter方法； --&gt; &lt;property name="rootClass" value="com._520it.mybatis.domain.BaseDomain"/&gt; &lt;!-- 设置是否在getter方法中，对String类型字段调用trim()方法 --&gt; &lt;property name="trimStrings" value="true"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成SQL map的XML文件生成器， 注意，在Mybatis3之后，我们可以使用mapper.xml文件+Mapper接口（或者不用mapper接口）， 或者只使用Mapper接口+Annotation，所以，如果 javaClientGenerator配置中配置了需要生成XML的话，这个元素就必须配置 targetPackage/targetProject:同javaModelGenerator --&gt; &lt;sqlMapGenerator targetPackage="com._520it.mybatis.mapper" targetProject="src/main/resources"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对于mybatis来说，即生成Mapper接口，注意，如果没有配置该元素，那么默认不会生成Mapper接口 targetPackage/targetProject:同javaModelGenerator type：选择怎么生成mapper接口（在MyBatis3/MyBatis3Simple下）： 1，ANNOTATEDMAPPER：会生成使用Mapper接口+Annotation的方式创建（SQL生成在annotation中），不会生成对应的XML； 2，MIXEDMAPPER：使用混合配置，会生成Mapper接口，并适当添加合适的Annotation，但是XML会生成在XML中； 3，XMLMAPPER：会生成Mapper接口，接口完全依赖XML； 注意，如果context是MyBatis3Simple：只支持ANNOTATEDMAPPER和XMLMAPPER --&gt; &lt;javaClientGenerator targetPackage="com._520it.mybatis.mapper" type="ANNOTATEDMAPPER" targetProject="src/main/java"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;!-- 可以为所有生成的接口添加一个父接口，但是MBG只负责生成，不负责检查 &lt;property name="rootInterface" value=""/&gt; --&gt; &lt;/javaClientGenerator&gt; &lt;!-- 选择一个table来生成相关文件，可以有一个或多个table，必须要有table元素 选择的table会生成一下文件： 1，SQL map文件 2，生成一个主键类； 3，除了BLOB和主键的其他字段的类； 4，包含BLOB的类； 5，一个用户生成动态查询的条件类（selectByExample, deleteByExample），可选； 6，Mapper接口（可选） tableName（必要）：要生成对象的表名； 注意：大小写敏感问题。正常情况下，MBG会自动的去识别数据库标识符的大小写敏感度，在一般情况下，MBG会 根据设置的schema，catalog或tablename去查询数据表，按照下面的流程： 1，如果schema，catalog或tablename中有空格，那么设置的是什么格式，就精确的使用指定的大小写格式去查询； 2，否则，如果数据库的标识符使用大写的，那么MBG自动把表名变成大写再查找； 3，否则，如果数据库的标识符使用小写的，那么MBG自动把表名变成小写再查找； 4，否则，使用指定的大小写格式查询； 另外的，如果在创建表的时候，使用的""把数据库对象规定大小写，就算数据库标识符是使用的大写，在这种情况下也会使用给定的大小写来创建表名； 这个时候，请设置delimitIdentifiers="true"即可保留大小写格式； 可选： 1，schema：数据库的schema； 2，catalog：数据库的catalog； 3，alias：为数据表设置的别名，如果设置了alias，那么生成的所有的SELECT SQL语句中，列名会变成：alias_actualColumnName 4，domainObjectName：生成的domain类的名字，如果不设置，直接使用表名作为domain类的名字；可以设置为somepck.domainName，那么会自动把domainName类再放到somepck包里面； 5，enableInsert（默认true）：指定是否生成insert语句； 6，enableSelectByPrimaryKey（默认true）：指定是否生成按照主键查询对象的语句（就是getById或get）； 7，enableSelectByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询语句； 8，enableUpdateByPrimaryKey（默认true）：指定是否生成按照主键修改对象的语句（即update)； 9，enableDeleteByPrimaryKey（默认true）：指定是否生成按照主键删除对象的语句（即delete）； 10，enableDeleteByExample（默认true）：MyBatis3Simple为false，指定是否生成动态删除语句； 11，enableCountByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询总条数语句（用于分页的总条数查询）； 12，enableUpdateByExample（默认true）：MyBatis3Simple为false，指定是否生成动态修改语句（只修改对象中不为空的属性）； 13，modelType：参考context元素的defaultModelType，相当于覆盖； 14，delimitIdentifiers：参考tableName的解释，注意，默认的delimitIdentifiers是双引号，如果类似MYSQL这样的数据库，使用的是`（反引号，那么还需要设置context的beginningDelimiter和endingDelimiter属性） 15，delimitAllColumns：设置是否所有生成的SQL中的列名都使用标识符引起来。默认为false，delimitIdentifiers参考context的属性 注意，table里面很多参数都是对javaModelGenerator，context等元素的默认属性的一个复写； --&gt; &lt;table tableName="userinfo" &gt; &lt;!-- 参考 javaModelGenerator 的 constructorBased属性--&gt; &lt;property name="constructorBased" value="false"/&gt; &lt;!-- 默认为false，如果设置为true，在生成的SQL中，table名字不会加上catalog或schema； --&gt; &lt;property name="ignoreQualifiersAtRuntime" value="false"/&gt; &lt;!-- 参考 javaModelGenerator 的 immutable 属性 --&gt; &lt;property name="immutable" value="false"/&gt; &lt;!-- 指定是否只生成domain类，如果设置为true，只生成domain类，如果还配置了sqlMapGenerator，那么在mapper XML文件中，只生成resultMap元素 --&gt; &lt;property name="modelOnly" value="false"/&gt; &lt;!-- 参考 javaModelGenerator 的 rootClass 属性 &lt;property name="rootClass" value=""/&gt; --&gt; &lt;!-- 参考javaClientGenerator 的 rootInterface 属性 &lt;property name="rootInterface" value=""/&gt; --&gt; &lt;!-- 如果设置了runtimeCatalog，那么在生成的SQL中，使用该指定的catalog，而不是table元素上的catalog &lt;property name="runtimeCatalog" value=""/&gt; --&gt; &lt;!-- 如果设置了runtimeSchema，那么在生成的SQL中，使用该指定的schema，而不是table元素上的schema &lt;property name="runtimeSchema" value=""/&gt; --&gt; &lt;!-- 如果设置了runtimeTableName，那么在生成的SQL中，使用该指定的tablename，而不是table元素上的tablename &lt;property name="runtimeTableName" value=""/&gt; --&gt; &lt;!-- 注意，该属性只针对MyBatis3Simple有用； 如果选择的runtime是MyBatis3Simple，那么会生成一个SelectAll方法，如果指定了selectAllOrderByClause，那么会在该SQL中添加指定的这个order条件； --&gt; &lt;property name="selectAllOrderByClause" value="age desc,username asc"/&gt; &lt;!-- 如果设置为true，生成的model类会直接使用column本身的名字，而不会再使用驼峰命名方法，比如BORN_DATE，生成的属性名字就是BORN_DATE,而不会是bornDate --&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;!-- generatedKey用于生成生成主键的方法， 如果设置了该元素，MBG会在生成的&lt;insert&gt;元素中生成一条正确的&lt;selectKey&gt;元素，该元素可选 column:主键的列名； sqlStatement：要生成的selectKey语句，有以下可选项： Cloudscape:相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2 :相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2_MF :相当于selectKey的SQL为：SELECT IDENTITY_VAL_LOCAL() FROM SYSIBM.SYSDUMMY1 Derby :相当于selectKey的SQL为：VALUES IDENTITY_VAL_LOCAL() HSQLDB :相当于selectKey的SQL为：CALL IDENTITY() Informix :相当于selectKey的SQL为：select dbinfo('sqlca.sqlerrd1') from systables where tabid=1 MySql :相当于selectKey的SQL为：SELECT LAST_INSERT_ID() SqlServer :相当于selectKey的SQL为：SELECT SCOPE_IDENTITY() SYBASE :相当于selectKey的SQL为：SELECT @@IDENTITY JDBC :相当于在生成的insert元素上添加useGeneratedKeys="true"和keyProperty属性 &lt;generatedKey column="" sqlStatement=""/&gt; --&gt; &lt;!-- 该元素会在根据表中列名计算对象属性名之前先重命名列名，非常适合用于表中的列都有公用的前缀字符串的时候， 比如列名为：CUST_ID,CUST_NAME,CUST_EMAIL,CUST_ADDRESS等； 那么就可以设置searchString为"^CUST_"，并使用空白替换，那么生成的Customer对象中的属性名称就不是 custId,custName等，而是先被替换为ID,NAME,EMAIL,然后变成属性：id，name，email； 注意，MBG是使用java.util.regex.Matcher.replaceAll来替换searchString和replaceString的， 如果使用了columnOverride元素，该属性无效； &lt;columnRenamingRule searchString="" replaceString=""/&gt; --&gt; &lt;!-- 用来修改表中某个列的属性，MBG会使用修改后的列来生成domain的属性； column:要重新设置的列名； 注意，一个table元素中可以有多个columnOverride元素哈~ --&gt; &lt;columnOverride column="username"&gt; &lt;!-- 使用property属性来指定列要生成的属性名称 --&gt; &lt;property name="property" value="userName"/&gt; &lt;!-- javaType用于指定生成的domain的属性类型，使用类型的全限定名 &lt;property name="javaType" value=""/&gt; --&gt; &lt;!-- jdbcType用于指定该列的JDBC类型 &lt;property name="jdbcType" value=""/&gt; --&gt; &lt;!-- typeHandler 用于指定该列使用到的TypeHandler，如果要指定，配置类型处理器的全限定名 注意，mybatis中，不会生成到mybatis-config.xml中的typeHandler 只会生成类似：where id = #&#123;id,jdbcType=BIGINT,typeHandler=com._520it.mybatis.MyTypeHandler&#125;的参数描述 &lt;property name="jdbcType" value=""/&gt; --&gt; &lt;!-- 参考table元素的delimitAllColumns配置，默认为false &lt;property name="delimitedColumnName" value=""/&gt; --&gt; &lt;/columnOverride&gt; &lt;!-- ignoreColumn设置一个MGB忽略的列，如果设置了改列，那么在生成的domain中，生成的SQL中，都不会有该列出现 column:指定要忽略的列的名字； delimitedColumnName：参考table元素的delimitAllColumns配置，默认为false 注意，一个table元素中可以有多个ignoreColumn元素 &lt;ignoreColumn column="deptId" delimitedColumnName=""/&gt; --&gt; &lt;/table&gt;&lt;/context&gt;&lt;/generatorConfiguration&gt;]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot自定义Banner]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpringBoot%E8%87%AA%E5%AE%9A%E4%B9%89Banner(%E4%BD%9B%E7%A5%96%E4%BF%9D%E4%BD%91)%2F</url>
    <content type="text"><![CDATA[在 Spring Boot 启动的时候会有一个默认的启动图案 1234567. ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.5.8.RELEASE) 我们在 src/main/resources 目录下新建一个 banner.txt 通过 http://patorjk.com/software/taag 网站生成字符串，将网站生成的字符复制到 banner.txt 中 再次运行这个程序 1234567891011121314151617181920212223$&#123;AnsiColor.BRIGHT_RED&#125;////////////////////////////////////////////////////////////////////// _ooOoo_ //// o8888888o //// 88&quot; . &quot;88 //// (| ^_^ |) //// O\ = /O //// ____/`---&apos;\____ //// .&apos; \\| |// `. //// / \\||| : |||// \ //// / _||||| -:- |||||- \ //// | | \\\ - /// | | //// | \_| &apos;&apos;\---/&apos;&apos; | | //// \ .-\__ `-` ___/-. / //// ___`. .&apos; /--.--\ `. . ___ //// .&quot;&quot; &apos;&lt; `.___\_&lt;|&gt;_/___.&apos; &gt;&apos;&quot;&quot;. //// | | : `- \`.;`\ _ /`;.`/ - ` : | | //// \ \ `-. \_ __\ /__ _/ .-` / / //// ========`-.____`-.___\_____/___.-`____.-&apos;======== //// `=---=&apos; //// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ //// 佛祖保佑 永不宕机 永无BUG ////////////////////////////////////////////////////////////////////// 常用属性设置： ${AnsiColor.BRIGHT_RED}：设置控制台中输出内容的颜色 ${application.version}：用来获取 MANIFEST.MF 文件中的版本号 ${application.formatted-version}：格式化后的 ${application.version} 版本信息 ${spring-boot.version}：Spring Boot 的版本号 ${spring-boot.formatted-version}：格式化后的 ${spring-boot.version} 版本信息]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
        <tag>Banner</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot整合Druid]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpringBoot%E6%95%B4%E5%90%88Druid%2F</url>
    <content type="text"><![CDATA[引入依赖在 pom.xml 文件中引入 druid-spring-boot-starter 依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt; 引入数据库连接依赖 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 配置 application.yml在 application.yml 中配置数据库连接 1234567891011spring: datasource: druid: url: jdbc:mysql://ip:port/dbname?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot整合Mybatis分页助手pagehelper]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpringBoot%E6%95%B4%E5%90%88Mybatis%E5%88%86%E9%A1%B5%E5%8A%A9%E6%89%8B%2F</url>
    <content type="text"><![CDATA[文档 github地址 使用案例 快速上手在 pom.xml 中添加如下依赖：123456789101112131415161718&lt;!--mybatis--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!--mapper--&gt;&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.4&lt;/version&gt;&lt;/dependency&gt;&lt;!--pagehelper--&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.10&lt;/version&gt;&lt;/dependency&gt; 在service中使用分页助手1234567891011121314151617181920212223242526272829@Servicepublic class UserService &#123; @Autowired private UserMapper userMapper; public PageResult&lt;User&gt; queryUserByPageAndSort( Integer page, Integer rows, String sortBy, Boolean desc, String key) &#123; // 开始分页 PageHelper.startPage(page, rows); // 过滤 Example example = new Example(User.class); if (StringUtils.isNotBlank(key)) &#123; example.createCriteria().andLike("name", "%" + key + "%") .orEqualTo("letter", key.toUpperCase()); &#125; if (StringUtils.isNotBlank(sortBy)) &#123; // 排序 String orderByClause = sortBy + (desc ? " DESC" : " ASC"); example.setOrderByClause(orderByClause); &#125; // 查询 List&lt;User&gt; list = (Page&lt;User&gt;) brandMapper.selectByExample(example); // 解析分页结果 PageInfo&lt;User&gt; info = new PageInfo&lt;&gt;(list); // 返回结果 return new PageResult&lt;&gt;(info.getTotal(),list); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
        <category>pagehelper</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>pagehelper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot整合tk.mybatis]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpringBoot%E6%95%B4%E5%90%88tk.mybatis%2F</url>
    <content type="text"><![CDATA[tk.mybatis 简介tk.mybatis 是在 MyBatis 框架的基础上提供了很多工具，让开发更加高效 引入依赖在 pom.xml 文件中引入 mapper-spring-boot-starter 依赖，该依赖会自动引入 MyBaits 相关依赖 12345&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; 配置 application.yml配置 MyBatis 123mybatis: type-aliases-package: 实体类的存放路径，如：com.funtl.hello.spring.boot.entity mapper-locations: classpath:mapper/*.xml 创建一个通用的父级接口主要作用是让 DAO 层的接口继承该接口，以达到使用 tk.mybatis 的目的 1234567891011package tk.mybatis;import tk.mybatis.mapper.common.Mapper;import tk.mybatis.mapper.common.MySqlMapper;/** * 自己的 Mapper * 特别注意，该接口不能被扫描到，否则会出错 * 不能放在主启动类所在包及其子包下 */public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; &#123;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>tk.mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security oAuth2快速上手]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpring%20Security%20oAuth2%2F</url>
    <content type="text"><![CDATA[简介什么是 oAuthOAuth是一个开放标准，允许用户让第三方应用访问该用户在某一网站上存储的私密的资源，而无需将用户名和密码提供给第三方应用。 OAuth允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的网站在特定的时段内访问特定的资源。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信息，而非所有内容。 OAuth是OpenID的一个补充，但是完全不同的服务。 什么是 Spring SecuritySpring Security 是一个安全框架，前身是 Acegi Security，能够为 Spring 企业应用系统提供声明式的安全访问控制。Spring Security 基于 Servlet 过滤器、IoC 和 AOP，为 Web 请求和方法调用提供身份确认和授权处理，避免了代码耦合，减少了大量重复代码工作。 什么是oAuth2OAuth 2.0是行业标准协议进行授权。OAuth 2.0取代原来的工作OAuth协议创建于2006年。OAuth 2.0客户机开发者专注于简单而为web应用程序提供特定授权流,桌面应用程序,手机,和客厅设备。本规范及其扩展在IETF OAuth工作组正在开发。 oAuth2.0四种授权方式 implicit：简化模式(为web浏览器应用设计)(不支持refresh token) 简化模式是一个简化的流程,可以使用公共客户,立即返回访问令牌的没有额外的授权代码交换步骤(通常不推荐使用) authorization code：授权码模式(正宗方式)(支持refresh token) 授权码模式适用于有自己的服务器的应用，它是一个一次性的临时凭证，用来换取 access_token 和 refresh_token。授权码模式使用机密和公共客户交换授权代码的访问令牌。当用户通过重定向URL返回到客户端,这时应用就会从URL中获取授权代码并请求一个访问令牌 resource owner password credentials：密码模式(为遗留系统设计)(支持refresh token) 密码模式中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向 “服务商提供商” 索要授权。在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分。 client credentials：客户端模式(为后台api服务消费者设计)(不支持refresh token) 调用者是一个后端的模块，没有用户界面的时候，可以使用客户端模式。鉴权服务器直接对客户端进行身份验证，验证通过后，返回 token。 Spring 对 oAuth2 的实现是 Spring Cloud Security 案例创建父工程项目编写pom.xml文件` 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;spring-security-oauth2&lt;/name&gt; &lt;modules&gt; &lt;/modules&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.source&gt;$&#123;java.version&#125;&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;$&#123;java.version&#125;&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;default&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;spring-javaformat.version&gt;0.0.9&lt;/spring-javaformat.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;io.spring.javaformat&lt;/groupId&gt; &lt;artifactId&gt;spring-javaformat-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-javaformat.version&#125;&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*Tests.java&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/Abstract*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;systemPropertyVariables&gt; &lt;java.security.egd&gt;file:/dev/./urandom&lt;/java.security.egd&gt; &lt;java.awt.headless&gt;true&lt;/java.awt.headless&gt; &lt;/systemPropertyVariables&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-enforcer-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;enforce-rules&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;enforce&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;rules&gt; &lt;bannedDependencies&gt; &lt;excludes&gt; &lt;exclude&gt;commons-logging:*:*&lt;/exclude&gt; &lt;/excludes&gt; &lt;searchTransitive&gt;true&lt;/searchTransitive&gt; &lt;/bannedDependencies&gt; &lt;/rules&gt; &lt;fail&gt;true&lt;/fail&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-milestone&lt;/id&gt; &lt;name&gt;Spring Milestone&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshot&lt;/id&gt; &lt;name&gt;Spring Snapshot&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestone&lt;/id&gt; &lt;name&gt;Spring Milestone&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshot&lt;/id&gt; &lt;name&gt;Spring Snapshot&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/project&gt; 创建统一的依赖管理模块编写pom.xml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2-dependencies&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;properties&gt; &lt;spring-cloud.version&gt;Greenwich.RELEASE&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-milestone&lt;/id&gt; &lt;name&gt;Spring Milestone&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-snapshot&lt;/id&gt; &lt;name&gt;Spring Snapshot&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestone&lt;/id&gt; &lt;name&gt;Spring Milestone&lt;/name&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshot&lt;/id&gt; &lt;name&gt;Spring Snapshot&lt;/name&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/project&gt; 修改父工程项目pom.xml文件 在&lt;modules&gt;节点下添加： 1&lt;module&gt;spring-security-oauth2-dependencies&lt;/module&gt; 在&lt;dependencies&gt;节点下添加 1234567&lt;dependency&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 创建认证服务器编写pom.xml文件1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-security-oauth2-server&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!-- Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Security --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.oauth2.OAuth2ServerApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 编写应用启动程序12345678910111213141516package club.codeopen.oauth2;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;/** * * @author by cheng * @Date 2019/5/2 */@SpringBootApplicationpublic class OAuth2ServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OAuth2ServerApplication.class,args); &#125;&#125; 基于内存存储令牌操作流程 配置认证服务器 配置客户端信息： 1ClientDetailsServiceConfigurer inMemory：内存配置 withClient：客户端标识 secret：客户端安全码 authorizedGrantTypes：客户端授权类型 scopes：客户端授权范围 redirectUris：注册回调地址 配置 Web 安全 通过 GET 请求访问认证服务器获取授权码 端点：/oauth/authorize 通过 POST 请求利用授权码访问认证服务器获取令牌 端点：/oauth/token 附：默认的端点 URL /oauth/authorize：授权端点 /oauth/token：令牌端点 /oauth/confirm_access：用户确认授权提交端点 /oauth/error：授权服务错误信息端点 /oauth/check_token：用于资源服务访问的令牌解析端点 /oauth/token_key：提供公有密匙的端点，如果你使用 JWT 令牌的话 配置认证服务器创建一个类继承 AuthorizationServerConfigurerAdapter 并添加相关注解： @Configuration @EnableAuthorizationServer 1234567891011121314151617181920212223242526272829303132package club.codeopen.oauth2.server.config;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;import org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;/** * * @author by cheng * @Date 2019/5/2 */@Configuration@EnableAuthorizationServerpublic class AuthorizationConfiguration extends AuthorizationServerConfigurerAdapter &#123; @Autowired private BCryptPasswordEncoder bCryptPasswordEncoder; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients // 配置客户端 .inMemory() // 使用内存设置 .withClient("client") // client_id .secret(bCryptPasswordEncoder.encode("secret")) // client_secret .authorizedGrantTypes("authorization_code") // 授权类型 .scopes("app") // 授权范围 .redirectUris("https://www.codeopen.club"); // 注册回调地址 &#125;&#125; 服务器安全配置创建一个类继承 WebSecurityConfigurerAdapter 并添加相关注解： @Configuration @EnableWebSecurity @EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)：全局方法拦截 123456789101112131415161718192021222324252627282930313233package club.codeopen.oauth2.server.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;/** * * @author by cheng * @Date 2019/5/2 */@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true,securedEnabled = true,jsr250Enabled = true)public class WebSecurityConfiguration extends WebSecurityConfigurerAdapter &#123; @Bean public BCryptPasswordEncoder bCryptPasswordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.inMemoryAuthentication() .withUser("admin").password(bCryptPasswordEncoder().encode("111")).roles("ADMIN") .and() .withUser("jack").password(bCryptPasswordEncoder().encode("111")).roles("USER"); &#125;&#125; 编写配置文件application.yml123456spring: application: name: oauth2-serverserver: port: 8080 获取访问授权码使用浏览器访问如下地址 1http://localhost:8080/oauth/authorize?client_id=client&amp;response_type=code 第一次访问会跳转到登录页面 验证成功后会询问用户是否授权客户端 选择授权后会跳转到我的博客，浏览器地址上还会包含一个授权码 （code=0H9Hrm），浏览器地址栏会显示如下地址： 1https://www.codeopen.club/?code=0H9Hrm 有了这个授权码就可以获取访问令牌了 通过授权码向服务器申请令牌方式一：通过CURL方式请求 1curl -X POST -H &quot;Content-Type: application/x-www-form-urlencoded&quot; -d &apos;grant_type=authorization_code&amp;code=0H9Hrm&apos; &quot;http://client:secret@localhost:8080/oauth/token&quot; 方式二：使用Postman请求 得到的响应结果如下： 123456&#123; &quot;access_token&quot;: &quot;ac7b5f4f-848d-409c-a086-32e4d3d55135&quot;, &quot;token_type&quot;: &quot;bearer&quot;, &quot;expires_in&quot;: 43199, &quot;scope&quot;: &quot;app&quot;&#125; 基于 JDBC 存储令牌操作流程 初始化 oAuth2 相关表 在数据库中配置客户端 配置认证服务器 配置数据源：DataSource 配置令牌存储方式：TokenStore -&gt; JdbcTokenStore 配置客户端读取方式：ClientDetailsService -&gt; JdbcClientDetailsService 配置服务端点信息： 1AuthorizationServerEndpointsConfigurer tokenStore：设置令牌存储方式 配置客户端信息： 1ClientDetailsServiceConfigurer withClientDetails：设置客户端配置读取方式 配置 Web 安全 配置密码加密方式：BCryptPasswordEncoder 配置认证信息：AuthenticationManagerBuilder 通过 GET 请求访问认证服务器获取授权码 端点：/oauth/authorize 通过 POST 请求利用授权码访问认证服务器获取令牌 端点：/oauth/token 附：默认的端点 URL /oauth/authorize：授权端点 /oauth/token：令牌端点 /oauth/confirm_access：用户确认授权提交端点 /oauth/error：授权服务错误信息端点 /oauth/check_token：用于资源服务访问的令牌解析端点 /oauth/token_key：提供公有密匙的端点，如果你使用 JWT 令牌的话 初始化 oAuth2 相关表使用官方提供的建表脚本初始化 oAuth2 相关表，SQL脚本地址 由于我们使用的是 MySQL 数据库，默认建表语句中主键为 VARCHAR(256)，这超过了最大的主键长度，请手动修改为 128，并用 BLOB 替换语句中的 LONGVARBINARY 类型，修改后的建表脚本如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869CREATE TABLE `clientdetails` ( `appId` varchar(128) NOT NULL, `resourceIds` varchar(256) DEFAULT NULL, `appSecret` varchar(256) DEFAULT NULL, `scope` varchar(256) DEFAULT NULL, `grantTypes` varchar(256) DEFAULT NULL, `redirectUrl` varchar(256) DEFAULT NULL, `authorities` varchar(256) DEFAULT NULL, `access_token_validity` int(11) DEFAULT NULL, `refresh_token_validity` int(11) DEFAULT NULL, `additionalInformation` varchar(4096) DEFAULT NULL, `autoApproveScopes` varchar(256) DEFAULT NULL, PRIMARY KEY (`appId`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `oauth_access_token` ( `token_id` varchar(256) DEFAULT NULL, `token` blob, `authentication_id` varchar(128) NOT NULL, `user_name` varchar(256) DEFAULT NULL, `client_id` varchar(256) DEFAULT NULL, `authentication` blob, `refresh_token` varchar(256) DEFAULT NULL, PRIMARY KEY (`authentication_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `oauth_approvals` ( `userId` varchar(256) DEFAULT NULL, `clientId` varchar(256) DEFAULT NULL, `scope` varchar(256) DEFAULT NULL, `status` varchar(10) DEFAULT NULL, `expiresAt` timestamp NULL DEFAULT NULL, `lastModifiedAt` timestamp NULL DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `oauth_client_details` ( `client_id` varchar(128) NOT NULL, `resource_ids` varchar(256) DEFAULT NULL, `client_secret` varchar(256) DEFAULT NULL, `scope` varchar(256) DEFAULT NULL, `authorized_grant_types` varchar(256) DEFAULT NULL, `web_server_redirect_uri` varchar(256) DEFAULT NULL, `authorities` varchar(256) DEFAULT NULL, `access_token_validity` int(11) DEFAULT NULL, `refresh_token_validity` int(11) DEFAULT NULL, `additional_information` varchar(4096) DEFAULT NULL, `autoapprove` varchar(256) DEFAULT NULL, PRIMARY KEY (`client_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `oauth_client_token` ( `token_id` varchar(256) DEFAULT NULL, `token` blob, `authentication_id` varchar(128) NOT NULL, `user_name` varchar(256) DEFAULT NULL, `client_id` varchar(256) DEFAULT NULL, PRIMARY KEY (`authentication_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `oauth_code` ( `code` varchar(256) DEFAULT NULL, `authentication` blob) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE `oauth_refresh_token` ( `token_id` varchar(256) DEFAULT NULL, `token` blob, `authentication` blob) ENGINE=InnoDB DEFAULT CHARSET=utf8; 在数据库中配置客户端在表 oauth_client_details 中增加一条客户端配置记录，需要设置的字段如下： client_id：客户端标识 client_secret：客户端安全码，此处不能是明文，需要加密 scope：客户端授权范围 authorized_grant_types：客户端授权类型 web_server_redirect_uri：服务器回调地址 使用 BCryptPasswordEncoder 为客户端安全码加密，代码如下： 1System.out.println(new BCryptPasswordEncoder().encode("secret")); 数据库配置客户端效果图如下： 修改统一依赖管理的pom.xml文件在&lt;dependencies&gt;节点新增如下内容 123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt; &lt;version&gt;$&#123;hikaricp.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;exclusions&gt; &lt;!-- 排除 tomcat-jdbc 以使用 HikariCP --&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt;&lt;/dependency&gt; 配置认证服务器修改pom.xml 在&lt;dependencies&gt;节点新增如下内容 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;exclusions&gt; &lt;!-- 排除 tomcat-jdbc 以使用 HikariCP --&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 修改spring-security-oauth2-server的AuthorizationConfiguration类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package club.codeopen.oauth2.server.config;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.boot.jdbc.DataSourceBuilder;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Primary;import org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;import org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;import org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerEndpointsConfigurer;import org.springframework.security.oauth2.provider.ClientDetailsService;import org.springframework.security.oauth2.provider.client.JdbcClientDetailsService;import org.springframework.security.oauth2.provider.token.TokenStore;import org.springframework.security.oauth2.provider.token.store.JdbcTokenStore;import javax.sql.DataSource;/** * * @author by cheng * @Date 2019/5/2 */@Configuration@EnableAuthorizationServerpublic class AuthorizationConfiguration extends AuthorizationServerConfigurerAdapter &#123; @Bean @Primary @ConfigurationProperties(prefix = "spring.datasource") public DataSource dataSource() &#123; // 配置数据源（注意，我使用的是 HikariCP 连接池），以上注解是指定数据源，否则会有冲突 return DataSourceBuilder.create().build(); &#125; @Bean public TokenStore tokenStore() &#123; // 基于 JDBC 实现，令牌保存到数据 return new JdbcTokenStore(dataSource()); &#125; @Bean public ClientDetailsService jdbcClientDetails() &#123; // 基于 JDBC 实现，需要事先在数据库配置客户端信息 return new JdbcClientDetailsService(dataSource()); &#125; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; // 设置令牌 endpoints.tokenStore(tokenStore()); &#125; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; // 读取客户端配置 clients.withClientDetails(jdbcClientDetails()); &#125;&#125; 配置应用配置文件application.yml1234567891011121314151617181920spring: application: name: oauth2-server datasource: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.jdbc.Driver jdbc-url: jdbc:mysql://127.0.0.1:3306/oauth2?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: root hikari: minimum-idle: 5 idle-timeout: 600000 maximum-pool-size: 10 auto-commit: true pool-name: MyHikariCP max-lifetime: 1800000 connection-timeout: 30000 connection-test-query: SELECT 1server: port: 8080 验证参照基于内存存储令牌 获取令牌成功后，数据库 oauth_access_token 表中会增加一笔记录，效果图如下： 附录 第三方应用程序（Third-party application）： 又称之为客户端（client），比如上节中提到的设备（PC、Android、iPhone、TV、Watch），我们会在这些设备中安装我们自己研发的 APP。又比如我们的产品想要使用 QQ、微信等第三方登录。对我们的产品来说，QQ、微信登录是第三方登录系统。我们又需要第三方登录系统的资源（头像、昵称等）。对于 QQ、微信等系统我们又是第三方应用程序。 HTTP 服务提供商（HTTP service）： 我们的云笔记产品以及 QQ、微信等都可以称之为“服务提供商”。 资源所有者（Resource Owner）： 又称之为用户（user）。 用户代理（User Agent）： 比如浏览器，代替用户去访问这些资源。 认证服务器（Authorization server）： 即服务提供商专门用来处理认证的服务器，简单点说就是登录功能（验证用户的账号密码是否正确以及分配相应的权限） 资源服务器（Resource server）： 即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。简单点说就是资源的访问入口，比如上节中提到的“云笔记服务”和“云相册服务”都可以称之为资源服务器。]]></content>
      <categories>
        <category>Spring Security oAuth2</category>
      </categories>
      <tags>
        <tag>Spring Security oAuth2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security oAuth2实现RBAC基于角色权限控制]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpring%20Security%20oAuth2%E5%AE%9E%E7%8E%B0RBAC%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[创建认证服务器概述RBAC（Role-Based Access Control，基于角色的访问控制），就是用户通过角色与权限进行关联。简单地说，一个用户拥有若干角色，每一个角色拥有若干权限。这样，就构造成“用户-角色-权限”的授权模型。在这种模型中，用户与角色之间，角色与权限之间，一般是多对多的关系。（如下图） 目标 对系统的所有资源进行权限控制，系统中的资源包括： 静态资源（对象资源）：功能操作、数据列 动态资源（数据资源）：数据 对应用系统的所有对象资源和数据资源进行权限控制，比如：功能菜单、界面按钮、数据显示的列、各种行级数据进行权限的操控 对象关系权限系统的所有权限信息。权限具有上下级关系，是一个树状的结构。如： 系统管理 用户管理 查看用户 新增用户 修改用户 删除用户 用户系统的具体操作者，可以归属于一个或多个角色，它与角色的关系是多对多的关系 角色 为了对许多拥有相似权限的用户进行分类管理，定义了角色的概念，例如系统管理员、管理员、用户、访客等角色。角色具有上下级关系，可以形成树状视图，父级角色的权限是自身及它的所有子角色的权限的综合。父级角色的用户、父级角色的组同理可推 关联图 功能模块图 数据表结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950CREATE TABLE `tb_permission` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `parent_id` bigint(20) DEFAULT NULL COMMENT '父权限', `name` varchar(64) NOT NULL COMMENT '权限名称', `enname` varchar(64) NOT NULL COMMENT '权限英文名称', `url` varchar(255) NOT NULL COMMENT '授权路径', `description` varchar(200) DEFAULT NULL COMMENT '备注', `created` datetime NOT NULL, `updated` datetime NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8 COMMENT='权限表';CREATE TABLE `tb_role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `parent_id` bigint(20) DEFAULT NULL COMMENT '父角色', `name` varchar(64) NOT NULL COMMENT '角色名称', `enname` varchar(64) NOT NULL COMMENT '角色英文名称', `description` varchar(200) DEFAULT NULL COMMENT '备注', `created` datetime NOT NULL, `updated` datetime NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8 COMMENT='角色表';CREATE TABLE `tb_role_permission` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `role_id` bigint(20) NOT NULL COMMENT '角色 ID', `permission_id` bigint(20) NOT NULL COMMENT '权限 ID', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8 COMMENT='角色权限表';CREATE TABLE `tb_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `username` varchar(50) NOT NULL COMMENT '用户名', `password` varchar(64) NOT NULL COMMENT '密码，加密存储', `phone` varchar(20) DEFAULT NULL COMMENT '注册手机号', `email` varchar(50) DEFAULT NULL COMMENT '注册邮箱', `created` datetime NOT NULL, `updated` datetime NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `username` (`username`) USING BTREE, UNIQUE KEY `phone` (`phone`) USING BTREE, UNIQUE KEY `email` (`email`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8 COMMENT='用户表';CREATE TABLE `tb_user_role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `user_id` bigint(20) NOT NULL COMMENT '用户 ID', `role_id` bigint(20) NOT NULL COMMENT '角色 ID', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8 COMMENT='用户角色表'; 实现流程 使用基于JDBC存储令牌的代码进行再次开发 初始化 RBAC 相关表 在数据库中配置“用户”、“角色”、“权限”相关信息 数据库操作使用 tk.mybatis 框架，故需要增加相关依赖 配置 Web 安全 配置使用自定义认证与授权 通过 GET 请求访问认证服务器获取授权码 端点：/oauth/authorize 通过 POST 请求利用授权码访问认证服务器获取令牌 端点：/oauth/token 附：默认的端点 URL /oauth/authorize：授权端点 /oauth/token：令牌端点 /oauth/confirm_access：用户确认授权提交端点 /oauth/error：授权服务错误信息端点 /oauth/check_token：用于资源服务访问的令牌解析端点 /oauth/token_key：提供公有密匙的端点，如果你使用 JWT 令牌的话 初始化 RBAC 相关表1234567891011121314151617181920insert into `tb_permission`(`id`,`parent_id`,`name`,`enname`,`url`,`description`,`created`,`updated`) values (37,0,'系统管理','System','/',NULL,'2019-04-04 23:22:54','2019-04-04 23:22:56'),(38,37,'用户管理','SystemUser','/users/',NULL,'2019-04-04 23:25:31','2019-04-04 23:25:33'),(39,38,'查看用户','SystemUserView','',NULL,'2019-04-04 15:30:30','2019-04-04 15:30:43'),(40,38,'新增用户','SystemUserInsert','',NULL,'2019-04-04 15:30:31','2019-04-04 15:30:44'),(41,38,'编辑用户','SystemUserUpdate','',NULL,'2019-04-04 15:30:32','2019-04-04 15:30:45'),(42,38,'删除用户','SystemUserDelete','',NULL,'2019-04-04 15:30:48','2019-04-04 15:30:45');insert into `tb_role`(`id`,`parent_id`,`name`,`enname`,`description`,`created`,`updated`) values (37,0,'超级管理员','admin',NULL,'2019-04-04 23:22:03','2019-04-04 23:22:05');insert into `tb_role_permission`(`id`,`role_id`,`permission_id`) values (37,37,37),(38,37,38),(39,37,39),(40,37,40),(41,37,41),(42,37,42);insert into `tb_user`(`id`,`username`,`password`,`phone`,`email`,`created`,`updated`) values (37,'admin','$2a$10$9ZhDOBp.sRKat4l14ygu/.LscxrMUcDAfeVOEPiYwbcRkoB09gCmi','15888888888','chengpenglog@163.com','2019-05-04 23:21:27','2019-05-04 23:21:29');insert into `tb_user_role`(`id`,`user_id`,`role_id`) values (37,37,37); 新增tk.mybatis依赖 修改spring-security-oauth2-dependencies的pom.xml 在properties新增配置 1&lt;mapper.tk.mybatis&gt;2.1.5&lt;/mapper.tk.mybatis&gt; 在dependencies新增依赖： 12345&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mapper.tk.mybatis&#125;&lt;/version&gt;&lt;/dependency&gt; 修改spring-security-oauth2-server的pom.xml 在dependencies新增依赖： 1234&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 修改认证服务器启动类增加了 Mapper 的包扫描配置注解 1@MapperScan(basePackages = "club.codeopen.oauth2.server.mapper") 获取用户信息 目的是为了实现自定义认证授权时可以通过数据库查询用户信息，Spring Security oAuth2 要求使用 username 的方式查询，提供相关用户信息后，认证工作由框架自行完成 1234567891011121314151617181920212223242526package club.codeopen.oauth2.server.service.impl;import club.codeopen.oauth2.server.domain.TbUser;import club.codeopen.oauth2.server.mapper.TbUserMapper;import club.codeopen.oauth2.server.service.TbUserService;import org.springframework.stereotype.Service;import tk.mybatis.mapper.entity.Example;import javax.annotation.Resource;/** * @author by cheng * @Date 2019/5/4 */@Servicepublic class TbUserServiceImpl implements TbUserService&#123; @Resource private TbUserMapper tbUserMapper; @Override public TbUser selectByUsername(String username) &#123; Example example = new Example(TbUser.class); example.createCriteria().andEqualTo("username",username); return tbUserMapper.selectOneByExample(example); &#125;&#125; 获取用户权限信息认证成功后需要给用户授权，具体的权限已经存储在数据库里了，编码配置如下： 编辑TbPermissionMapper.java接口 123public interface TbPermissionMapper extends MyMapper&lt;TbPermission&gt; &#123; List&lt;TbPermission&gt; selectByUserId(@Param(&quot;userId&quot;) Long userId);&#125; 编辑TbPermissionMapper.xml文件，新增如下语句： 123456789101112 &lt;select id=&quot;selectByUserId&quot; parameterType=&quot;long&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT p.* FROM tb_user u LEFT JOIN tb_user_role ur ON u.id = ur.user_id LEFT JOIN tb_role r ON r.id = ur.role_id LEFT JOIN tb_role_permission rp ON rp.role_id = r.id LEFT JOIN tb_permission p ON p.id = rp.permission_id WHERE u.id = $&#123;userId&#125;&lt;/select&gt; 自定义认证授权实现类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package club.codeopen.oauth2.server.config.service;import club.codeopen.oauth2.server.domain.TbPermission;import club.codeopen.oauth2.server.domain.TbUser;import club.codeopen.oauth2.server.service.TbPermissionService;import club.codeopen.oauth2.server.service.TbUserService;import org.assertj.core.util.Lists;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.security.core.GrantedAuthority;import org.springframework.security.core.authority.SimpleGrantedAuthority;import org.springframework.security.core.userdetails.User;import org.springframework.security.core.userdetails.UserDetails;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.core.userdetails.UsernameNotFoundException;import org.springframework.stereotype.Service;import java.util.List;/** * 自定义用户认证与授权 * * @author by cheng * @Date 2019/5/4 */@Servicepublic class UserDeatilsServiceImpl implements UserDetailsService &#123; @Autowired private TbUserService tbUserService; @Autowired private TbPermissionService tbPermissionService; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; // 查询用户信息 TbUser tbUser = tbUserService.selectByUsername(username); List&lt;GrantedAuthority&gt; grantedAuthorities = Lists.newArrayList(); if (tbUser != null) &#123; // 获取用户授权 List&lt;TbPermission&gt; tbPermissions = tbPermissionService.selectByUserId(tbUser.getId()); // 声明用户授权 tbPermissions.forEach(tbPermission -&gt; &#123; if (tbPermission != null &amp;&amp; tbPermission.getEnname() != null) &#123; GrantedAuthority grantedAuthority = new SimpleGrantedAuthority(tbPermission.getEnname()); grantedAuthorities.add(grantedAuthority); &#125; &#125;); &#125; // 由框架完成认证工作 return new User(tbUser.getUsername(), tbUser.getPassword(), grantedAuthorities); &#125;&#125; 修改服务器安全配置类WebSecurityConfigurerAdapter.java 1234567891011121314151617181920212223242526272829303132333435package club.codeopen.oauth2.server.config;import club.codeopen.oauth2.server.config.service.UserDeatilsServiceImpl;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;/** * * @author by cheng * @Date 2019/5/2 */@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true,securedEnabled = true,jsr250Enabled = true)public class WebSecurityConfiguration extends WebSecurityConfigurerAdapter &#123; @Bean public UserDetailsService userDetailsService()&#123; return new UserDeatilsServiceImpl(); &#125; @Bean public BCryptPasswordEncoder bCryptPasswordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; // 使用自定义认证与授权 auth.userDetailsService(userDetailsService()); &#125;&#125; 编辑application.yml配置文件 增加 Mybatis 配置 123mybatis: type-aliases-package: club.codeopen.oauth2.server.domain mapper-locations: classpath:mapper/*.xml 验证参照基于内存存储令牌方式验证认证与授权 创建资源服务器在开发资源服务器之前，我们需要对认证服务器的配置进行修改，需要修改的内容如下： 解决访问 /oauth/check_token 端点的 403 问题 优化 RBAC 模型数据，以便更好的演示资源服务器的概念 认证服务器配置服务器安全配置资源服务器需要访问 /oauth/check_token 端点来检查 access_token 的有效性，此时该端点是受保护的资源，当我们访问该端点时会遇到 403 问题，将该端点暴露出来即可，在WebSecurityConfiguration.java中新增暴露端点的关键代码为： 12345@Overridepublic void configure(WebSecurity web) throws Exception &#123; // 将 check_token 暴露出去，否则资源服务器访问时报 403 错误 web.ignoring().antMatchers("/oauth/check_token");&#125; 重新初始化 RBAC 相关表增加了内容管理权限的数据，以便于我演示资源服务器的用法，SQL 语句如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485DROP TABLE IF EXISTS `tb_permission`;CREATE TABLE `tb_permission` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `parent_id` bigint(20) DEFAULT NULL COMMENT '父权限', `name` varchar(64) NOT NULL COMMENT '权限名称', `enname` varchar(64) NOT NULL COMMENT '权限英文名称', `url` varchar(255) NOT NULL COMMENT '授权路径', `description` varchar(200) DEFAULT NULL COMMENT '备注', `created` datetime NOT NULL, `updated` datetime NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=49 DEFAULT CHARSET=utf8 COMMENT='权限表';insert into `tb_permission`(`id`,`parent_id`,`name`,`enname`,`url`,`description`,`created`,`updated`) values (37,0,'系统管理','System','/',NULL,now(),now()),(38,37,'用户管理','SystemUser','/users/',NULL,now(),now()),(39,38,'查看用户','SystemUserView','/users/view/**',NULL,now(),now()),(40,38,'新增用户','SystemUserInsert','/users/insert/**',NULL,now(),now()),(41,38,'编辑用户','SystemUserUpdate','/users/update/**',NULL,now(),now()),(42,38,'删除用户','SystemUserDelete','/users/delete/**',NULL,now(),now()),(44,37,'内容管理','SystemContent','/contents/',NULL,now(),now()),(45,44,'查看内容','SystemContentView','/contents/view/**',NULL,now(),now()),(46,44,'新增内容','SystemContentInsert','/contents/insert/**',NULL,now(),now()),(47,44,'编辑内容','SystemContentUpdate','/contents/update/**',NULL,now(),now()),(48,44,'删除内容','SystemContentDelete','/contents/delete/**',NULL,now(),now());DROP TABLE IF EXISTS `tb_role`;CREATE TABLE `tb_role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `parent_id` bigint(20) DEFAULT NULL COMMENT '父角色', `name` varchar(64) NOT NULL COMMENT '角色名称', `enname` varchar(64) NOT NULL COMMENT '角色英文名称', `description` varchar(200) DEFAULT NULL COMMENT '备注', `created` datetime NOT NULL, `updated` datetime NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=38 DEFAULT CHARSET=utf8 COMMENT='角色表';insert into `tb_role`(`id`,`parent_id`,`name`,`enname`,`description`,`created`,`updated`) values (37,0,'超级管理员','admin',NULL,now(),now());DROP TABLE IF EXISTS `tb_role_permission`;CREATE TABLE `tb_role_permission` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `role_id` bigint(20) NOT NULL COMMENT '角色 ID', `permission_id` bigint(20) NOT NULL COMMENT '权限 ID', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=48 DEFAULT CHARSET=utf8 COMMENT='角色权限表';insert into `tb_role_permission`(`id`,`role_id`,`permission_id`) values (37,37,37),(38,37,38),(39,37,39),(40,37,40),(41,37,41),(42,37,42),(43,37,44),(44,37,45),(45,37,46),(46,37,47),(47,37,48);DROP TABLE IF EXISTS `tb_user`;CREATE TABLE `tb_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `username` varchar(50) NOT NULL COMMENT '用户名', `password` varchar(64) NOT NULL COMMENT '密码，加密存储', `phone` varchar(20) DEFAULT NULL COMMENT '注册手机号', `email` varchar(50) DEFAULT NULL COMMENT '注册邮箱', `created` datetime NOT NULL, `updated` datetime NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `username` (`username`) USING BTREE, UNIQUE KEY `phone` (`phone`) USING BTREE, UNIQUE KEY `email` (`email`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=38 DEFAULT CHARSET=utf8 COMMENT='用户表';insert into `tb_user`(`id`,`username`,`password`,`phone`,`email`,`created`,`updated`) values (37,'admin','$2a$10$9ZhDOBp.sRKat4l14ygu/.LscxrMUcDAfeVOEPiYwbcRkoB09gCmi','18888888888','chengpenglog@163.com',now(),now());DROP TABLE IF EXISTS `tb_user_role`;CREATE TABLE `tb_user_role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `user_id` bigint(20) NOT NULL COMMENT '用户 ID', `role_id` bigint(20) NOT NULL COMMENT '角色 ID', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=38 DEFAULT CHARSET=utf8 COMMENT='用户角色表';insert into `tb_user_role`(`id`,`user_id`,`role_id`) values (37,37,37); 创建资源服务器模块操作流程 初始化资源服务器数据库 POM 所需依赖同认证服务器 配置资源服务器 配置资源(Controller) 初始化资源服务器数据库 首先创建一个数据库oauth2-resource 初始化数据库表 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849CREATE TABLE `tb_content` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `category_id` bigint(20) NOT NULL COMMENT '内容类目ID', `title` varchar(200) DEFAULT NULL COMMENT '内容标题', `sub_title` varchar(100) DEFAULT NULL COMMENT '子标题', `title_desc` varchar(500) DEFAULT NULL COMMENT '标题描述', `url` varchar(500) DEFAULT NULL COMMENT '链接', `pic` varchar(300) DEFAULT NULL COMMENT '图片绝对路径', `pic2` varchar(300) DEFAULT NULL COMMENT '图片2', `content` text COMMENT '内容', `created` datetime DEFAULT NULL, `updated` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `category_id` (`category_id`), KEY `updated` (`updated`)) ENGINE=InnoDB AUTO_INCREMENT=42 DEFAULT CHARSET=utf8;insert into `tb_content`(`id`,`category_id`,`title`,`sub_title`,`title_desc`,`url`,`pic`,`pic2`,`content`,`created`,`updated`) values (28,89,'标题','子标题','标题说明','http://www.jd.com',NULL,NULL,NULL,now(),now()),(29,89,'ad2','ad2','ad2','http://www.baidu.com',NULL,NULL,NULL,now(),now()),(30,89,'ad3','ad3','ad3','http://www.sina.com.cn',NULL,NULL,NULL,now(),now()),(31,89,'ad4','ad4','ad4','https://www.codeopen.club',NULL,NULL,NULL,now(),now());CREATE TABLE `tb_content_category` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '类目ID', `parent_id` bigint(20) DEFAULT NULL COMMENT '父类目ID=0时，代表的是一级的类目', `name` varchar(50) DEFAULT NULL COMMENT '分类名称', `status` int(1) DEFAULT '1' COMMENT '状态。可选值:1(正常),2(删除)', `sort_order` int(4) DEFAULT NULL COMMENT '排列序号，表示同级类目的展现次序，如数值相等则按名称次序排列。取值范围:大于零的整数', `is_parent` tinyint(1) DEFAULT '1' COMMENT '该类目是否为父类目，1为true，0为false', `created` datetime DEFAULT NULL COMMENT '创建时间', `updated` datetime DEFAULT NULL COMMENT '创建时间', PRIMARY KEY (`id`), KEY `parent_id` (`parent_id`,`status`) USING BTREE, KEY `sort_order` (`sort_order`)) ENGINE=InnoDB AUTO_INCREMENT=98 DEFAULT CHARSET=utf8 COMMENT='内容分类';insert into `tb_content_category`(`id`,`parent_id`,`name`,`status`,`sort_order`,`is_parent`,`created`,`updated`) values (30,0,'LeeShop',1,1,1,now(),now()),(86,30,'首页',1,1,1,now(),now()),(87,30,'列表页面',1,1,1,now(),now()),(88,30,'详细页面',1,1,1,now(),now()),(89,86,'大广告',1,1,0,now(),now()),(90,86,'小广告',1,1,0,now(),now()),(91,86,'商城快报',1,1,0,now(),now()),(92,87,'边栏广告',1,1,0,now(),now()),(93,87,'页头广告',1,1,0,now(),now()),(94,87,'页脚广告',1,1,0,now(),now()),(95,88,'边栏广告',1,1,0,now(),now()),(96,86,'中广告',1,1,1,now(),now()),(97,96,'中广告1',1,1,0,now(),now()); 实现资源服务器模块 创建一个模块：spring-security-oauth2-resource 编写pom.xml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;club.codeopen&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;spring-security-oauth2-resource&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!-- Spring Boot --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Security --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;exclusions&gt; &lt;!-- 排除 tomcat-jdbc 以使用 HikariCP --&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mainClass&gt;club.codeopen.oauth2.OAuth2ResourceApplication&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 逆向工程生成代码 创建启动类 123456789101112131415161718package club.codeopen.oauth2;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import tk.mybatis.spring.annotation.MapperScan;/** * * @author by cheng * @Date 2019/5/5 */@SpringBootApplication@MapperScan(basePackages = "club.codeopen.oauth2.resource.mapper")public class OAuth2ResourceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OAuth2ResourceApplication.class,args); &#125;&#125; 编写配置文件application.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243spring: application: name: oauth2-server datasource: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/oauth2-resource?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: root hikari: minimum-idle: 5 idle-timeout: 600000 maximum-pool-size: 10 auto-commit: true pool-name: MyHikariCP max-lifetime: 1800000 connection-timeout: 30000 connection-test-query: SELECT 1security: oauth2: client: client-id: client client-secret: secret access-token-uri: http://localhost:8080/oauth/token user-authorization-uri: http://localhost:8080/oauth/authorize resource: token-info-uri: http://localhost:8080/oauth/check_tokenserver: port: 8081 servlet: context-path: /contentsmybatis: type-aliases-package: club.codeopen.oauth2.resource.domain mapper-locations: classpath:mapper/*.xmllogging: level: root: INFO org.springframework.web: INFO org.springframework.security: INFO org.springframework.security.oauth2: INFO 内容管理Service接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package club.codeopen.oauth2.resource.service;import club.codeopen.oauth2.resource.domain.TbContent;import java.util.List;/** * @author by cheng * @Date 2019/5/5 */public interface TbContentService &#123; /** * 根据 ID 获取 * * @param id ID * @return &#123;@link TbContent&#125; */ default TbContent getById(Long id) &#123; return null; &#125; /** * 获取全部数据 * * @return &#123;@link List&lt;TbContent&gt;&#125; */ default List&lt;TbContent&gt; selectAll() &#123; return null; &#125; /** * 新增 * * @param tbContent &#123;@link TbContent&#125; * @return int 数据库受影响行数 */ default int insert(TbContent tbContent) &#123; return 0; &#125; /** * 编辑 * * @param tbContent &#123;@link TbContent&#125; * @return int 数据库受影响行数 */ default int update(TbContent tbContent) &#123; return 0; &#125; /** * 删除 * * @param id ID * @return int 数据库受影响行数 */ default int delete(Long id) &#123; return 0; &#125;&#125; 内容管理Service接口实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445package club.codeopen.oauth2.resource.service.impl;import club.codeopen.oauth2.resource.domain.TbContent;import club.codeopen.oauth2.resource.mapper.TbContentMapper;import club.codeopen.oauth2.resource.service.TbContentService;import org.springframework.stereotype.Service;import javax.annotation.Resource;import java.util.List;/** * @author by cheng * @Date 2019/5/5 */@Servicepublic class TbContentServiceImpl implements TbContentService&#123; @Resource private TbContentMapper tbContentMapper; @Override public TbContent getById(Long id) &#123; return tbContentMapper.selectByPrimaryKey(id); &#125; @Override public List&lt;TbContent&gt; selectAll() &#123; return tbContentMapper.selectAll(); &#125; @Override public int insert(TbContent tbContent) &#123; return tbContentMapper.insert(tbContent); &#125; @Override public int update(TbContent tbContent) &#123; return tbContentMapper.updateByPrimaryKey(tbContent); &#125; @Override public int delete(Long id) &#123; return tbContentMapper.deleteByPrimaryKey(id); &#125;&#125; 内容管理前端控制器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package club.codeopen.oauth2.resource.controller;import club.codeopen.oauth2.resource.domain.TbContent;import club.codeopen.oauth2.resource.dto.ResponseResult;import club.codeopen.oauth2.resource.service.TbContentService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.*;import java.util.List;/** * * @author by cheng * @Date 2019/5/5 */@RestControllerpublic class TbContentController &#123; @Autowired private TbContentService tbContentService; /** * 获取全部资源 * * @return */ @GetMapping("/") public ResponseResult&lt;List&lt;TbContent&gt;&gt; selectAll() &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.OK.value()), HttpStatus.OK.toString(), tbContentService.selectAll()); &#125; /** * 获取资源详情 * * @param id * @return */ @GetMapping("/view/&#123;id&#125;") public ResponseResult&lt;TbContent&gt; getById(@PathVariable Long id) &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.OK.value()), HttpStatus.OK.toString(), tbContentService.getById(id)); &#125; /** * 新增资源 * * @param tbContent * @return */ @PostMapping("/insert") public ResponseResult&lt;Integer&gt; insert(@RequestBody TbContent tbContent) &#123; int count = tbContentService.insert(tbContent); if (count &gt; 0) &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.OK.value()), HttpStatus.OK.toString(), count); &#125; else &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.BAD_REQUEST.value()), HttpStatus.BAD_REQUEST.toString()); &#125; &#125; /** * 更新资源 * * @param tbContent * @return */ @PutMapping("/update") public ResponseResult&lt;Integer&gt; update(@RequestBody TbContent tbContent) &#123; int count = tbContentService.update(tbContent); if (count &gt; 0) &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.OK.value()), HttpStatus.OK.toString(), count); &#125; else &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.BAD_REQUEST.value()), HttpStatus.BAD_REQUEST.toString()); &#125; &#125; /** * 删除资源 * * @param id * @return */ @DeleteMapping("/delete/&#123;id&#125;") public ResponseResult&lt;Integer&gt; delete(@PathVariable Long id) &#123; int count = tbContentService.delete(id); if (count &gt; 0) &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.OK.value()), HttpStatus.OK.toString(), count); &#125; else &#123; return new ResponseResult&lt;&gt;(Integer.valueOf(HttpStatus.BAD_REQUEST.value()), HttpStatus.BAD_REQUEST.toString()); &#125; &#125;&#125; 配置资源服务器 创建一个类继承 ResourceServerConfigurerAdapter 并添加相关注解： @Configuration @EnableResourceServer：资源服务器 @EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)：全局方法拦截 12345678910111213141516171819202122232425262728293031package club.codeopen.oauth2.resource.config;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.http.SessionCreationPolicy;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;import org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;@Configuration@EnableResourceServer@EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true, jsr250Enabled = true)public class ResourceServerConfiguration extends ResourceServerConfigurerAdapter &#123; @Override public void configure(HttpSecurity http) throws Exception &#123; http .exceptionHandling() .and() .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() .authorizeRequests() // 以下为配置所需保护的资源路径及权限，需要与认证服务器配置的授权部分对应 .antMatchers("/").hasAuthority("SystemContent") .antMatchers("/view/**").hasAuthority("SystemContentView") .antMatchers("/insert/**").hasAuthority("SystemContentInsert") .antMatchers("/update/**").hasAuthority("SystemContentUpdate") .antMatchers("/delete/**").hasAuthority("SystemContentDelete"); &#125;&#125; 附：通用的返回对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138package club.codeopen.oauth2.resource.dto;import lombok.Data;import java.io.Serializable;/** * 通用的返回对象 * @Date 2019/5/5 */@Datapublic class ResponseResult&lt;T&gt; implements Serializable &#123; private static final long serialVersionUID = 8486468966063544884L; /** * 状态码 */ private Integer state; /** * 消息 */ private String message; /** * 返回对象 */ private T data; public ResponseResult() &#123; super(); &#125; public ResponseResult(Integer state) &#123; super(); this.state = state; &#125; public ResponseResult(Integer state, String message) &#123; super(); this.state = state; this.message = message; &#125; public ResponseResult(Integer state, Throwable throwable) &#123; super(); this.state = state; this.message = throwable.getMessage(); &#125; public ResponseResult(Integer state, T data) &#123; super(); this.state = state; this.data = data; &#125; public ResponseResult(Integer state, String message, T data) &#123; super(); this.state = state; this.message = message; this.data = data; &#125; public Integer getState() &#123; return state; &#125; public void setState(Integer state) &#123; this.state = state; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125; @Override public int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + ((data == null) ? 0 : data.hashCode()); result = prime * result + ((message == null) ? 0 : message.hashCode()); result = prime * result + ((state == null) ? 0 : state.hashCode()); return result; &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) &#123; return true; &#125; if (obj == null) &#123; return false; &#125; if (getClass() != obj.getClass()) &#123; return false; &#125; ResponseResult&lt;?&gt; other = (ResponseResult&lt;?&gt;) obj; if (data == null) &#123; if (other.data != null) &#123; return false; &#125; &#125; else if (!data.equals(other.data)) &#123; return false; &#125; if (message == null) &#123; if (other.message != null) &#123; return false; &#125; &#125; else if (!message.equals(other.message)) &#123; return false; &#125; if (state == null) &#123; if (other.state != null) &#123; return false; &#125; &#125; else if (!state.equals(other.state)) &#123; return false; &#125; return true; &#125; @Override public String toString() &#123; return "ResponseResult [state=" + state + ", message=" + message + ", data=" + data + "]"; &#125;&#125;]]></content>
      <categories>
        <category>Spring Security oAuth2</category>
      </categories>
      <tags>
        <tag>Spring Security oAuth2</tag>
        <tag>RBAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot整合Druid并开启监控]]></title>
    <url>%2F2019%2F05%2F23%2Fspring%2FSpringBoot%20%E6%95%B4%E5%90%88%20Druid%2F</url>
    <content type="text"><![CDATA[引入依赖在pom.xml文件中引入druid-spring-boot-starter 依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt;&lt;/dependency&gt; 引入数据库连接依赖 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 配置 application.yml在 application.yml 中配置数据库连接 1234567891011spring: datasource: druid: url: jdbc:mysql://ip:port/dbname?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false username: root password: 123456 initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.jdbc.Driver 配置Druid监控 编写druid监控配置类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @author by cheng * @Classname DruidConfig * @Description druid 配置 * @Date 2019/2/12 13:38 */@Configurationpublic class DruidConfig &#123; @Bean public ServletRegistrationBean statViewServlet()&#123; ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean(new StatViewServlet(),"/druid/*"); // 白名单 servletRegistrationBean.addInitParameter("allow", "127.0.0.1"); // IP 黑名单 (存在共同是， deny 优先于 allow ) servletRegistrationBean.addInitParameter("deny", "192.168.0.103"); // 登录查看详细的账号、密码 servletRegistrationBean.addInitParameter("loginUsername","druid"); servletRegistrationBean.addInitParameter("loginPassword","123456"); // 是否能够重置数据 servletRegistrationBean.addInitParameter("resetEnable","false"); return servletRegistrationBean; &#125; @Bean public FilterRegistrationBean statFilter()&#123; FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new WebStatFilter()); // 添加过滤规则 filterRegistrationBean.addUrlPatterns("/*"); // 添加不需要忽略的格式信息 filterRegistrationBean.addInitParameter("exclusions","*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid2/*"); return filterRegistrationBean; &#125; @Bean PersistenceExceptionTranslationPostProcessor persistenceExceptionTranslationPostProcessor()&#123; return new PersistenceExceptionTranslationPostProcessor(); &#125; /** * 配置数据库的基本连接信息 * @return */ @Bean(name = "dataSource") @Primary @ConfigurationProperties(prefix = "spring.datasource") // 可以在 application.yml 中直接获取 public DataSource dataSource()&#123; return DataSourceBuilder.create().type(DruidDataSource.class).build(); &#125; @Bean public SqlSessionFactoryBean sqlSessionFactoryBean(@Qualifier("dataSource")DataSource dataSource) throws IOException &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); bean.setMapperLocations(resolver.getResources("classpath:/mapper/*.xml")); return bean; &#125;&#125; 访问druid监控页面 http://127.0.0.1:8080/druid/login.html 用户名、密码可自定义 用户名：druid 密码：123456]]></content>
      <categories>
        <category>SpringBoot</category>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件RabbitMQ]]></title>
    <url>%2F2019%2F05%2F23%2FRabbitMQ%2F%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6RabbitMQ%2F</url>
    <content type="text"><![CDATA[消息中间件RabbitMQ 消息队列中间件简介 消息队列中间件是分布式系统中重要的组件，主要解决应用耦合，异步消息，流量削锋等问题实现高性能，高可用，可伸缩和最终一致性[架构] 使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ 以下介绍消息队列在实际应用中常用的使用场景：异步处理，应用解耦，流量削锋和消息通讯四个场景 RabbitMQ 简介 RabbitMQ 是一个由 Erlang语言开发的AMQP的开源实现。 AMQP：Advanced Message Queue，高级消息队列协议。它是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端与消息中间件可传递消息，并不受产品、开发语言等条件的限制。 主要概念 RabbitMQ Server： 也叫broker server，它是一种传输服务。 他的角色就是维护一条从Producer到Consumer的路线，保证数据能够按照指定的方式进行传输。 Producer： 消息生产者，如图A、B、C，数据的发送方。消息生产者连接RabbitMQ服务器然后将消息投递到Exchange。 Consumer：消息消费者，如图1、2、3，数据的接收方。消息消费者订阅队列，RabbitMQ将Queue中的消息发送到消息消费者。 Exchange：生产者将消息发送到Exchange（交换器），由Exchange将消息路由到一个或多个Queue中（或者丢弃）。Exchange并不存储消息。RabbitMQ中的Exchange有direct、fanout、topic、headers四种类型，每种类型对应不同的路由规则。 Queue：（队列）是RabbitMQ的内部对象，用于存储消息。消息消费者就是通过订阅队列来获取消息的，RabbitMQ中的消息都只能存储在Queue中，生产者生产消息并最终投递到Queue中，消费者可以从Queue中获取消息并消费。多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。 RoutingKey：生产者在将消息发送给Exchange的时候，一般会指定一个routing key，来指定这个消息的路由规则，而这个routing key需要与Exchange Type binding key联合使用才能最终生效。在Exchange Type与binding key固定的情况下（在正常使用时一般这些内容都是固定配置好的），我们的生产者就可以在发送消息给Exchange时，通过指定routing key来决定消息流向哪里。RabbitMQ为routing key设定的长度限制为255bytes。 Connection： （连接）：Producer和Consumer都是通过TCP连接到RabbitMQ Server的。以后我们可以看到，程序的起始处就是建立这个TCP连接。 Channels： （信道）：它建立在上述的TCP连接中。数据流动都是在Channel中进行的。也就是说，一般情况是程序起始建立TCP连接，第二步就是建立这个Channel。 VirtualHost：权限控制的基本单位，一个VirtualHost里面有若干Exchange和MessageQueue，以及指定被哪些user使用 RabbitMQ安装与启动 编写docker-compose.yml文件 1234567891011121314151617181920version: '3.1'services: rabbitmq: restart: always image: rabbitmq:management container_name: rabbitmq ports: - 5672:5672 - 15672:15672 - 5671:5617 - 15671:15671 - 25672:25672 environment: TZ: Asia/Shanghai RABBITMQ_DEFAULT_USER: rabbit RABBITMQ_DEFAULT_PASS: 123456 volumes: - data:/var/lib/rabbitmqvolumes: data: rabbitmq需要有映射以下端口: 5671、5672、4369、15671、15672、25672 15672 (if management plugin is enabled) 15671 management监听端口 5672, 5671 (AMQP 0-9-1 without and with TLS) 4369 (epmd) epmd 代表 Erlang 端口映射守护进程 25672 (Erlang distribution) 启动容器docker-compose up -d 访问地址：http://IP:15672,即可看到管理界面的登陆页]]></content>
      <categories>
        <category>Docker Compose</category>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis安装]]></title>
    <url>%2F2019%2F05%2F23%2FRedis%2FRedis%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[编写docker-compose.yml配置文件12345678910111213version: &apos;3.1&apos;services: redis: restart: always image: redis container_name: redis command: redis-server --requirepass 123456 --appendonly yes ports: - &quot;6379:6379&quot; volumes: - data:/datavolumes: data: --requirepass:设置密码，--appendonly:开启持久化 使用命令docker-compose up -d运行]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 负载均衡]]></title>
    <url>%2F2019%2F05%2F23%2FNginx%2FNginx%20%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[Nginx 实现负载均衡 nginx 作为负载均衡服务器，用户请求先到达 nginx，再由 nginx 根据负载配置将请求转发至 tomcat 服务器 nginx 负载均衡服务器：192.168.75.145:80 tomcat1 服务器：192.168.75.145:9090 tomcat2 服务器：192.168.75.145:9091 Nginx 配置负载均衡修改 /usr/local/docker/nginx/conf 目录下的 nginx.conf 配置文件： 1234567891011121314151617181920212223242526272829user nginx;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream myapp1 &#123; server 192.168.75.145:9090 weight=10; server 192.168.75.145:9091 weight=10; &#125; server &#123; listen 80; server_name nginx.funtl.com; location / &#123; proxy_pass http://myapp1; index index.jsp index.html index.htm; &#125; &#125;&#125; 相关配置说明1234567# 定义负载均衡设备的 Ip及设备状态 upstream myServer &#123; server 127.0.0.1:9090 down; server 127.0.0.1:8080 weight=2; server 127.0.0.1:6060; server 127.0.0.1:7070 backup;&#125; 在需要使用负载的 Server 节点下添加 1proxy_pass http://myServer; upstream：每个设备的状态: down：表示当前的 server 暂时不参与负载 weight：默认为 1 weight 越大，负载的权重就越大。 max_fails：允许请求失败的次数默认为 1 当超过最大次数时，返回 proxy_next_upstream 模块定义的错误 fail_timeout:max_fails 次失败后，暂停的时间。 backup：其它所有的非 backup 机器 down 或者忙的时候，请求 backup 机器。所以这台机器压力会最轻]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 虚拟主机]]></title>
    <url>%2F2019%2F05%2F23%2FNginx%2FNginx%20%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[使用 Docker 来安装和运行 Nginx，docker-compose.yml 配置如下：1234567891011version: '3.1'services: nginx: restart: always image: nginx container_name: nginx ports: - 81:80 volumes: - ./conf/nginx.conf:/etc/nginx/nginx.conf - ./wwwroot:/usr/share/nginx/wwwroot 基于端口的虚拟主机配置需求 Nginx 对外提供 80 和 8080 两个端口监听服务 请求 80 端口则请求 html80 目录下的 html 请求 8080 端口则请求 html8080 目录下的 html 创建目录及文件在/usr/local/docker/nginx/wwwroot 目录下创建 html80 和 html8080 两个目录，并分辨创建两个 index.html 文件 配置虚拟主机修改 /usr/local/docker/nginx/conf 目录下的 nginx.conf 配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 启动进程,通常设置成和 CPU 的数量相等worker_processes 1;events &#123; # epoll 是多路复用 IO(I/O Multiplexing) 中的一种方式 # 但是仅用于 linux2.6 以上内核,可以大大提高 nginx 的性能 use epoll; # 单个后台 worker process 进程的最大并发链接数 worker_connections 1024;&#125;http &#123; # 设定 mime 类型,类型由 mime.type 文件定义 include mime.types; default_type application/octet-stream; # sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，对于普通应用， # 必须设为 on，如果用来进行下载等应用磁盘 IO 重负载应用，可设置为 off，以平衡磁盘与网络 I/O 处理速度，降低系统的 uptime. sendfile on; # 连接超时时间 keepalive_timeout 65; # 设定请求缓冲 client_header_buffer_size 2k; # 配置虚拟主机 192.168.75.145 server &#123; # 监听的ip和端口，配置 192.168.75.145:80 listen 80; # 虚拟主机名称这里配置ip地址 server_name 192.168.75.145; # 所有的请求都以 / 开始，所有的请求都可以匹配此 location location / &#123; # 使用 root 指令指定虚拟主机目录即网页存放目录 # 比如访问 http://ip/index.html 将找到 /usr/local/docker/nginx/wwwroot/html80/index.html # 比如访问 http://ip/item/index.html 将找到 /usr/local/docker/nginx/wwwroot/html80/item/index.html root /usr/share/nginx/wwwroot/html80; # 指定欢迎页面，按从左到右顺序查找 index index.html index.htm; &#125; &#125; # 配置虚拟主机 192.168.75.245 server &#123; listen 8080; server_name 192.168.75.145; location / &#123; root /usr/share/nginx/wwwroot/html8080; index index.html index.htm; &#125; &#125;&#125; 基于域名的虚拟主机配置需求 两个域名指向同一台 Nginx 服务器，用户访问不同的域名显示不同的网页内容 两个域名是 admin.service.com 和 admin.web..com Nginx 服务器使用虚拟机 192.168.75.145 配置 Windows Hosts 文件 通过 host 文件指定 admin.service.com 和 admin.web.com 对应 192.168.75.145 虚拟机： 修改 window 的 hosts 文件：（C:\Windows\System32\drivers\etc） 创建目录及文件在 /usr/local/docker/nginx/wwwroot 目录下创建 htmlservice 和 htmlweb 两个目录，并分辨创建两个 index.html 文件 配置虚拟主机12345678910111213141516171819202122232425262728293031323334user nginx;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name admin.service.com; location / &#123; root /usr/share/nginx/wwwroot/htmlservice; index index.html index.htm; &#125; &#125; server &#123; listen 80; server_name admin.web.com; location / &#123; root /usr/share/nginx/wwwroot/htmlweb; index index.html index.htm; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 搭建FTP]]></title>
    <url>%2F2019%2F05%2F23%2FLinux%2FCentOS7%E6%90%AD%E5%BB%BAFTP%2F</url>
    <content type="text"><![CDATA[查看是否安装了FTP：rpm -qa |grep vsftpd 如果没有任何输出，表示没有安装。 如果出现如下版本信息，则表示已经安装。 如果没有安装，可以使用如下命令直接安装 1yum -y install vsftpd 默认安装目录：/etc/vsftpd 添加FTP账号1useradd admin -s /sbin/nologin 该账户路径默认指向/home/admin目录 设置密码：passwd admin 一些常用设置 设置匿名用户可以下载上传将文件/etc/vsftpd/vsftpd.conf 中下面两句的注释删除 12anon_upload_enable=YESanon_mkdir_write_enable=YES 根据个人需要设置默认目录修改/etc/passwd文件，找到你的用户名的那一行修改路径，然后保存即可，无需重启 12将admin:x:500:500::/home/admin:/sbin/nologin改成admin:x:500:500::/自定义文件夹:/sbin/nologin 启动 启动：systemctl start vsftpd 重启：systemctl restart vsftpd 开机自启：systemctl enable vsftpd]]></content>
      <categories>
        <category>FTP</category>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 反向代理]]></title>
    <url>%2F2019%2F05%2F23%2FNginx%2FNginx%20%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[使用 Nginx 反向代理 Tomcat 需求 两个 tomcat 服务通过 nginx 反向代理 nginx 服务器：192.168.75.145:80 tomcat1 服务器：192.168.75.145:9090 tomcat2 服务器：192.168.75.145:9091 启动 Tomcat 容器启动两个 Tomcat 容器，映射端口为 9090 和 9091，docker-compose.yml 如下： 12345678910111213version: &apos;3&apos;services: tomcat1: image: tomcat container_name: tomcat1 ports: - 9090:8080 tomcat2: image: tomcat container_name: tomcat2 ports: - 9091:8080 配置 Nginx 反向代理修改 /usr/local/docker/nginx/conf 目录下的 nginx.conf 配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748user nginx;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 配置一个代理即 tomcat1 服务器 upstream tomcat_server1 &#123; server 192.168.75.145:9090; &#125; # 配置一个代理即 tomcat2 服务器 upstream tomcat_server2 &#123; server 192.168.75.145:9091; &#125; # 配置一个虚拟主机 server &#123; listen 80; server_name admin.service.itoken.funtl.com; location / &#123; # 域名 admin.service.itoken.funtl.com 的请求全部转发到 tomcat_server1 即 tomcat1 服务上 proxy_pass http://tomcat_server1; # 欢迎页面，按照从左到右的顺序查找页面 index index.jsp index.html index.htm; &#125; &#125; server &#123; listen 80; server_name admin.web.itoken.funtl.com; location / &#123; # 域名 admin.web.itoken.funtl.com 的请求全部转发到 tomcat_server2 即 tomcat2 服务上 proxy_pass http://tomcat_server2; index index.jsp index.html index.htm; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes入门]]></title>
    <url>%2F2019%2F05%2F23%2Fk8s%2FKubernetes%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Kubernetes简介Kubernetes 项目由 Google 公司在 2014 年启动。Kubernetes 建立在 Google 公司超过十余年的运维经验基础之上，Google 所有的应用都运行在容器上, 再与社区中最好的想法和实践相结合，也许它是最受欢迎的容器平台。Kubernetes 是一个跨主机集群的开源的容器调度平台，它可以自动化应用容器的部署、扩展和操作, 提供以容器为中心的基础架构。 使用 Kubernetes, 您可以快速高效地响应客户需求: 快速、可预测地部署您的应用程序 拥有即时扩展应用程序的能力 不影响现有业务的情况下，无缝地发布新功能 优化硬件资源，降低成本 目标是构建一个软件和工具的生态系统，以减轻在公共云或私有云运行应用程序的负担。 特点 便携性: 无论公有云、私有云、混合云还是多云架构都全面支持 可扩展: 它是模块化、可插拔、可挂载、可组合的，支持各种形式的扩展 自修复: 它可以自保持应用状态、可自重启、自复制、自缩放的，通过声明式语法提供了强大的自修复能力 Kubernetes能做什么最基础的，Kubernetes 可以在物理或虚拟机集群上调度和运行应用程序容器。然而，Kubernetes 还允许开发人员从物理和虚拟机’脱离’，从以主机为中心的基础架构转移到以容器为中心的基础架构，这样可以提供容器固有的全部优点和益处。Kubernetes 提供了基础设施来构建一个真正以容器为中心的开发环境。 Kubernetes 满足了生产中运行应用程序的许多常见的需求，例如： Pod 提供复合应用并保留一个应用一个容器的容器模型, 挂载外部存储, Secret管理, 应用健康检查, 副本应用实例, 横向自动扩缩容, 服务发现, 负载均衡, 滚动更新, 资源监测, 日志采集和存储, 支持自检和调试, 认证和鉴权. 这提供了平台即服务 (PAAS) 的简单性以及基础架构即服务 (IAAS) 的灵活性，并促进跨基础设施供应商的可移植性。]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Compose安装Jenkins]]></title>
    <url>%2F2019%2F05%2F23%2FJenkins%2FDocker-Compose%E5%AE%89%E8%A3%85Jenkins%2F</url>
    <content type="text"><![CDATA[配置文件编写docker-compose.ymlJenkins 是一个简单易用的持续集成软件平台，我们依然采用 Docker 的方式部署，docker-compose.yml 配置文件如下： 12345678910111213version: '3'services: jenkins: restart: always image: jenkins/jenkins container_name: jenkins ports: - 8080:8080 # 发布端口 - 50000:50000 # 基于 JNLP 的 Jenkins 代理通过 TCP 端口 50000 与 Jenkins master 进行通信 environment: TZ: Asia/Shanghai volumes: - ./data:/var/jenkins_home 安装过程中会出现 Docker 数据卷 权限问题，用以下命令解决： 1chown -R 1000 /usr/local/docker/jenkins/data 解锁 JenkinsJenkins 第一次启动时需要输入一个初始密码用以解锁安装流程，使用 docker logs jenkins 即可方便的查看到初始密码 注意： 安装时可能会因为网速等原因导致安装时间比较长，请大家耐心等待。如果长时间停留在安装页没反应，请尝试使用 F5 刷新一下。 使用自定义插件的方式安装插件是 Jenkins 的核心，其丰富的插件（截止到 2019.1.2 共有 79400 个插件）可以满足不同人群的不同需求 插件地址：https://plugins.jenkins.io/注意： 除了默认勾选的插件外，一定要勾选 Publish over SSH 插件，这是我们实现持续交付的重点插件。 开始安装了，根据网络情况，安装时间可能会比较长，请耐心等待 很多插件装不上怎么办？不要慌，记住这些插件的名字，咱们稍后可以手动安装 安装成功效果图创建管理员 安装完成，进入首页 附：Jenkins 手动安装插件使用插件管理器安装 Manage Jenkins -&gt; Manage Plugins -&gt; Avaliable 过滤出想要安装的插件，然后点击 Download now and install after restart 手动上传 .hpi文件 点击进入插件中心 点击 Archives 下载需要的版本 在插件管理器中选择 Advanced 选择上传即可 重启 Jenkins12docker-compose downdocker-compose up -d 注意： 请留意需要下载插件的警告信息，如果不满足安装条件，Jenkins 是会拒绝安装的。如下图：]]></content>
      <categories>
        <category>Docker Compose</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 Jenkins安装]]></title>
    <url>%2F2019%2F05%2F23%2FJenkins%2FJenkins%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[下载jenkins 1wget https://pkg.jenkins.io/redhat/jenkins‐2.83‐1.1.noarch.rpm 安装jenkins 1rpm ‐ivh jenkins‐2.83‐1.1.noarch.rpm 配置jenkins 修改用户和端口vi /etc/sysconfig/jenkins 12JENKINS_USER="root"JENKINS_PORT="8888" 配置java环境变量vi /etc/rc.d/init.d/jenkins 1234567891011# see http://www.nabble.com/guinea-pigs-wanted-----Hudson-RPM-for-RedHat-Linux-td25673707.htmlcandidates="/etc/alternatives/java/usr/lib/jvm/java-1.8.0/bin/java/usr/lib/jvm/jre-1.8.0/bin/java/usr/lib/jvm/java-1.7.0/bin/java/usr/lib/jvm/jre-1.7.0/bin/java/usr/bin/java#在下面加入java环境变量/usr/java/jdk1.8.0_161/bin/java" 启动服务 1systemctl start jenkins 访问链接 http://IP:8888 从/var/lib/jenkins/secrets/initialAdminPassword中获取初始密码串]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置 Jenkins]]></title>
    <url>%2F2019%2F05%2F23%2FJenkins%2F%E9%85%8D%E7%BD%AE%20Jenkins%2F</url>
    <content type="text"><![CDATA[本教程基于docker-compose安装Jenkins 详细安装教程：站内搜索Docker-Compose安装Jenkins 配置 JDK &amp; Maven 上传 JDK 和 Maven 的 tar 包到服务器（容器数据卷目录/usr/local/docker/jenkins/data） 解压 JDK 和 Maven 的 tar 包，完成后删除压缩包 Manage Jenkins -&gt; Global Tool Configuration 安装 JDK(JAVA_HOME 的路径是宿主机目录) 1/var/jenkins_home/jdk1.8.0_152 安装 Maven(MAVEN_HOME 的路径是宿主机目录) 1/var/jenkins_home/apache-maven-3.5.3 别忘记保存 配置本地化（显示中文） 安装 Locale 插件 Manage Jenkins -&gt; Configure System -&gt; Locale 本地化效果图]]></content>
      <categories>
        <category>Docker Compose</category>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm切换镜像到淘宝镜像]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2Fnpm%E5%88%87%E6%8D%A2%E9%95%9C%E5%83%8F%E5%88%B0%E6%B7%98%E5%AE%9D%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[NPM安装完成Node(傻瓜式安装)应该自带了NPM了，在控制台输入npm -v查看: npm默认的仓库地址是在国外网站，速度较慢，建议大家设置到淘宝镜像。但是切换镜像是比较麻烦的。推荐一款切换镜像的工具：nrm 我们首先安装nrm，这里-g代表全局安装 1npm install nrm -g 然后通过nrm ls命令查看npm的仓库列表,带*的就是当前选中的镜像仓库： 通过nrm use taobao来指定要使用的镜像源： 然后通过nrm test npm来测试速度： 注意：(重要的事情说三遍！！！) 安装完成请一定要重启下电脑！！！ 安装完成请一定要重启下电脑！！！ 安装完成请一定要重启下电脑！！！]]></content>
      <categories>
        <category>npm</category>
      </categories>
      <tags>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo建站(博客)]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2FHexo%E5%BB%BA%E7%AB%99%2F</url>
    <content type="text"><![CDATA[环境准备 Hexo安装 VScode(IDE：个人喜好) 建站安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 新建完成后，指定文件夹的目录如下： 123456789.├── _config.yml├── node_modules/├── package.json├── package-lock.json├── scaffolds├── source| └── _posts└── themes _config.yml网站的 配置 信息，您可以在此配置大部分的参数。 package.json应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 12345678910111213141516171819&#123; &quot;name&quot;: &quot;hexo-site&quot;, &quot;version&quot;: &quot;0.0.0&quot;, &quot;private&quot;: true, &quot;hexo&quot;: &#123; &quot;version&quot;: &quot;&quot; &#125;, &quot;dependencies&quot;: &#123; &quot;hexo&quot;: &quot;^3.7.0&quot;, &quot;hexo-generator-archive&quot;: &quot;^0.1.5&quot;, &quot;hexo-generator-category&quot;: &quot;^0.1.3&quot;, &quot;hexo-generator-index&quot;: &quot;^0.2.1&quot;, &quot;hexo-generator-tag&quot;: &quot;^0.2.0&quot;, &quot;hexo-renderer-ejs&quot;: &quot;^0.3.1&quot;, &quot;hexo-renderer-stylus&quot;: &quot;^0.3.3&quot;, &quot;hexo-renderer-marked&quot;: &quot;^0.3.2&quot;, &quot;hexo-server&quot;: &quot;^0.3.1&quot; &#125;&#125; scaffolds模版 文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的markdown文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。 source资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。 themes主题 文件夹。Hexo 会根据主题来生成静态页面。 修改站点信息修改 _config.yml(不是主题的 _config.yml)中的信息，例如： 12345678# Sitetitle: 程鹏subtitle: 不积跬步，无以至千里；不积小流，无以成江海description:keywords: 技术博客,blog,hexoauthor: 程鹏language: zh-Hans # hexo NexT主题用timezone:]]></content>
      <categories>
        <category>blog</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next第三方服务集成]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2FHexo%20Next%E7%AC%AC%E4%B8%89%E6%96%B9%E6%9C%8D%E5%8A%A1%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[评论系统NexT 支持多款评论系统(大多数由于种种原因被劝退)。 如需取消某个 页面/文章 的评论，在 md 文件的 front-matter 中增加 comments: false 说几个勉强可以用的吧 来必力 由 asmoker 贡献（站长使用ing）登陆 来必力 获取你的 LiveRe UID。 编辑 主题配置文件， 编辑 livere_uid 字段，设置如下： 1livere_uid: #your livere_uid Gitment github集成的评论系统，推荐使用，一般应该不会被墙吧，哈哈1234567891011121314151617181920212223242526- 注册 OAuth Application - [点击此处](https://github.com/settings/applications/new) 来注册一个新的 OAuth Application。其他内容可以随意填写，但要确保填入正确的 callback URL（一般是评论页面对应的域名，如站长网站 `https://www.codeopen.club`）。 - 你会得到一个 `client ID` 和一个 `client secret`，这个将被用于之后的用户登录。- 引入 Gitment 将下面的代码添加到你的页面： ~~~html &lt;div id=&quot;container&quot;&gt;&lt;/div&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://imsun.github.io/gitment/style/default.css&quot;&gt; &lt;script src=&quot;https://imsun.github.io/gitment/dist/gitment.browser.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var gitment = new Gitment(&#123; id: &apos;页面 ID&apos;, // 可选。默认为 location.href owner: &apos;你的 GitHub ID&apos;, repo: &apos;存储评论的 repo&apos;, oauth: &#123; client_id: &apos;你的 client ID&apos;, client_secret: &apos;你的 client secret&apos;, &#125;, &#125;) gitment.render(&apos;container&apos;) &lt;/script&gt; 注意，上述代码引用的 Gitment 将会随着开发变动。如果你希望始终使用最新的界面与特性即可引入上述代码。 如果你希望引用确定版本的 Gitment，则应该使用 npm 进行安装。 1$ npm install --save gitment 初始化评论 修改主题配置文件,找到配置文件中gitment节点，示例如下： 1234567891011gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide 'Powered by ...' on footer, and more language: zh-Hans # Force language, or auto switch by theme github_user: 填写你的github用户名 github_repo: 填写你的存储评论的仓库的git地址(HTTP) client_id: 填写刚刚注册OAuth Application的client ID client_secret: 填写刚刚注册OAuth Application的client secret DISQUS(不建议使用，在墙内你懂的)Next版本&gt;=5.1.1编辑 主题配置文件， 将 disqus 下的 enable 设定为 true，同时提供您的 shortname。count 用于指定是否显示评论数量。 1234disqus: enable: false shortname: count: true Next版本&lt;5.1.1编辑 主题配置文件，设定 disqus_shortname 的值即可。 1disqus_shortname: your-disqus-shortname 数据统计与分析百度统计 登录 百度统计，定位到站点的代码获取页面 复制 hm.js? 后面那串统计脚本 id，如下图所示： 编辑 主题配置文件， 修改字段 baidu_analytics，值设置成你的百度统计脚本 id。 Google 分析编辑 主题配置文件， 修改字段 google_analytics， 值设置成你的 Google 跟踪 ID。跟踪 ID 通常是以 UA- 开头。 腾讯分析 由 Cissoid 贡献请登录 腾讯分析，登录并获取分析的 ID。 然后在 主题配置文件 里将 ID 放置 tencent_analytics 字段。 不蒜子统计 由 panzhitian 贡献注意： 此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 全局配置（站长使用ing） 编辑 主题配置文件 中的busuanzi_count的配置项。 当enable: true时，代表开启全局开关。若site_uv、site_pv、page_pv的值均为false时，不蒜子仅作记录而不会在页面上显示。 站点UV配置 当site_uv: true时，代表在页面底部显示站点的UV值。 site_uv_header和site_uv_footer为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为[site_uv_header]UV值[site_uv_footer]。 12345&gt; # 效果：本站访客数12345人次&gt; site_uv: true&gt; site_uv_header: 本站访客数&gt; site_uv_footer: 人次&gt; 站点Pv配置 当site_pv: true时，代表在页面底部显示站点的PV值。 site_pv_header和site_pv_footer为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为[site_pv_header]PV值[site_pv_footer]。 12345&gt; # 效果：本站总访问量12345次&gt; site_pv: true&gt; site_pv_header: 本站总访问量&gt; site_pv_footer: 次&gt; 单页面配置 当page_pv: true时，代表在文章页面的标题下显示该页面的PV值（阅读数）。 page_pv_header和page_pv_footer为自定义样式配置，相关的值留空时将不显示，可以使用（带特效的）font-awesome。显示效果为[page_pv_header]PV值[page_pv_footer]。 12345&gt; # 效果：本文总阅读量12345次&gt; page_pv: true&gt; page_pv_header: 本文总阅读量&gt; page_pv_footer: 次&gt; 没得耐心看的人，被坑很正常，站长踩过的坑分享： 不蒜子统计在主题中的链接已失效，需改： 修改themes\next\layout\_third-party\analytics\busuanzi-counter.swig中的链接，位置自己找，如果真的找不到，那可能这个不适合你吧 1&lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 腾讯移动分析 由 aleonchen 贡献编辑 主题配置文件，填写 tencent_mta 的值。 12# Tencent MTA IDtencent_mta: your-tencent-mta-id 阅读次数统计（LeanCloud) 由 Doublemine 贡献 注册LeanCloud 创建应用 我们新建一个应用来专门进行博客的访问统计的数据操作。首先，打开控制台，如下图所示： 在出现的界面点击创建应用 在接下来的页面，新建的应用名称我们可以随意输入，即便是输入的不满意我们后续也是可以更改的 这里我新创建一个取名为blog的应用。创建完成之后我们点击新创建的应用的名字来进行该应用的参数配置 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表，为了便于区分我们新建一张表来保存我们的数据。点击左侧右上角的齿轮图标，新建Class：在弹出的选项中选择创建Class来新建Class用来专门保存我们博客的文章访问量等数据:点击创建Class之后，理论上来说名字可以随意取名，只要你交互代码做相应的更改即可，但是为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字必须为Counter: 由于LeanCloud升级了默认的ACL权限，如果你想避免后续因为权限的问题导致次数统计显示不正常，建议在此处选择无限制。 创建完成之后，左侧数据栏应该会多出一栏名为Counter的栏目，这个时候我们点击顶部的设置，切换到test应用的操作界面:在弹出的界面中，选择左侧的应用Key选项，即可发现我们创建应用的AppID以及AppKey，有了它，我们就有权限能够通过主题中配置好的Javascript代码与这个应用的Counter表进行数据存取操作了: 复制AppID以及AppKey并在NexT主题的_config.yml文件中我们相应的位置填入即可，正确配置之后文件内容像这个样子: 1234leancloud_visitors: enable: true app_id: joaeuuc4hsqudUUwx4gIvGF6-gzGzoHsz app_key: E9UJsJpw1omCHuS22PdSpKoh 这个时候重新生成部署Hexo博客，应该就可以正常使用文章阅读量统计的功能了。需要特别说明的是：记录文章访问量的唯一标识符是文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计。 后台管理当你配置部分完成之后，初始的文章统计量显示为0，但是这个时候我们LeanCloud对应的应用的Counter表中并没有相应的记录，只是单纯的显示为0而已，当博客文章在配置好阅读量统计服务之后第一次打开时，便会自动向服务器发送数据来创建一条数据，该数据会被记录在对应的应用的Counter表中。 我们可以修改其中的time字段的数值来达到修改某一篇文章的访问量的目的（博客文章访问量快递提升人气的装逼利器）。双击具体的数值，修改之后回车即可保存。 url字段被当作唯一ID来使用，因此如果你不知道带来的后果的话请不要修改。 title字段显示的是博客文章的标题，用于后台管理的时候区分文章之用，没有什么实际作用。 其他字段皆为自动生成，具体作用请查阅LeanCloud官方文档，如果你不知道有什么作用请不要随意修改。 Web安全因为AppID以及AppKey是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。 选择应用的设置的安全中心选项卡: 在Web 安全域名中填入我们自己的博客域名，来确保数据调用的安全 如果你不知道怎么填写安全域名而或者填写完成之后发现博客文章访问量显示不正常，打开浏览器调试模式，发现如下图的输出 这说明你的安全域名填写错误，导致服务器拒绝了数据交互的请求，你可以更改为正确的安全域名或者你不知道如何修改请在本博文中留言或者放弃设置Web安全域名。 内容分享服务JiaThis编辑 主题配置文件， 添加/修改字段 jiathis，值为 true。 JiaThis 内容分享服务配置示例 12# JiaThis 分享服务jiathis: true 百度分享编辑 主题配置文件，添加/修改字段 baidushare，值为 true。 百度内容分享服务配置示例 12# 百度分享服务baidushare: true AddThis 由 hackjustu 贡献(站长使用ing) 在网站 AddThis 上注册账号。 可以使用 Google/Facebook/Twitter 账号进行第三方登陆 从下面菜单获得 AddThis id：More.. --&gt; General --&gt; ID。 具体 ID 获得方式参考以下截图 在 主题配置文件 中，把#Share下的 #add_this_id取消注释， 改为add_this_id: put_your_add_this_id_here 搜索服务NexT 支持集成 Swiftype、 微搜索、Local Search 和 Algolia。 具体自己官网看吧，一堆外国产品注册，都不想看了，站长就用了一个Local Search Local Search 由 flashlab 贡献添加百度/谷歌/本地 自定义站点内容搜索 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 1$ npm install hexo-generator-searchdb --save 编辑 站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true 想要更多的第三方集成，自己百度，不然就翻墙走谷歌]]></content>
      <categories>
        <category>blog</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo NexT主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo安装]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2FHexo%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[什么是 Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装安装前提安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js(傻瓜式安装) 安装完成Node应该自带了NPM，但却龟速，站内搜索npm切换镜像到淘宝镜像,修改npm镜像 Git(傻瓜式安装) 如果您的电脑中已经安装上述必备程序，那么恭喜您！接下来只需要使用 npm 即可完成 Hexo 的安装。 1$ npm install -g hexo-cli 如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。 Mac 用户 您在编译时可能会遇到问题，请先到 App Store 安装 Xcode，Xcode 完成后，启动并进入 Preferences -&gt; Download -&gt; Command Line Tools -&gt; Install 安装命令行工具。 安装 Git Windows：下载并安装 git. Mac：使用 Homebrew, MacPorts ：brew install git;或下载 安装程序 安装。 Linux (Ubuntu, Debian)：sudo apt-get install git-core Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core Windows 用户 由于众所周知的原因，从上面的链接下载git for windows最好挂上一个代理，否则下载速度十分缓慢。也可以参考这个页面，收录了存储于百度云的下载地址。 安装 Node.js安装 Node.js 的最佳方式是使用 nvm。 cURL: 1$ curl https://raw.github.com/creationix/nvm/v0.33.11/install.sh | sh Wget: 1$ wget -qO- https://raw.github.com/creationix/nvm/v0.33.11/install.sh | sh 安装完成后，重启终端并执行下列命令即可安装 Node.js。 1$ nvm install stable 或者您也可以下载 安装程序 来安装。 Windows 用户 对于windows用户来说，建议使用安装程序进行安装。安装时，请勾选Add to PATH选项。另外，您也可以使用Git Bash，这是git for windows自带的一组程序，提供了Linux风格的shell，在该环境下，您可以直接用上面提到的命令来安装Node.js。打开它的方法很简单，在任意位置单击右键，选择“Git Bash Here”即可。由于Hexo的很多操作都涉及到命令行，您可以考虑始终使用Git Bash来进行操作。 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 1$ npm install -g hexo-cli]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next构建个人博客]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2FHexo%20Next%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[环境支持 Hexo 安装 Hexo NexT主题安装 Hexo Next主题配置 Hexo Next第三方服务集成 写作 在source/_posts下写md文章会自动处理为html文件 每篇文章开头根据需要写 123456789---title: Hexo Next构建个人博客tags: #标签 - 随便写，乐意就要，一个也行 - 随便写，乐意就要categories: # 分类 - 随便写，乐意就要，一个也行 - 随便写，乐意就要--- 部署 安装 hexo-deployer-git。 1$ npm install hexo-deployer-git --save 修改配置。 1`deploy: type: git repo: &lt;repository url&gt; #https://bitbucket.org/JohnSmith/johnsmith.bitbucket.io branch: [branch] #published message: [message]` 参数 描述 repo 库（Repository）地址 branch 分支名称。如果您使用的是 GitHub 或 GitCafe 的话，程序会尝试自动检测。 message 自定义提交信息 (默认为) 生成站点文件并推送至远程库。执行hexo clean &amp;&amp; hexo deploy命令。前者清除站点文件，后者重新生成站点文件并将之推送到指定的库分支。（如果您的Hexo是局部安装，则需要执行hexo clean &amp;&amp; hexo deploy。） 登入 Github/BitBucket/Gitlab，请在库设置（Repository Settings）中将默认分支设置为_config.yml配置中的分支名称。稍等片刻，您的站点就会显示在您的Github Pages中。(不会自己百度) 更多部署https://hexo.io/zh-cn/docs/deployment 至此，一个个人博客就完成了]]></content>
      <categories>
        <category>blog</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo NexT个人博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题安装]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2FHexo%20Next%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。 为了描述方便，在以下说明中，将前者称为 站点配置文件， 后者称为 主题配置文件。 安装 NexT Hexo 安装主题的方式非常简单，只需要将主题文件拷贝至站点目录的 themes 目录下， 然后修改下配置文件即可。具体到 NexT 来说，安装步骤如下。 下载主题如果你熟悉 Git， 建议你使用 克隆最新版本 的方式，之后的更新可以通过 git pull 来快速更新， 而不用再次下载压缩包替换。 在终端窗口下，定位到 Hexo 站点目录下。使用 Git checkout 代码： 12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 启用 NexT 主题 1theme: next 到此，NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前， 我们最好使用 hexo clean 来清除 Hexo 的缓存。 验证主题首先启动 Hexo 本地站点，并开启调试模式（即加上 --debug），整个命令是 hexo s --debug。 在服务启动的过程，注意观察命令行输出是否有任何异常信息，如果你碰到问题，这些信息将帮助他人更好的定位错误。 当命令行输出中提示出： 1INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop. 此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。 当你看到站点的外观与下图所示类似时即说明你已成功安装 NexT 主题。这是 NexT 默认的 Scheme —— Muse 现在，你已经成功安装并启用了 NexT 主题。下一步我们将要更改一些主题的设定，包括个性化以及集成第三方服务。 主题设定选择 SchemeScheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新 Scheme 的切换通过更改主题配置文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。 选择 Pisces Scheme 123#scheme: Muse#scheme: Mistscheme: Pisces 设置语言编辑 站点配置文件， 将 language 设置成你所需要的语言。建议明确设置你所需要的语言，例如选用简体中文，配置如下： 1language: zh-Hans 目前 NexT 支持的语言如以下表格所示： 语言 代码 设定示例 English en language: en 简体中文 zh-Hans language: zh-Hans Français fr-FR language: fr-FR Português pt language: pt or language: pt-BR 繁體中文 zh-hk 或者 zh-tw language: zh-hk Русский язык ru language: ru Deutsch de language: de 日本語 ja language: ja Indonesian id language: id Korean ko language: ko 设置菜单菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。 NexT 使用的是 Font Awesome 提供的图标， Font Awesome 提供了 600+ 的图标，可以满足绝大的多数的场景，同时无须担心在 Retina 屏幕下 图标模糊的问题。 编辑 主题配置文件，修改以下内容： 设定菜单内容，对应的字段是 menu。 菜单内容的设置格式是：item name: link。其中 item name是一个名称，这个名称并不直接显示在页面上，她将用于匹配图标以及翻译。 菜单示例配置: 1234567menu: home: / archives: /archives #about: /about categories: /categories tags: /tags #commonweal: /404.html 若你的站点运行在子目录中，请将链接前缀的 / 去掉 NexT 默认的菜单项有（标注 的项表示需要手动创建这个页面）： | 键值 | 设定值 | 显示文本（简体中文） || ———- | ————————- | ——————– || home | home: / | 主页 || archives | archives: /archives | 归档页 || categories | categories: /categories | 分类页 || tags | tags: /tags | 标签页 || about | about: /about | 关于页面 || commonweal | commonweal: /404.html | 公益 404 | 设置菜单项的显示文本。在第一步中设置的菜单的名称并不直接用于界面上的展示。Hexo 在生成的时候将使用 这个名称查找对应的语言翻译，并提取显示文本。这些翻译文本放置在 NexT 主题目录下的 languages/{language}.yml （{language} 为你所使用的语言）。 以简体中文为例，若你需要添加一个菜单项，比如 something。那么就需要修改简体中文对应的翻译文件 languages/zh-Hans.yml，在 menu 字段下添加一项： 123456789menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 commonweal: 公益404 something: 有料 设定菜单项的图标，对应的字段是 menu_icons。 此设定格式是 item name: icon name，其中 item name 与上一步所配置的菜单名字对应，icon name 是 Font Awesome 图标的 名字。而 enable 可用于控制是否显示图标，你可以设置成 false 来去掉图标。 菜单图标配置示例: 123456789menu_icons: enable: true # Icon Mapping. home: home about: user categories: th tags: tags archives: archive commonweal: heartbeat 在菜单图标开启的情况下，如果菜单项与菜单未匹配（没有设置或者无效的 Font Awesome 图标名字） 的情况下，NexT 将会使用 ? 作为图标 设置侧栏默认情况下，侧栏仅在文章页面（拥有目录列表）时才显示，并放置于右侧位置。 可以通过修改 主题配置文件 中的 sidebar 字段来控制侧栏的行为。侧栏的设置包括两个部分，其一是侧栏的位置， 其二是侧栏显示的时机。 设置侧栏的位置，修改 sidebar.position 的值，支持的选项有： left - 靠左放置 right - 靠右放置 目前仅 Pisces Scheme 支持 position 配置。影响版本5.0.0及更低版本。 12sidebar: position: left 设置侧栏显示的时机，修改 sidebar.display 的值，支持的选项有： post - 默认行为，在文章页面（拥有目录列表）时显示 always - 在所有页面中都显示 hide - 在所有页面中都隐藏（可以手动展开） remove - 完全移除 12sidebar: display: post 已知侧栏在 use motion: false 的情况下不会展示。 影响版本5.0.0及更低版本。 设置头像编辑 主题配置文件， 修改字段 avatar， 值设置成头像的链接地址。其中，头像的链接地址可以是： 地址 值 完整的互联网 URI http://example.com/avatar.png 站点内的地址 将头像放置主题目录下的 source/uploads/ （新建 uploads 目录若不存在） 配置为：avatar: /uploads/avatar.png或者 放置在 source/images/ 目录下 配置为：avatar: /images/avatar.png 头像设置示例: 1avatar: http://example.com/avatar.png 设置作者昵称编辑 站点配置文件， 设置 author 为你的昵称。 站点描述编辑 站点配置文件， 设置 description 字段为你的站点描述。站点描述可以是你喜欢的一句签名:)]]></content>
      <categories>
        <category>blog</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo NexT主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题配置]]></title>
    <url>%2F2019%2F05%2F23%2Fhexo-next-blog%E6%90%AD%E5%BB%BA%2FHexo%20Next%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[设置 RSSNexT 中 RSS 有三个设置选项，满足特定的使用场景。 更改 主题配置文件，设定 rss 字段的值： false：禁用 RSS，不在页面上显示 RSS 连接。 留空：使用 Hexo 生成的 Feed 链接。 你可以需要先安装 hexo-generator-feed 插件。 具体的链接地址：适用于已经烧制过 Feed 的情形。 添加「标签」页面新建「标签」页面，并在菜单中显示「标签」链接。「标签」页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。 底下代码是一篇包含标签的文章的例子： 123456---title: 标签测试文章tags: - Testing - Another Tag--- 请参阅 Hexo 的分类与标签文档，了解如何为文章添加标签或者分类。 新建标签页面 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 tags： 12$ cd your-hexo-site$ hexo new page tags 设置页面类型 编辑刚新建的页面，将页面的类型设置为 tags ，主题将自动为这个页面显示标签云。页面内容如下： 1234title: 标签date: 2019-01-08 12:39:04type: &quot;tags&quot;--- 修改菜单 在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： ​ 禁用评论示例 12345title: 标签date: 2019-01-08 12:39:04type: &quot;tags&quot;comments: false--- 添加「分类」页面新建「分类」页面，并在菜单中显示「分类」链接。「分类」页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。 底下代码是一篇包含分类的文章的例子： 123title: 分类测试文章categories: Testing--- 请参阅 Hexo 的分类与标签文档，了解如何为文章添加标签或者分类。 新建页面 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 categories ： 12$ cd your-hexo-site$ hexo new page categories 设置页面类型 编辑刚新建的页面，将页面的 type 设置为 categories ，主题将自动为这个页面显示分类。页面内容如下： 1234title: 分类date: 2019-01-08 12:39:04type: "categories"--- 修改菜单 在菜单中添加链接。编辑 主题配置文件 ， 添加 categories 到 menu 中，如下: 1234menu: home: / archives: /archives categories: /categories 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 禁用评论示例: 12345title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false--- 设置字体注意： 此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 为了解决 Google Fonts API 不稳定的问题，NexT 在 5.0.1 中引入此特性。 通过此特性，你可以指定所使用的字体库外链地址；与此同时，NexT 开放了 5 个特定范围的字体设定，他们是： 全局字体：定义的字体将在全站范围使用 标题字体：文章内标题的字体（h1, h2, h3, h4, h5, h6） 文章字体：文章所使用的字体 Logo字体：Logo 所使用的字体 代码字体： 代码块所使用的字体 各项所指定的字体将作为首选字体，当他们不可用时会自动 Fallback 到 NexT 设定的基础字体组： 非代码类字体：Fallback 到 &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, sans-serif 代码类字体： Fallback 到 consolas, Menlo, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, monospace 另外，每一项都有一个额外的 external 属性，此属性用来控制是否使用外链字体库。 开放此属性方便你设定那些已经安装在系统中的字体，减少不必要的请求（请求大小）。 配置示例: 12345678910111213141516171819202122232425262728293031font: enable: true # 外链字体库地址，例如 //fonts.googleapis.com (默认值) host: # 全局字体，应用在 body 元素上 global: external: true family: Monda # 标题字体 (h1, h2, h3, h4, h5, h6) headings: external: true family: Roboto Slab # 文章字体 posts: external: true family: # Logo 字体 logo: external: true family: Lobster Two size: 24 # 代码字体，应用于 code 以及代码块 codes: external: true family: PT Mono 设置代码高亮主题NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties： 更改 highlight_theme 字段，将其值设定成你所喜爱的高亮主题，例如： 高亮主题设置示例: 1234# Code Highlight theme# Available value: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal 侧边栏社交链接侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件 中。 链接放置在 social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址。 配置示例 12345678# Social linkssocial: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 微博: http://weibo.com/your-user-name 豆瓣: http://douban.com/people/your-user-name 知乎: http://www.zhihu.com/people/your-user-name # 等等 设定链接的图标，对应的字段是 social_icons。其键值格式是 匹配键: Font Awesome 图标名称， 匹配键 与上一步所配置的链接的 显示文本 相同（大小写严格匹配），图标名称 是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，你可以设置成 false 来去掉图标。 配置示例 1234567# Social Iconssocial_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 微博: weibo 开启打赏功能 由 habren 贡献越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 主题配置文件 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能。 打赏功能配置示例 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 友情链接 由 iamwent 贡献编辑 主题配置文件 添加： 友情链接配置示例 12345# titlelinks_title: Linkslinks: MacTalk: http://macshuo.com/ Title: http://example.com/ 腾讯公益404页面 由 xirong 贡献腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如下 http://www.ixirong.com/404.html 使用方法，新建 404.html 页面，放到主题的 source 目录下，内容如下： 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 站点建立时间这个时间将在站点的底部显示，例如 © 2013 - 2015。 编辑 主题配置文件，新增字段 since。 配置示例 1since: 2013 订阅微信公众号 由 huiwang 贡献注意： 此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后 在每篇文章的末尾显示微信公众号二维码，扫一扫，轻松订阅博客。 在微信公众号平台下载您的二维码，并将它存放于博客source/uploads/目录下。 然后编辑 主题配置文件，如下： 配置示例 12345# Wechat Subscriberwechat_subscriber: enabled: true qcode: /uploads/wechat-qcode.jpg description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！ 设置「动画效果」NexT 默认开启动画效果，效果使用 JavaScript 编写，因此需要等待 JavaScript 脚本完全加载完毕后才会显示内容。 如果您比较在乎速度，可以将设置此字段的值为 false 来关闭动画。 编辑 主题配置文件， 搜索 use_motion，根据您的需求设置值为 true 或者 false 即可： 12use_motion: true # 开启动画效果use_motion: false # 关闭动画效果 设置「背景动画」NexT 自带两种背景动画效果 编辑 主题配置文件， 搜索 canvas_nest 或 three_waves，根据您的需求设置值为 true 或者 false即可： 注意： three_waves 在版本 5.1.1 中引入。只能同时开启一种背景动画效果。 canvas_nest 配置示例: 123# canvas_nestcanvas_nest: true //开启动画canvas_nest: false //关闭动画 three_waves 配置示例: 123# three_wavesthree_waves: true //开启动画three_waves: false //关闭动画]]></content>
      <categories>
        <category>blog</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo NexT主题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop常见报错]]></title>
    <url>%2F2019%2F05%2F23%2Fhadoop%2FHadoop%E5%B8%B8%E8%A7%81%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[使用root用户配置启动Hadoop报错报错信息12345678910[root@root hadoop-3.1.2]# sbin/start-dfs.shStarting namenodes on [node1]ERROR: Attempting to operate on hdfs namenode as rootERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.Starting datanodesERROR: Attempting to operate on hdfs datanode as rootERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.Starting secondary namenodes [node1]ERROR: Attempting to operate on hdfs secondarynamenode as rootERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation. 解决办法 修改Hadoop目录中sbin/start-dfs.sh，在文件顶部添加如下参数 1234HDFS_DATANODE_USER=rootHADOOP_SECURE_DN_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root 修改Hadoop目录中sbin/start-yarn.sh，在文件顶部添加如下参数 123YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarnYARN_NODEMANAGER_USER=root Hadoop启动时SSH报错报错信息1234567891011[root@root hadoop-3.1.2]# sbin/start-dfs.shWARNING: HADOOP_SECURE_DN_USER has been replaced by HDFS_DATANODE_SECURE_USER. Using value of HADOOP_SECURE_DN_USER.Starting namenodes on [node1]Last login: Mon Apr 22 00:52:56 PDT 2019 from 192.168.239.1 on pts/0node1: ssh: Could not resolve hostname node1: Name or service not knownStarting datanodesLast login: Mon Apr 22 01:31:06 PDT 2019 on pts/0node1: ssh: Could not resolve hostname node1: Name or service not knownStarting secondary namenodes [node1]Last login: Mon Apr 22 01:31:22 PDT 2019 on pts/0node1: ssh: Could not resolve hostname node1: Name or service not known 解决办法配置/etc/hosts文件，将其中的中的localhost.localdomain 替换为node1 即修改如下配置 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 修改为： 12127.0.0.1 localhost node1 localhost4 localhost4.localdomain4::1 localhost node1 localhost6 localhost6.localdomain6 Hadoop启动报错JAVA_HOME未找到错误信息123456789101112[root@node1 hadoop-3.1.2]# ./sbin/start-dfs.sh WARNING: HADOOP_SECURE_DN_USER has been replaced by HDFS_DATANODE_SECURE_USER. Using value of HADOOP_SECURE_DN_USER.Starting namenodes on [node1]Last login: Mon Apr 22 01:47:10 PDT 2019 from 192.168.239.1 on pts/0node1: Warning: Permanently added &apos;node1,fe80::4a4e:4fca:6a97:380d%ens33&apos; (ECDSA) to the list of known hosts.node1: ERROR: JAVA_HOME is not set and could not be found.Starting datanodesLast login: Mon Apr 22 01:47:34 PDT 2019 on pts/0node1: ERROR: JAVA_HOME is not set and could not be found.Starting secondary namenodes [node1]Last login: Mon Apr 22 01:48:46 PDT 2019 from 192.168.239.1 on pts/1node1: ERROR: JAVA_HOME is not set and could not be found. 解决办法修改hadoop-env.sh配置文件，在其中显示地重新声明JAVA_HOME，在文件顶部添加如下配置： 1export JAVA_HOME=/usr/local/java/jdk1.8.0_211]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop搭建HA集群]]></title>
    <url>%2F2019%2F05%2F23%2Fhadoop%2FHadoop%E6%90%AD%E5%BB%BAHA%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[集群规划 NameNode-1 NameNode-2 DN ZK ZKFC JNN RSM NM hadoop01 * * * hadoop02 * * * * * * hadoop03 * * * * * hadoop04 * * * * 注意：DN:DataNode；ZKFC：ZKFailoverController，要部署在NameNode节点上；JNN:JournalNode；RSM:ResourceManager；NM:NodeManager 集群配置官网配置详情HDFS的HA配置官网 YARN的HA配置官网 配置hadoop-env.sh在文件末尾添加如下配置： 12345678export JAVA_HOME=/usr/local/java/jdk1.8.0_211export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_JOURNALNODE_USER=rootexport HDFS_ZKFC_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root 否则会报如下错误： ERROR: Attempting to operate on hdfs journalnode as rootERROR: but there is no HDFS_JOURNALNODE_USER defined. Aborting operation.ERROR: Attempting to operate on hdfs zkfc as rootERROR: but there is no HDFS_ZKFC_USER defined. Aborting operation. ERROR: Attempting to operate on yarn resourcemanager as rootERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.Starting nodemanagersERROR: Attempting to operate on yarn nodemanager as rootERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation. 配置hdfs-site.xml配置&lt;configuration&gt;节点，添加如下内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop01:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop02:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop01:9870&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop02:9870&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/HA/journalnode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置core-site.xml配置&lt;configuration&gt;节点，添加如下内容： 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/HA&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop02:2181,hadoop03:2181,hadoop04:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置mapred-site.xml配置&lt;configuration&gt;节点，添加如下内容： 1234567891011121314151617181920&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /root/hadoop-3.1.2/etc/hadoop, /root/hadoop-3.1.2/share/hadoop/common/*, /root/hadoop-3.1.2/share/hadoop/common/lib/*, /root/hadoop-3.1.2/share/hadoop/hdfs/*, /root/hadoop-3.1.2/share/hadoop/hdfs/lib/*, /root/hadoop-3.1.2/share/hadoop/mapreduce/*, /root/hadoop-3.1.2/share/hadoop/mapreduce/lib/*, /root/hadoop-3.1.2/share/hadoop/yarn/*, /root/hadoop-3.1.2/share/hadoop/yarn/lib/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置yarn-site.xml配置&lt;configuration&gt;节点，添加如下内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop04&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop03:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop04:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.zk.address&lt;/name&gt; &lt;value&gt;hadoop02:2181,hadoop03:2181,hadoop04:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Zookeeper集群配置安装 下载安装包(地址)，并解压 配置Zookeeper配置文件 进入Zookeeper安装的配置目录：cd $ZOOKEEPER_HOME/conf 复制zoo_sample.cfg为zoo.cfg：cp zoo_sample.cfg zoo.cfg 配置zoo.cfg 1234567# 修改 dataDir 的位置dataDir=/var/zookeeper# 在文末新增集群配置server.1=hadoop02:2888:3888server.2=hadoop03:2888:3888server.3=hadoop04:2888:3888 创建dataDir(mkdir /var/zookeeper)目录，并在其中创建myid文件，在hadoop02设置1，在hadoop03设置2，在hadoop04设置3 12345678# hadoop02echo 1 &gt; /var/zookeeper/myid# hadoop03echo 2 &gt; /var/zookeeper/myid# hadoop04echo 3 &gt; /var/zookeeper/myid 启动集群启动JournalNode分别在hadoop01、hadoop02、hadoop03节点执行如下命令： 1hdfs --daemon start journalnode 初始化NameNode在hadoop01节点执行： 1hdfs namenode -format 启动NameNode在hadoop01节点执行： 1hdfs daemon start namenode 备份NameNode元数据信息在hadoop02节点执行： 1hdfs namenode -bootstrapStandby 启动Zookeeper集群在hadoop02、hadoop03、hadoop04节点执行如下命令(已配置ZK环境变量)： 1zkServer.sh start 环境变量配置： 修改/etc/profile,执行vi + /etc/profile，在文件末尾添加如下内容： 123&gt; export ZOOKEEPER_HOME=/root/zookeeper-3.4.12&gt; PATH=$PATH:$ZOOKEEPER_HOME/bin&gt; 在Zookeeper中初始化声明HA在hadoop01节点执行： 1hdfs zkfc -formatZK 启动HDFS在hadoop01节点执行： 1start-dfs.sh 启动YARN 在hadoop01节点启动YARN：start-yarn.sh 分别在hadoop03和hadoop04节点启动RSM：yarn-daemon.sh start resourcemanager 验证HA集群搭建成功在Zookeeper中验证HDFS执行get /hadoop-ha/mycluster/ActiveBreadCrumb可以得到如下信息： 12345678910111213 myclusternnhadoop01 ?&gt;(?&gt;cZxid = 0x50000000dctime = Thu Apr 25 05:54:37 PDT 2019mZxid = 0x50000000dmtime = Thu Apr 25 05:54:37 PDT 2019pZxid = 0x50000000dcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 32numChildren = 0 执行get /hadoop-ha/mycluster/ActiveStandbyElectorLock可以得到如下信息： 12345678910111213 myclusternnhadoop02 �&gt;(�&gt;cZxid = 0x5000000cfctime = Thu Apr 25 09:25:29 PDT 2019mZxid = 0x5000000cfmtime = Thu Apr 25 09:25:29 PDT 2019pZxid = 0x5000000cfcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x300013f8c990016dataLength = 32numChildren = 0 不难发现hadoop01处于active状态，hadoop02处于standby`状态。 通过Web页面访问验证HDFS输入hadoop01:9870进行访问，可以看到hadoop01处于active状态，hadoop02处于standby状态 停止hadoop01服务器的NameNode(hdfs daemon stop namenode)刷新两个网站，可以看到hadoop01无法访问，但hadoop02的状态成为了active 通过Web页面访问验证YARN输入hadoop03:9870进行访问 点击About，可以知道此时hadoop03节点的RSM是处于active状态 然后直接将地址栏中hadoop03改为hadoop04，可以查看到hadoop04节点的RSM处于standby状态 注意：直接输入hadoop04:8088会直接重定向到hadoop03:8088]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop全分布式搭建]]></title>
    <url>%2F2019%2F05%2F23%2Fhadoop%2FHadoop%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[安装虚拟机 下载CentOS 7镜像 使用Vmware安装镜像 将安装好的服务器克隆(完全克隆)三份 配置克隆后的服务器 修改网卡信息(MAC地址) 选择虚拟机，然后选择虚拟机 &gt; 设置。 在硬件选项卡中，选择虚拟网络适配器，然后单击高级。 在 MAC 地址文本框中键入新的 MAC 地址，或者单击生成以让 Workstation Pro 生成一个新地址。 单击确定以保存所做的更改。 修改主机名 执行vi /etc/sysconfig/network，添加或修改为如下内容 12NETWORKING=yesHOSTNAME=node1 注意：node1为当前主机名，不同主机设置不同主机名 修改IP信息 执行vi /etc/sysconfig/network-scripts/ifcfg-ens33，其中ifcfg-ens33为网卡名(根据实际情况设置)，修改其中的UUID(使每台机器不同即可)，IP(使每台机器不同即可)、HWADDR(与第一步设置的MAC地址信息一致即可) 12345678910111213141516171819TYPE="Ethernet"PROXY_METHOD="none"BROWSER_ONLY="no"BOOTPROTO="static"DEFROUTE="yes"IPV4_FAILURE_FATAL="no"IPV6INIT="yes"IPV6_AUTOCONF="yes"IPV6_DEFROUTE="yes"IPV6_FAILURE_FATAL="no"IPV6_ADDR_GEN_MODE="stable-privacy"NAME="ens33"UUID="fdd6ef22-2bc9-41f1-b2cb-b52e2d369bb5"DEVICE="ens33"ONBOOT="yes"IPADDR=192.168.239.144GATEWAY=192.168.239.2NETMASK=255.255.255.0HWADDR=00:50:56:23:2B:1C 修改映射 执行vi + /etc/hosts，在文末添加IP地址与主机名的映射关系 1234192.168.239.144 node1192.168.239.143 node2192.168.239.145 node3192.168.239.146 node4 重启网卡 执行service network restart即可 全分布式Hadoop集群集群规划 主机名 IP地址 功能 node1 192.168.239.144 NamdNode、ResourceManager node2 192.168.239.143 DataNade、SecordaryNameNode、NodeManager node3 192.168.239.145 DataNade、NodeManager node4 192.168.239.146 DataNade、NodeManager 环境搭建安装JDK解压JDK压缩包，配置环境变量即可 安装Hadoop解压Hadoop压缩包，配置环境变量即可 配置SSH配置hadoop-env.sh进入hadoop的安装目录cd $HADOOP_HOME，修改hadoop-env.sh文件vi + etc/hadoop/hadoop-env.sh 1234export JAVA_HOME=/usr/local/java/jdk1.8.0_211export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=root 配置core-site.xml修改core-site.xml文件vi etc/hadoop/core-site.xml（进入hadoop的安装目录cd $HADOOP_HOME） 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node1:9820&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置hdfs-site.xml修改hdfs-site.xml文件vi etc/hadoop/hdfs-site.xml（进入hadoop的安装目录cd $HADOOP_HOME） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;configuration&gt; &lt;!-- 副本数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小 --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; &lt;!-- HDFS的元数据存储位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;!-- HDFS的数据存储位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;!-- HDFS的检测目录 --&gt; &lt;property&gt; &lt;name&gt;fs.checkpoint.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/checkpoint/dfs/cname&lt;/value&gt; &lt;/property&gt; &lt;!-- HDFS的NameNode的Web UI地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;node1:9870&lt;/value&gt; &lt;/property&gt; &lt;!-- HDFS的SceordaryNameNode的Web UI地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node1:9868&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否开启Web操作HDFS --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 是否启用HDFS的权限(ACL) --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置mapred-site.xml修改mapred-site.xml文件vi etc/hadoop/mapred-site.xml（进入hadoop的安装目录cd $HADOOP_HOME） 123456789101112131415161718&lt;configuration&gt; &lt;!--指定mapreduce运行框架--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--历史服务的通讯地址--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node1:10020&lt;/value&gt; &lt;/property&gt; &lt;!--历史服务Web UI地址--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node1:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置yarn-site.xml修改yarn-site.xml文件vi etc/hadoop/yarn-site.xml（进入hadoop的安装目录cd $HADOOP_HOME） 12345678910111213141516171819202122232425262728293031323334353637&lt;configuration&gt; &lt;!--指定ResouceManager所启动的服务器主机名--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node1&lt;/value&gt; &lt;/property&gt; &lt;!--指定MapReduce的shuffle--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--指定ResouceManager的内部通信地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;node1:8032&lt;/value&gt; &lt;/property&gt; &lt;!--指定ResouceManager的scheduler的内部通信地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;node1:8030&lt;/value&gt; &lt;/property&gt; &lt;!--指定ResouceManager的resource-tracker的内部通信地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;node1:8031&lt;/value&gt; &lt;/property&gt; &lt;!--指定ResouceManager的admin的内部通信地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;node1:8033&lt;/value&gt; &lt;/property&gt; &lt;!--指定ResouceManager的Web UI监控地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;node1:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置workers修改workers文件vi etc/hadoop/workers（进入hadoop的安装目录cd $HADOOP_HOME） 123node2node3node4 将配置远程分发到所有机器上123scp -r ../hadoop-3.1.1 node2:/rootscp -r ../hadoop-3.1.1 node3:/rootscp -r ../hadoop-3.1.1 node4:/root 启动之前，在NameNode服务器上先格式化，只需要一次即可1hadoop namenode -format 启动集群 全启动： start-all.sh 模块启动： start-dfs.sh start-yarn.sh 单个进程启动： hadoop-daemon.sh start/stop namenode hadoop-daemons.sh start/stop datanode yarn-daemon.sh start/stop namenode yarn-daemons.sh start/stop datanode mr-jobhistory-daemon.sh start/stop historyserver]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门]]></title>
    <url>%2F2019%2F05%2F23%2Fhadoop%2FHadoop%2F</url>
    <content type="text"><![CDATA[Hadoop简介​ Apache Hadoop™®项目开发开源软件可靠、可扩展、分布式计算。 ​ Apache Hadoop软件图书馆是一个框架,允许跨集群的分布式处理大型数据集的电脑使用简单的编程模型。它的目的是扩大从单一服务器到成千上万的机器,每个提供本地计算和存储。而不是依靠硬件来实现高可用性,图书馆本身的目的是检测和处理失败在应用程序层,所以提供高可用性服务的一个计算机集群,每一种都可能容易失败。 Hadoop的主要组成模块： Hadoop Common: 常见的实用程序,支持其他Hadoop模块。 Hadoop Distributed File System (HDFS™): 一个分布式文件系统,它提供了高通量访问应用程序数据。 Hadoop YARN: 一个集群作业调度和资源管理的框架。 Hadoop MapReduce:YARN-based系统并行处理大型数据集。 Hadoop Ozone: Hadoop的对象存储。 Hadoop Submarine: Hadoop的机器学习引擎。 Hadoop 存储模型(字节) 文件先行切割成块(Block):偏移量 offset (byte, 中文？) Block分散存储在集群节点中 单一文件Block大小一致，文件与文件可以不一致 Block可以设置副本数，副本无序分散在不同节点中 副本数不要超过节点数量 文件上传可以设置Block大小和副本数(资源不够开辟的进程) 已上传的文件Block副本数可以调整，大小不变 只支持一次写入多次读取，同一时刻只有一个写入者 可以append追加数据 Block的副本放置策略 第一个副本：放置在上传文件的DN(DataNode)；如果时集群外提交，则随机挑选一台磁盘不太慢，CPU不太忙的节点。 第二个副本：放置在第一个副本不同的机架的节点上 第三个副本：与第二个副本相同机架的节点。 更多副本：随机节点 Hadoop 架构模型 文件元数据(MetaData)，文件数据 元数据 数据本身 (主)NameNode 节点保存文件元数据：单节点 posix (从)DataNode节点保存文件Block数据：多节点 DataNode 与 NameNode 保持心跳，提交 Block 列表 HDFSClient 与 NameNode 交互元数据信息 HDFSClient 与 DataNode 交互文件 Block 数据(cs) DataNode 利用服务器本地文件系统存储数据块 Hadoop 3 新特性 Classpath isolation 防止不同版本jar包冲突 Shell重写 支持HDFS中的擦除编码Erasure Encoding 默认的EC策略可以节省50%的存储空间，同时还可以承受更多的存储故障 DataNode内部添加了负载均衡 Disk Balancer 磁盘之间的负载均衡 MapReduce任务级本地优化 MapReduce内存参数自动推断 mapreduce.{map,reduce}.memory.mb 和 mapreduce.{map,reduce}.java.opts 基于cgroup的内存隔离和IO Disk隔离 支持更改分配容器的资源Container resizing HDFSHDFS 概念HDFS源自于Google的GFS论文，全称 Hadoop Distributed File System。是一个易于扩展的分布式文件系统，运行在大量普通的廉价的机器上，并且提供容错机制，为大量用户提供性能不错的文件存取服务。 HDFS 设计目标 存储量大 自动快速检测应对硬件错误 流式访问数据 移动计算(程序)比移动数据本身更划算 简单一致性模型。一次写入，多次读取 异构平台可移植 HDFS 的特点优点 高可靠性：Hadoop一般都在成千的计算机集群之上，且可以搭建Hadoop的高可靠集群，及内部容错功能优秀 高扩展性：Hadoop时可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便的扩展到数以千计的节点中 高效性：Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快 高容错性：Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配 缺点 不适合低延迟数据访问 无法高效存储大量小文件 不支持多用户写入及任意修改文件 HDFS 三个服务 NadmNode：负责客户端请求的响应；元数据的管理（查询、修改） SecondaryNameNode：合并NameNode的edit logs到fsimage文件中，即做持久化工作 DataNode：存储管理用户的文件块数据；定期向NameNode汇报自身所持有的block信息 Hadoop 单机版安装安装教程：官网文档]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git快速入手]]></title>
    <url>%2F2019%2F05%2F23%2FGit%2FGit%E5%BF%AB%E9%80%9F%E5%85%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[简介Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Git is easy to learn and has a tiny footprint with lightning fast performance. It outclasses SCM tools like Subversion, CVS, Perforce, and ClearCase with features like cheap local branching, convenient staging areas, and multiple workflows. 官网 下载 git安装 官方教程：https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git windows详细安装教程：https://www.jianshu.com/p/414ccd423efc 配置 git config –list git config –-global user.name &#39;yourname&#39; git config –-global user.email &#39;youremail&#39; ssh-keygen -t rsa -C &#39;youremail&#39; 把本地的 ~/.ssh/xxx.pub 添加到github类似平台的SSH keys中 常用命令 创建自己远端项目 github gitee 码市 gitlab … 分支操作： git branch 创建分支 git branch -b 创建并切换到新建的分支上 git checkout 切换分支 git branch 查看分支列表 git branch -v 查看所有分支的最后一次操作 git branch -vv 查看当前分支 git brabch -b 分支名 origin/分支名 创建远程分支到本地 git branch –merged 查看别的分支和当前分支合并过的分支 git branch –no-merged 查看未与当前分支合并的分支 git branch -d 分支名 删除本地分支 git branch -D 分支名 强行删除分支 git branch origin :分支名 删除远处仓库分支 git merge 分支名 合并分支到当前分支上 暂存操作： git stash 暂存当前修改 git stash apply 恢复最近的一次暂存 git stash pop 恢复暂存并删除暂存记录 git stash list 查看暂存列表 git stash drop 暂存名(例：stash@{0}) 移除某次暂存 git stash clear 清除暂存 回退操作： git reset –hard HEAD^ 回退到上一个版本 git reset –hard ahdhs1(commit_id) 回退到某个版本 git checkout – file撤销修改的文件(如果文件加入到了暂存区，则回退到暂存区的，如果文件加入到了版本库，则还原至加入版本库之后的状态) git reset HEAD file 撤回暂存区的文件修改到工作区 标签操作： git tag 标签名 添加标签(默认对当前版本) git tag 标签名 commit_id 对某一提交记录打标签 git tag -a 标签名 -m ‘描述’ 创建新标签并增加备注 git tag 列出所有标签列表 git show 标签名 查看标签信息 git tag -d 标签名 删除本地标签 git push origin 标签名 推送标签到远程仓库 git push origin –tags 推送所有标签到远程仓库 git push origin :refs/tags/标签名 从远程仓库中删除标签 常规操作： git push origin test 推送本地分支到远程仓库 git rm -r –cached 文件/文件夹名字 取消文件被版本控制 git reflog 获取执行过的命令 git log –graph 查看分支合并图 git merge –no-ff -m ‘合并描述’ 分支名 不使用Fast forward方式合并，采用这种方式合并可以看到合并记录 git check-ignore -v 文件名 查看忽略规则 git add -f 文件名 强制将文件提交 git创建项目仓库： git init 初始化 git remote add origin url 关联远程仓库 git pull git fetch 获取远程仓库中所有的分支到本地 忽略已加入到版本库中的文件： git update-index –assume-unchanged file 忽略单个文件 git rm -r –cached 文件/文件夹名字 (. 忽略全部文件) 取消忽略文件： git update-index –no-assume-unchanged file 拉取、上传免密码： git config –global credential.helper store]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git克隆提交出现ssh connect to host github.com port 22 Connection timed out问题]]></title>
    <url>%2F2019%2F05%2F23%2FGit%2FGit%E5%85%8B%E9%9A%86%E6%8F%90%E4%BA%A4%E5%87%BA%E7%8E%B0ssh%20connect%20to%20host%20github.com%20port%2022%20Connection%20timed%20out%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题使用ssh克隆github上面的代码时出现问题： 分析：输出信息prot 22: Operation timed out，初步分析端口问题我们修改端口试试，这里将端口改为443。 解决方法在存放私钥公钥（id_rsa和id_rsa.pub）文件里，新建config文本。命令vim ~/.ssh/config,输入一下内容： 123456Host github.comUser YourEmail@163.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 保存退出。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Docker 安装 FastDFS]]></title>
    <url>%2F2019%2F05%2F23%2FFastDFS%2F%E5%9F%BA%E4%BA%8E%20Docker%20%E5%AE%89%E8%A3%85%20FastDFS%2F</url>
    <content type="text"><![CDATA[环境准备所需全部环境配置文件及安装包 libfastcommon.tar.gz fastdfs-5.11.tar.gz nginx-1.13.6.tar.gz fastdfs-nginx-module_v1.16.tar.gz 创建工作目录在 Linux 服务器上创建 /usr/local/docker/fastdfs/environmen 目录 说明： /usr/local/docker/fastdfs：用于存放 docker-compose.yml 配置文件及 FastDFS 的数据卷 /usr/local/docker/fastdfs/environmen：用于存放 Dockerfile 镜像配置文件及 FastDFS 所需环境 docker-compose.yml123456789version: '3.1'services: fastdfs: build: environment restart: always container_name: fastdfs volumes: - ./storage:/fastdfs/storage network_mode: host Dockerfile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960FROM ubuntu:xenialMAINTAINER topsale@vip.qq.com# 更新数据源WORKDIR /etc/aptRUN echo &apos;deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiverse&apos; &gt; sources.listRUN echo &apos;deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiverse&apos; &gt;&gt; sources.listRUN echo &apos;deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiverse&apos; &gt;&gt; sources.listRUN echo &apos;deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse&apos; &gt;&gt; sources.listRUN apt-get update# 安装依赖RUN apt-get install make gcc libpcre3-dev zlib1g-dev --assume-yes# 复制工具包ADD fastdfs-5.11.tar.gz /usr/local/srcADD fastdfs-nginx-module_v1.16.tar.gz /usr/local/srcADD libfastcommon.tar.gz /usr/local/srcADD nginx-1.13.6.tar.gz /usr/local/src# 安装 libfastcommonWORKDIR /usr/local/src/libfastcommonRUN ./make.sh &amp;&amp; ./make.sh install# 安装 FastDFSWORKDIR /usr/local/src/fastdfs-5.11RUN ./make.sh &amp;&amp; ./make.sh install# 配置 FastDFS 跟踪器ADD tracker.conf /etc/fdfsRUN mkdir -p /fastdfs/tracker# 配置 FastDFS 存储ADD storage.conf /etc/fdfsRUN mkdir -p /fastdfs/storage# 配置 FastDFS 客户端ADD client.conf /etc/fdfs# 配置 fastdfs-nginx-moduleADD config /usr/local/src/fastdfs-nginx-module/src# FastDFS 与 Nginx 集成WORKDIR /usr/local/src/nginx-1.13.6RUN ./configure --add-module=/usr/local/src/fastdfs-nginx-module/srcRUN make &amp;&amp; make installADD mod_fastdfs.conf /etc/fdfsWORKDIR /usr/local/src/fastdfs-5.11/confRUN cp http.conf mime.types /etc/fdfs/# 配置 NginxADD nginx.conf /usr/local/nginx/confCOPY entrypoint.sh /usr/local/bin/ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;]WORKDIR /EXPOSE 8888CMD [&quot;/bin/bash&quot;] entrypoint.sh1234#!/bin/sh/etc/init.d/fdfs_trackerd start/etc/init.d/fdfs_storaged start/usr/local/nginx/sbin/nginx -g &apos;daemon off;&apos; 注：Shell 创建后是无法直接使用的，需要赋予执行的权限，使用 chmod +x entrypoint.sh 命令 各种配置文件说明tracker.confFastDFS 跟踪器配置，容器中路径为：/etc/fdfs，修改为： 1base_path=/fastdfs/tracker storage.confFastDFS 存储配置，容器中路径为：/etc/fdfs，修改为： 1234base_path=/fastdfs/storagestore_path0=/fastdfs/storagetracker_server=192.168.75.128:22122http.server_port=8888 client.confFastDFS 客户端配置，容器中路径为：/etc/fdfs，修改为： 12base_path=/fastdfs/trackertracker_server=192.168.75.128:22122 configfastdfs-nginx-module 配置文件，容器中路径为：/usr/local/src/fastdfs-nginx-module/src，修改为： 1234567# 修改前CORE_INCS=&quot;$CORE_INCS /usr/local/include/fastdfs /usr/local/include/fastcommon/&quot;CORE_LIBS=&quot;$CORE_LIBS -L/usr/local/lib -lfastcommon -lfdfsclient&quot;# 修改后CORE_INCS=&quot;$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/&quot;CORE_LIBS=&quot;$CORE_LIBS -L/usr/lib -lfastcommon -lfdfsclient&quot; mod_fastdfs.conffastdfs-nginx-module 配置文件，容器中路径为：/usr/local/src/fastdfs-nginx-module/src，修改为： 1234connect_timeout=10tracker_server=192.168.75.128:22122url_have_group_name = truestore_path0=/fastdfs/storage nginx.confNginx 配置文件，容器中路径为：/usr/local/src/nginx-1.13.6/conf，修改为： 1234567891011121314151617181920212223242526272829user root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 8888; server_name localhost; location ~/group([0-9])/M00 &#123; ngx_fastdfs_module; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 启动容器1docker-compose up -d 测试上传交互式进入容器1docker exec -it fastdfs /bin/bash 测试文件上传1/usr/bin/fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/fastdfs-5.11/INSTALL 服务器反馈上传地址1group1/M00/00/00/wKhLi1oHVMCAT2vrAAAeSwu9hgM3976341 测试 Nginx 访问1http://192.168.0.128:8888/group1/M00/00/00/wKhLi1oHVMCAT2vrAAAeSwu9hgM3976341]]></content>
      <categories>
        <category>FastDFS</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>FastDFS</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下安装FastDFS]]></title>
    <url>%2F2019%2F05%2F23%2FFastDFS%2FUbuntu%E4%B8%8B%E5%AE%89%E8%A3%85FastDFS%2F</url>
    <content type="text"><![CDATA[Ubuntu安装FastDFS安装依赖安装libevent1234567891011121314防火墙ufw enableufw disable自启动管理：apt-get install sysv-rc-confapt-get install makeapt-get install unzipapt-get install gccapt-get install libevent-dev 安装libfastcommon通过git下载即可： https://github.com/happyfish100/libfastcommon.git 12apt install unzipapt install gcc nginx依赖12345678910111213安装gcc g++的依赖库sudo apt-get install build-essentialsudo apt-get install libtool安装pcre依赖库（http://www.pcre.org/）sudo apt-get updatesudo apt-get install libpcre3 libpcre3-dev安装zlib依赖库（http://www.zlib.net）sudo apt-get install zlib1g-dev安装SSL依赖库（16.04默认已经安装了）sudo apt-get install openssl]]></content>
      <categories>
        <category>FastDFS</category>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>FastDFS</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastDFS快速入门]]></title>
    <url>%2F2019%2F05%2F23%2FFastDFS%2FFastDFS%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[什么是FastDFSFastDFS是由淘宝的余庆先生所开发的一个轻量级、高性能的开源分布式文件系统。用纯C语言开发，功能丰富： 文件存储 文件同步 文件访问（上传、下载） 存取负载均衡 在线扩容 适合有大容量存储需求的应用或系统。同类的分布式文件系统有谷歌的GFS、HDFS（Hadoop）、TFS（淘宝）等。 FastDFS的架构架构图先上图： FastDFS两个主要的角色：Tracker Server 和 Storage Server 。 Tracker Server：跟踪服务器，主要负责调度storage节点与client通信，在访问上起负载均衡的作用，和记录storage节点的运行状态，是连接client和storage节点的枢纽。 Storage Server：存储服务器，保存文件和文件的meta data（元数据），每个storage server会启动一个单独的线程主动向Tracker cluster中每个tracker server报告其状态信息，包括磁盘使用情况，文件同步情况及文件上传下载次数统计等信息 Group：文件组，多台Storage Server的集群。上传一个文件到同组内的一台机器上后，FastDFS会将该文件即时同步到同组内的其它所有机器上，起到备份的作用。不同组的服务器，保存的数据不同，而且相互独立，不进行通信。 Tracker Cluster：跟踪服务器的集群，有一组Tracker Server（跟踪服务器）组成。 Storage Cluster ：存储集群，有多个Group组成。 上传和下载流程 上传 Client通过Tracker server查找可用的Storage server。 Tracker server向Client返回一台可用的Storage server的IP地址和端口号。 Client直接通过Tracker server返回的IP地址和端口与其中一台Storage server建立连接并进行文件上传。 上传完成，Storage server返回Client一个文件ID，文件上传结束。 下载 Client通过Tracker server查找要下载文件所在的的Storage server。 Tracker server向Client返回包含指定文件的某个Storage server的IP地址和端口号。 Client直接通过Tracker server返回的IP地址和端口与其中一台Storage server建立连接并指定要下载文件。 下载文件成功。 安装和使用安装资源准备，将如下文件下载 百度云盘资源下载地址 Centos安装：站内搜索Centos下安装FastDFSUbuntu安装：站内搜索Ubuntu下安装FastDFSjava客户端余庆先生提供了一个Java客户端，但是作为一个C程序员，写的java代码可想而知。而且已经很久不维护了。 这里推荐一个开源的FastDFS客户端，支持最新的SpringBoot2.0。 配置使用极为简单，支持连接池，支持自动生成缩略图，狂拽酷炫吊炸天啊，有木有。 地址：tobato/FastDFS_client 引入依赖在父工程中，我们已经管理了依赖，版本为： 1&lt;fastDFS.client.version&gt;1.26.2&lt;/fastDFS.client.version&gt; 因此，这里我们直接引入坐标即可： 1234&lt;dependency&gt; &lt;groupId&gt;com.github.tobato&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt;&lt;/dependency&gt; 引入配置类纯java配置： 123456@Configuration@Import(FdfsClientConfig.class)// 解决jmx重复注册bean的问题@EnableMBeanExport(registration = RegistrationPolicy.IGNORE_EXISTING)public class FastClientImporter &#123;&#125; 编写FastDFS属性12345678fdfs: so-timeout: 1501 connect-timeout: 601 thumb-image: # 缩略图 width: 60 height: 60 tracker-list: # tracker地址 - FastDFS主机IP:22122 测试12345678910111213141516171819202122232425262728293031323334353637@RunWith(SpringRunner.class)@SpringBootTest(classes = UploadService.class)public class FdfsTest &#123; @Autowired private FastFileStorageClient storageClient; @Autowired private ThumbImageConfig thumbImageConfig; @Test public void testUpload() throws FileNotFoundException &#123; File file = new File("D:\\test\\baby.png"); // 上传并且生成缩略图 StorePath storePath = this.storageClient.uploadFile( new FileInputStream(file), file.length(), "png", null); // 带分组的路径 System.out.println(storePath.getFullPath()); // 不带分组的路径 System.out.println(storePath.getPath()); &#125; @Test public void testUploadAndCreateThumb() throws FileNotFoundException &#123; File file = new File("D:\\test\\baby.png"); // 上传并且生成缩略图 StorePath storePath = this.storageClient.uploadImageAndCrtThumbImage( new FileInputStream(file), file.length(), "png", null); // 带分组的路径 System.out.println(storePath.getFullPath()); // 不带分组的路径 System.out.println(storePath.getPath()); // 获取缩略图路径 String path = thumbImageConfig.getThumbImagePath(storePath.getPath()); System.out.println(path); &#125;&#125;]]></content>
      <categories>
        <category>FastDFS</category>
      </categories>
      <tags>
        <tag>FastDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastDFS单机版安装]]></title>
    <url>%2F2019%2F05%2F23%2FFastDFS%2FFastDFS%E5%8D%95%E6%9C%BA%E7%89%88%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[配置镜像加速器:修改daemon配置文件/etc/docker/daemon.json12sudo mkdir -p /etc/dockersudo vi /etc/docker/daemon.json 在key为registry-mirrors追加&quot;https://1031vuk0.mirror.aliyuncs.com&quot; 1234# 若没有直接cv大法&#123; "registry-mirrors": ["https://1031vuk0.mirror.aliyuncs.com"]&#125; 重启docker刷新配置12sudo systemctl daemon-reloadsudo systemctl restart docker 编写docker-compose.yml123456789version: '3.1'services: fastdfs: image: registry.cn-shenzhen.aliyuncs.com/dev_docker_resp/fastdfs restart: always container_name: fastdfs volumes: - ./storage:/fastdfs/storage network_mode: host 运行docker-compose up -d]]></content>
      <categories>
        <category>Docker Compose</category>
        <category>FastDFS</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>FastDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos下安装FastDFS]]></title>
    <url>%2F2019%2F05%2F23%2FFastDFS%2FCentos%E4%B8%8B%E5%AE%89%E8%A3%85FastDFS%2F</url>
    <content type="text"><![CDATA[Centos下安装FastDFS安装依赖FastDFS运行需要一些依赖，在课前资料提供的虚拟中已经安装好了这些依赖，如果大家想要从头学习，可以按下面方式安装： 安装GCC依赖GCC用来对C语言代码进行编译运行，使用yum命令安装： 1sudo yum -y install gcc 安装unzip工具unzip工具可以帮我们对压缩包进行解压 1sudo yum install -y unzip zip 安装libevent1sudo yum -y install libevent 安装Nginx所需依赖1sudo yum -y install pcre pcre-devel zlib zlib-devel openssl openssl-devel 安装libfastcommon-master这个没有yum包，只能通过编译安装： 解压刚刚上传的libfastcommon-master.zip 1unzip libfastcommon-master.zip 进入解压完成的目录： 1cd libfastcommon-master 编译并且安装： 12sudo ./make.sh sudo ./make.sh install 到这里为止，所有依赖都已经安装完毕，接下来我们安装FastDFS： 安装FastDFS编译安装这里我们也采用编译安装，步骤与刚才的编译安装方式一样： 解压 1tar -xvf FastDFS_v5.08.tar.gz 进入目录 1cd FastDFS 编译并安装 12sudo ./make.sh sudo ./make.sh install 校验安装结果 1）安装完成，我们应该能在/etc/init.d/目录，通过命令ll /etc/init.d/ | grep fdfs看到FastDFS提供的启动脚本： 其中： fdfs_trackerd 是tracker启动脚本 fdfs_storaged 是storage启动脚本 2）我们可以在 /etc/fdfs目录，通过命令查看到以下配置文件模板： 其中： tarcker.conf.sample 是tracker的配置文件模板 storage.conf.sample 是storage的配置文件模板 client.conf.sample 是客户端的配置文件模板 启动trackercdFastDFS的tracker和storage在刚刚的安装过程中，都已经被安装了，因此我们安装这两种角色的方式是一样的。不同的是，两种需要不同的配置文件。 我们要启动tracker，就修改刚刚看到的tarcker.conf，并且启动fdfs_trackerd脚本即可。 编辑tracker配置 首先我们将模板文件进行赋值和重命名： 12sudo cp tracker.conf.sample tracker.confsudo vim tracker.conf 打开tracker.conf，修改base_path配置： 1base_path=/leyou/fdfs/tracker # tracker的数据和日志存放目录 创建目录 刚刚配置的目录可能不存在，我们创建出来 1sudo mkdir -p /leyou/fdfs/tracker 启动tracker 我们可以使用 sh /etc/init.d/fdfs_trackerd 启动，不过安装过程中，fdfs已经被设置为系统服务，我们可以采用熟悉的服务启动方式： 1sudo service fdfs_trackerd start # 启动fdfs_trackerd服务，停止用stop 另外，我们可以通过以下命令，设置tracker开机启动： 1sudo chkconfig fdfs_trackerd on 启动storage我们要启动tracker，就修改刚刚看到的tarcker.conf，并且启动fdfs_trackerd脚本即可。 编辑storage配置 首先我们将模板文件进行赋值和重命名： 12sudo cp storage.conf.sample storage.confsudo vim storage.conf 打开storage.conf，修改base_path配置： 123base_path=/leyou/fdfs/storage # storage的数据和日志存放目录store_path0=/leyou/fdfs/storage # storage的上传文件存放路径tracker_server=192.168.56.101:22122 # tracker的地址 创建目录 刚刚配置的目录可能不存在，我们创建出来 1sudo mkdir -p /leyou/fdfs/storage 启动storage 我们可以使用 sh /etc/init.d/fdfs_storaged 启动，同样我们可以用服务启动方式： 1sudo service fdfs_storaged start # 启动fdfs_storaged服务，停止用stop 另外，我们可以通过以下命令，设置tracker开机启动： 1sudo chkconfig fdfs_storaged on 最后，通过ps -ef | grep fdfs 查看进程： 安装Nginx及FastDFS模块FastDFS的Nginx模块 解压 1tar -xvf fastdfs-nginx-module_v1.16.tar.gz 配置config文件 123456# 进入配置目录cd /home/leyou/fdfs/fastdfs-nginx-module/src/# 修改配置vim config# 执行下面命令（将配置中的/usr/local改为/usr）：:%s+/usr/local/+/usr/+g 配置mod_fastdfs.conf 1234# 将src目录下的mod_fastdfs.conf复制到 /etc/fdfs目录：sudo cp mod_fastdfs.conf /etc/fdfs/# 编辑该文件sudo vim /etc/fdfs/mod_fastdfs.conf 修改一下配置： 1234connect_timeout=10 # 客户端访问文件连接超时时长（单位：秒）tracker_server=192.168.56.101:22122 # tracker服务IP和端口url_have_group_name=true # 访问链接前缀加上组名store_path0=/leyou/fdfs/storage # 文件存储路径 复制 FastDFS的部分配置文件到/etc/fdfs目录 12cd /home/leyou/fdfs/FastDFS/conf/cp http.conf mime.types /etc/fdfs/ 安装Nginx 解压 1tar -xvf nginx-1.10.0.tar.gz 配置 1sudo ./configure --prefix=/opt/nginx --sbin-path=/usr/bin/nginx --add-module=/home/leyou/fdfs/fastdfs-nginx-module/src 编译安装 1sudo make &amp;&amp; sudo make install 配置nginx整合fastdfs-module模块 我们需要修改nginx配置文件，在/opt/nginx/config/nginx.conf文件中： 1sudo vim /opt/nginx/conf/nginx.conf 将文件中，原来的server 80{ ...} 部分代码替换为如下代码： 12345678910111213141516171819server &#123; listen 80; server_name image.leyou.com; # 监听域名中带有group的，交给FastDFS模块处理 location ~/group([0-9])/ &#123; ngx_fastdfs_module; &#125; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 启动 123nginx # 启动nginx -s stop # 停止nginx -s reload # 重新加载配置 设置nginx开机启动 创建一个开机启动的脚本： 1vim /etc/init.d/nginx 添加以下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15# description: NGINX is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ "$NETWORKING" = "no" ] &amp;&amp; exit 0nginx="/usr/bin/nginx"prog=$(basename $nginx)NGINX_CONF_FILE="/opt/nginx/conf/nginx.conf"[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginxlockfile=/var/lock/subsys/nginxmake_dirs() &#123; # make required directories user=`$nginx -V 2&gt;&amp;1 | grep "configure arguments:.*--user=" | sed 's/[^*]*--user=\([^ ]*\).*/\1/g' -` if [ -n "$user" ]; then if [ -z "`grep $user /etc/passwd`" ]; then useradd -M -s /bin/nologin $user fi options=`$nginx -V 2&gt;&amp;1 | grep 'configure arguments:'` for opt in $options; do if [ `echo $opt | grep '.*-temp-path'` ]; then value=`echo $opt | cut -d "=" -f 2` if [ ! -d "$value" ]; then # echo "creating" $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done fi&#125;start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $"Starting $prog: " daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125;stop() &#123; echo -n $"Stopping $prog: " killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125;restart() &#123; configtest || return $? stop sleep 1 start&#125;reload() &#123; configtest || return $? echo -n $"Reloading $prog: " killproc $nginx -HUP RETVAL=$? echo&#125;force_reload() &#123; restart&#125;configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125;rh_status() &#123; status $prog&#125;rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125;case "$1" in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $"Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;" exit 2esac 修改文件权限，并加入服务列表 1234# 修改权限chmod 777 /etc/init.d/nginx # 添加到服务列表chkconfig --add /etc/init.d/nginx 设置开机启动 1systemctl enable nginx]]></content>
      <categories>
        <category>FastDFS</category>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>FastDFS</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch的Head插件]]></title>
    <url>%2F2019%2F05%2F23%2FElasticsearch%2FElasticSearch%E7%9A%84Head%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Head是elasticsearch的集群管理工具，可以用于数据的浏览和查询，elasticsearch-head是一款开源软件，被托管在github上面 源码安装下载插件123git clone https://github.com/mobz/elasticsearch-head.gitcd /usr/local/elasticsearch-head 安装elasticsearch-head依赖包 1npm install -g grunt-cli 安装 123npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install 启动1npm run start docker安装编写docker-compose.yml 12345678910version: &apos;3&apos;services: elasticsearch-head: restart: always image: mobz/elasticsearch-head container_name: elasticsearch-head environment: TZ: &apos;Asia/Shanghai&apos; ports: - &apos;9100:9100&apos;]]></content>
      <categories>
        <category>Elasticsearch head</category>
      </categories>
      <tags>
        <tag>Elasticsearch head</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Compose安装ElasticSearch单机版]]></title>
    <url>%2F2019%2F05%2F23%2FElasticsearch%2FDocker-Compose%E5%AE%89%E8%A3%85ElasticSearch%E5%8D%95%E6%9C%BA%E7%89%88%2F</url>
    <content type="text"><![CDATA[系统调优 修改/etc/security/limits.conf ，追加内容 12* soft nofile 65536* hard nofile 65536 nofile是单个进程允许打开的最大文件个数 soft nofile 是软限制 hard nofile是硬限制 修改/etc/sysctl.conf，追加内容 1vm.max_map_count=655360 限制一个进程可以拥有的VMA(虚拟内存区域)的数量 执行下面命令 修改内核参数马上生效 1sysctl -p 编写docker-compose.yml文件 1234567891011121314151617181920212223242526version: '3'services: elasticsearch: restart: always image: elasticsearch:5.6.13 container_name: elasticsearch environment: - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms512m -Xmx512m" ulimits: memlock: soft: -1 hard: -1 volumes: - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./plugins:/usr/share/elasticsearch/plugins - ./data:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 networks: - esnetvolumes: data:networks: esnet: 编写elasticsearch.yml 123456789# 5.6.13 版本http.host: 0.0.0.0# Uncomment the following lines for a production cluster deploymenttransport.host: 0.0.0.0#discovery.zen.minimum_master_nodes: 1http.cors.enabled: truehttp.cors.allow-origin: "*" 123456789# 6.5.3 版本network.bind_host: 0.0.0.0cluster.name: elastic-clusternetwork.host: 0.0.0.0# discovery.zen.minimum_master_nodes: 1# bootstrap.memory_lock: truediscovery.type: single-nodehttp.cors.enabled: truehttp.cors.allow-origin: "*" 启动容器docker-compose up -d]]></content>
      <categories>
        <category>Docker Compose</category>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch快速入门]]></title>
    <url>%2F2019%2F05%2F23%2FElasticsearch%2FElasticsearch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Elasticsearch介绍和安装简介ElasticElastic官网：https://www.elastic.co/cn/ ) Elastic有一条完整的产品线：Elasticsearch、Kibana、Logstash等，前面说的三个就是大家常说的ELK技术栈。 ElasticsearchElasticsearch官网：https://www.elastic.co/cn/products/elasticsearch 如上所述，Elasticsearch具备以下特点： 分布式，无需人工搭建集群（solr就需要人为配置，使用Zookeeper作为注册中心） Restful风格，一切API都遵循Rest原则，容易上手 近实时搜索，数据更新在Elasticsearch中几乎是完全同步的。 版本目前Elasticsearch最新的版本是6.5.4，我们就使用这个版本 需要虚拟机JDK1.8及以上 CentOS7安装和配置新建一个用户1useradd esearch 设置密码： 1passwd esearch 出于安全考虑，elasticsearch默认不允许以root账号运行。 切换用户： 1su - esearch 上传安装包,并解压我们将安装包上传到：/home/esearch目录 解压缩： 1tar xvf elasticsearch-6.5.4.tar.gz 修改文件拥有者与所属组 1chown esearch:esearch elasticsearch-6.5.4 -R 我们把目录重命名： 1mv elasticsearch-6.5.4 elasticsearch 进入，查看目录结构： 修改配置我们进入config目录：cd config 需要修改的配置文件有两个： 修改jvm配置 Elasticsearch基于Lucene的，而Lucene底层是java实现，因此我们需要配置jvm参数 1vim jvm.options 默认配置如下： 12-Xms1g-Xmx1g 内存占用太多了，我们调小一些： 12-Xms512m-Xmx512m 修改elasticsearch.yml 1vim elasticsearch.yml 修改数据和日志目录： 12path.data: /home/esearch/elasticsearch/data # 数据目录位置path.logs: /home/esearch/elasticsearch/logs # 日志目录位置 修改绑定的ip： 1network.host: 0.0.0.0 # 绑定到0.0.0.0，允许任何ip来访问 默认只允许本机访问，修改为0.0.0.0后则可以远程访问 目前我们是做的单机安装，如果要做集群，只需要在这个配置文件中添加其它节点信息即可。 elasticsearch.yml的其它可配置信息： 属性名 说明 cluster.name 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称。 node.name 节点名，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理 path.conf 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/ elasticsearch path.data 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开 path.logs 设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins 设置插件的存放路径，默认是es根目录下的plugins文件夹 bootstrap.memory_lock 设置为true可以锁住ES使用的内存，避免内存进行swap network.host 设置bind_host和publish_host，设置为0.0.0.0允许外网访问 http.port 设置对外服务的http端口，默认为9200。 transport.tcp.port 集群结点之间通信端口 discovery.zen.ping.timeout 设置ES自动发现节点连接超时的时间，默认为3秒，如果网络延迟高可设置大些 discovery.zen.minimum_master_nodes 主结点数量的最少值 ,此值的公式为：(master_eligible_nodes / 2) + 1 ，比如：有3个符合要求的主结点，那么这里要设置为2 创建data和logs目录刚才我们修改配置，把data和logs目录修改指向了elasticsearch的安装目录。但是这两个目录并不存在，因此我们需要创建出来： 进入elasticsearch的根目录，然后创建： 12mkdir datamkdir logs 运行进入elasticsearch/bin目录，可以看到下面的执行文件： 然后输入命令： 1./elasticsearch 发现报错了，启动失败. 错误1:1[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536] 我们用的是esearch用户，而不是root，所以文件权限不足。 首先用root用户登录。 然后修改配置文件: 1vim /etc/security/limits.conf 添加下面的内容： 1234567* soft nofile 65536* hard nofile 131072* soft nproc 4096* hard nproc 4096 错误2：1[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 继续修改配置文件： 1vim /etc/sysctl.conf 添加下面内容： 1vm.max_map_count=655360 然后执行命令： 1sysctl -p 重启终端窗口所有错误修改完毕，一定要重启你的 Xshell终端，否则配置无效。 启动再次启动，终于成功了！ 可以看到绑定了两个端口: 9300：集群节点间通讯接口 9200：客户端访问接口 我们在浏览器中访问(由于博主使用的是云服务器，访问需要使用公网IP访问):http://119.23.208.179:9200 安装kibana什么是Kibana？ Kibana是一个基于Node.js的Elasticsearch索引库数据统计工具，可以利用Elasticsearch的聚合功能，生成各种图表，如柱形图，线状图，饼图等。 而且还提供了操作Elasticsearch索引数据的控制台，并且提供了一定的API提示，非常有利于我们学习Elasticsearch的语法。 安装因为Kibana依赖于node，我的虚拟机没有安装node，而window中安装过。所以我们选择在window下使用kibana。 最新版本与elasticsearch保持一致，也是6.5.4 解压即可： 配置运行 配置 进入安装目录下的config目录，修改kibana.yml文件： 修改elasticsearch服务器的地址： 1elasticsearch.url: &quot;http://119.23.208.179:9200&quot; 运行 进入安装目录下的bin目录： 双击运行kibana.bat： 发现kibana的监听端口是5601 我们访问：http://127.0.0.1:5601 控制台选择左侧的DevTools菜单，即可进入控制台页面： 在页面右侧，我们就可以输入请求，访问Elasticsearch了。 安装ik分词器Lucene的IK分词器早在2012年已经没有维护了，现在我们要使用的是在其基础上维护升级的版本，并且开发为Elasticsearch的集成插件了，与Elasticsearch一起维护升级，版本也保持一致，最新版本：6.5.4 安装下载并上传elasticsearch-analysis-ik-6.5.4.zip包，在/home/esearch/elasticsearch/plugins中，将elasticsearch-analysis-ik-6.5.4.zip解压到ik-analyzer目录中： 使用unzip命令解压： 1unzip elasticsearch-analysis-ik-6.5.4.zip -d ik-analyzer 删除elasticsearch-analysis-ik-6.5.4.zip,然后重启elasticsearch： 测试大家先不管语法，我们先测试一波。 在kibana控制台输入下面的请求： 12345POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;我是中国人&quot;&#125; 运行得到结果： 123456789101112131415161718192021222324252627282930313233343536373839&#123; "tokens" : [ &#123; "token" : "我", "start_offset" : 0, "end_offset" : 1, "type" : "CN_CHAR", "position" : 0 &#125;, &#123; "token" : "是", "start_offset" : 1, "end_offset" : 2, "type" : "CN_CHAR", "position" : 1 &#125;, &#123; "token" : "中国人", "start_offset" : 2, "end_offset" : 5, "type" : "CN_WORD", "position" : 2 &#125;, &#123; "token" : "中国", "start_offset" : 2, "end_offset" : 4, "type" : "CN_WORD", "position" : 3 &#125;, &#123; "token" : "国人", "start_offset" : 3, "end_offset" : 5, "type" : "CN_WORD", "position" : 4 &#125; ]&#125; APIElasticsearch提供了Rest风格的API，即http请求接口，而且也提供了各种语言的客户端API Rest风格API文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html 客户端APIElasticsearch支持的客户端非常多：https://www.elastic.co/guide/en/elasticsearch/client/index.html 点击Java Rest Client后，你会发现又有两个： Low Level Rest Client是低级别封装，提供一些基础功能，但更灵活 High Level Rest Client，是在Low Level Rest Client基础上进行的高级别封装，功能更丰富和完善，而且API会变的简单 如何学习建议先学习Rest风格API，了解发起请求的底层实现，请求体格式等。 操作索引基本概念Elasticsearch也是基于Lucene的全文检索库，本质也是存储数据，很多概念与MySQL类似的。 对比关系： 索引（indices）——————————–Databases 数据库 ​ 类型（type）—————————–Table 数据表 ​ 文档（Document）—————-Row 行 ​ 字段（Field）——————-Columns 列 详细说明： 概念 说明 索引库（indices) indices是index的复数，代表许多的索引， 类型（type） 类型是模拟mysql中的table概念，一个索引库下可以有不同类型的索引，比如商品索引，订单索引，其数据格式不同。不过这会导致索引库混乱，因此未来版本中会移除这个概念 文档（document） 存入索引库原始的数据。比如每一条商品信息，就是一个文档 字段（field） 文档中的属性 映射配置（mappings） 字段的数据类型、属性、是否索引、是否存储等特性 是不是与Lucene和solr中的概念类似。 另外，在SolrCloud中，有一些集群相关的概念，在Elasticsearch也有类似的： 索引集（Indices，index的复数）：逻辑上的完整索引 分片（shard）：数据拆分后的各个部分 副本（replica）：每个分片的复制 要注意的是：Elasticsearch本身就是分布式的，因此即便你只有一个节点，Elasticsearch默认也会对你的数据进行分片和副本操作，当你向集群添加新数据时，数据也会在新加入的节点中进行平衡。 创建索引语法Elasticsearch采用Rest风格API，因此其API就是一次http请求，你可以用任何工具发起http请求 创建索引的请求格式： 请求方式：PUT 请求路径：/索引库名 请求参数：json格式： 123456&#123; "settings": &#123; "number_of_shards": 3, "number_of_replicas": 2 &#125;&#125; settings：索引库的设置 number_of_shards：分片数量 number_of_replicas：副本数量 测试我们先用Insomnia来试试 响应： 可以看到索引创建成功了。 使用kibana创建kibana的控制台，可以对http请求进行简化，示例： 相当于是省去了elasticsearch的服务器地址 而且还有语法提示，非常舒服。 查看索引设置 语法 Get请求可以帮我们查看索引信息，格式： 1GET /索引库名 或者，我们可以使用*来查询所有索引库配置： 删除索引删除索引使用DELETE请求 语法 1DELETE /索引库名 示例 再次查看heima2： 当然，我们也可以用HEAD请求，查看索引是否存在： 映射配置索引有了，接下来肯定是添加数据。不过数据存储到索引库中，必须指定一些相关属性，在学习Lucene中我们都见到过，包括到不限于： 字段的数据类型 是否要存储 是否要索引 是否分词 分词器是什么 只有配置清楚，Elasticsearch才会帮我们进行索引库的创建（不一定） 创建映射字段 语法 请求方式依然是PUT 1234567891011PUT /索引库名/_mapping/类型名称&#123; &quot;properties&quot;: &#123; &quot;字段名&quot;: &#123; &quot;type&quot;: &quot;类型&quot;, &quot;index&quot;: true， &quot;store&quot;: true， &quot;analyzer&quot;: &quot;分词器&quot; &#125; &#125;&#125; 类型名称：就是前面将的type的概念，类似于数据库中的不同表字段名：任意填写 ，可以指定许多属性，例如： type：类型，可以是text、long、short、date、integer、object等 index：是否索引，默认为true store：是否存储，默认为false analyzer：分词器，这里的ik_max_word即使用ik分词器 示例 发起请求： 12345678910111213141516PUT test/_mapping/goods&#123; "properties": &#123; "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125;, "images": &#123; "type": "keyword", "index": "false" &#125;, "price": &#123; "type": "float" &#125; &#125;&#125; 响应结果： 123&#123; "acknowledged" : true&#125; 查看映射关系 语法： 1GET /索引库名/_mapping 示例： 1GET /heima/_mapping 响应： 123456789101112131415161718192021&#123; "test" : &#123; "mappings" : &#123; "goods" : &#123; "properties" : &#123; "images" : &#123; "type" : "keyword", "index" : false &#125;, "price" : &#123; "type" : "float" &#125;, "title" : &#123; "type" : "text", "analyzer" : "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; 字段属性详解 type Elasticsearch中支持的数据类型非常丰富： 我们说几个关键的： String类型，又分两种： text：可分词，不可参与聚合 keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合 Numerical：数值类型，分两类 基本数据类型：long、interger、short、byte、double、float、half_float 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 elasticsearch可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为long，节省空间。 index index影响字段的索引情况。 true：字段会被索引，则可以用来进行搜索。默认值就是true false：字段不会被索引，不能用来搜索 index的默认值就是true，也就是说你不进行任何配置，所有字段都会被索引。 但是有些字段是我们不希望被索引的，比如商品的图片信息，就需要手动设置index为false。 store 是否将数据进行额外存储。 在学习lucene和solr时，我们知道如果一个字段的store设置为false，那么在文档列表中就不会有这个字段的值，用户的搜索结果中不会显示出来。 但是在Elasticsearch中，即便store设置为false，也可以搜索到结果。 原因是Elasticsearch在创建文档索引时，会将文档中的原始数据备份，保存到一个叫做_source的属性中。而且我们可以通过过滤_source来选择哪些要显示，哪些不显示。 而如果设置store为true，就会在_source以外额外存储一份数据，多余，因此一般我们都会将store设置为false，事实上，store的默认值就是false。 boost 激励因子，这个与lucene中一样 其它的不再一一讲解，用的不多，大家参考官方文档： 新增数据随机生成id通过POST请求，可以向一个已经存在的索引库中添加数据。 语法： 1234POST /索引库名/类型名&#123; &quot;key&quot;:&quot;value&quot;&#125; 示例： 123456POST /test/goods/&#123; "title":"小米手机", "images":"https://i1.mifile.cn/f/i/g/2015/cn-index/note7320-220.png", "price":3299.00&#125; 响应： 1234567891011121314&#123; "_index" : "test", "_type" : "goods", "_id" : "iDZznGgB-wYBfzEzkJGx", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 3, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125; 通过kibana查看数据： 123456get _search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; 1234567891011&#123; "_index" : "test", "_type" : "goods", "_id" : "iDZznGgB-wYBfzEzkJGx", "_score" : 1.0, "_source" : &#123; "title" : "小米手机", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/note7320-220.png", "price" : 3299.0 &#125;&#125; _source：源文档信息，所有的数据都在里面。 _id：这条文档的唯一标示，与文档自己的id字段没有关联 自定义id如果我们想要自己新增的时候指定id，可以这么做： 1234POST /索引库名/类型/id值&#123; ...&#125; 示例： 123456POST /test/goods/2&#123; &quot;title&quot;:&quot;大米手机&quot;, &quot;images&quot;:&quot;http://image.codeopen.club/12479122.jpg&quot;, &quot;price&quot;:2899.00&#125; 响应结果: 1234567891011121314&#123; "_index" : "test", "_type" : "goods", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 3, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125; 智能判断在学习Solr时我们发现，我们在新增数据时，只能使用提前配置好映射属性的字段，否则就会报错。 不过在Elasticsearch中并没有这样的规定。 事实上Elasticsearch非常智能，你不需要给索引库设置任何mapping映射，它也可以根据你输入的数据来判断类型，动态添加数据映射。 测试一下： 12345678POST /test/goods/3&#123; "title":"超米手机", "images":"http://image.codeopen.club/12479122.jpg", "price":2899.00, "stock": 200, "saleable":true&#125; 我们额外添加了stock库存，和saleable是否上架两个字段。 来看结果： 1234567891011121314&#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 3, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 在看下索引库的映射关系GET test/_mapping: 123456789101112131415161718192021222324252627&#123; "test" : &#123; "mappings" : &#123; "goods" : &#123; "properties" : &#123; "images" : &#123; "type" : "keyword", "index" : false &#125;, "price" : &#123; "type" : "float" &#125;, "saleable" : &#123; "type" : "boolean" &#125;, "stock" : &#123; "type" : "long" &#125;, "title" : &#123; "type" : "text", "analyzer" : "ik_max_word" &#125; &#125; &#125; &#125; &#125;&#125; stock和saleable都被成功映射了。 修改数据把刚才新增的请求方式改为PUT，就是修改了。不过修改必须指定id， id对应文档存在，则修改 id对应文档不存在，则新增 比如，我们把id为3的数据进行修改： 12345678PUT /test/goods/3&#123; "title":"超大米手机", "images":"http://image.codeopen.club/12479122.jpg", "price":4899.00, "stock": 300, "saleable":true&#125; 结果： 1234567891011121314&#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 3, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1&#125; 删除数据删除使用DELETE请求，同样，需要根据id进行删除： 语法 1DELETE /索引库名/类型名/id值 示例： 查询我们从4块来讲查询： 基本查询 _source过滤 结果过滤 高级查询 排序 基本查询： 基本语法 12345678GET /索引库名/_search&#123; "query":&#123; "查询类型":&#123; "查询条件":"查询条件值" &#125; &#125;&#125; 这里的query代表一个查询对象，里面可以有不同的查询属性 查询类型： 例如：match_all， match，term ， range 等等 查询条件：查询条件会根据类型的不同，写法也有差异，后面详细讲解 查询所有（match_all) 示例： 123456GET /test/_search&#123; "query":&#123; "match_all": &#123;&#125; &#125;&#125; query：代表查询对象 match_all：代表查询所有 结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took" : 1, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 1.0, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "2", "_score" : 1.0, "_source" : &#123; "title" : "大米手机", "images" : "http://image.codeopen.club/12479122.jpg", "price" : 2899.0 &#125; &#125;, &#123; "_index" : "test", "_type" : "goods", "_id" : "iDZznGgB-wYBfzEzkJGx", "_score" : 1.0, "_source" : &#123; "title" : "小米手机", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/note7320-220.png", "price" : 3299.0 &#125; &#125; ] &#125;&#125; took：查询花费时间，单位是毫秒 time_out：是否超时 _shards：分片信息 hits：搜索结果总览对象 total：搜索到的总条数 max_score：所有结果中文档得分的最高分 hits：搜索结果的文档对象数组，每个元素是一条搜索到的文档信息 _index：索引库 _type：文档类型 _id：文档id _score：文档得分 _source：文档的源数据 匹配查询（match）我们先加入一条数据，便于测试： 123456PUT /test/goods/3&#123; "title":"小米电视4S", "images":"https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png", "price":7999.00&#125; 现在，索引库中有2部手机，1台电视： or关系 match类型查询，会把查询条件进行分词，然后进行查询,多个词条之间是or的关系 12345678GET /test/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;小米电视&quot; &#125; &#125;&#125; 结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took" : 10, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.74487394, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_score" : 0.74487394, "_source" : &#123; "title" : "小米电视4S", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png", "price" : 7999.0 &#125; &#125;, &#123; "_index" : "test", "_type" : "goods", "_id" : "iDZznGgB-wYBfzEzkJGx", "_score" : 0.22108285, "_source" : &#123; "title" : "小米手机", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/note7320-220.png", "price" : 3299.0 &#125; &#125; ] &#125;&#125; 在上面的案例中，不仅会查询到电视，而且与小米相关的都会查询到，多个词之间是or的关系。 and关系 某些情况下，我们需要更精确查找，我们希望这个关系变成and，可以这样做： 12345678GET /test/_search&#123; "query":&#123; "match":&#123; "title":&#123;"query":"小米电视","operator":"and"&#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took" : 20, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.74487394, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_score" : 0.74487394, "_source" : &#123; "title" : "小米电视4S", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png", "price" : 7999.0 &#125; &#125; ] &#125;&#125; 本例中，只有同时包含小米和电视的词条才会被搜索到。 or和and之间？ 在 or 与 and 间二选一有点过于非黑即白。 如果用户给定的条件分词后有 5 个查询词项，想查找只包含其中 4 个词的文档，该如何处理？将 operator 操作符参数设置成 and 只会将此文档排除。 有时候这正是我们期望的，但在全文搜索的大多数应用场景下，我们既想包含那些可能相关的文档，同时又排除那些不太相关的。换句话说，我们想要处于中间某种结果。 match 查询支持 minimum_should_match 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量： 1234567891011GET /test/_search&#123; "query":&#123; "match":&#123; "title":&#123; "query":"小米曲面电视", "minimum_should_match": "75%" &#125; &#125; &#125;&#125; 本例中，搜索语句可以分为3个词，如果使用and关系，需要同时满足3个词才会被搜索到。这里我们采用最小品牌数：75%，那么也就是说只要匹配到总词条数量的75%即可，这里3*75% 约等于2。所以只要包含2个词条就算满足条件了。 结果： 123456789101112131415161718192021222324252627&#123; "took" : 4, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.74487394, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_score" : 0.74487394, "_source" : &#123; "title" : "小米电视4S", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png", "price" : 7999.0 &#125; &#125; ] &#125;&#125; 多字段查询（multi_match）multi_match与match类似，不同的是它可以在多个字段中查询 123456789GET /test/_search&#123; "query":&#123; "multi_match": &#123; "query": "小米", "fields": [ "title", "subTitle" ] &#125; &#125;&#125; 本例中，我们会在title字段和subtitle字段中查询小米这个词 结果： 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took" : 4, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.22108285, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "iDZznGgB-wYBfzEzkJGx", "_score" : 0.22108285, "_source" : &#123; "title" : "小米手机", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/note7320-220.png", "price" : 3299.0 &#125; &#125;, &#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_score" : 0.15512443, "_source" : &#123; "title" : "小米电视4S", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png", "price" : 7999.0 &#125; &#125; ] &#125;&#125; 词条匹配(term)term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些未分词的字 12345678GET /test/_search&#123; "query":&#123; "term":&#123; "price":7999.00 &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took" : 1, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 1.0, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "3", "_score" : 1.0, "_source" : &#123; "title" : "小米电视4S", "images" : "https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png", "price" : 7999.0 &#125; &#125; ] &#125;&#125; 多词条精确匹配(terms)terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件： 12345678GET /test/_search&#123; "query":&#123; "terms":&#123; "price":[2899.00,3299.00,7999.00] &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 3, &quot;successful&quot; : 3, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 3, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;goods&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;大米手机&quot;, &quot;images&quot; : &quot;http://image.codeopen.club/12479122.jpg&quot;, &quot;price&quot; : 2899.0 &#125; &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;goods&quot;, &quot;_id&quot; : &quot;iDZznGgB-wYBfzEzkJGx&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米手机&quot;, &quot;images&quot; : &quot;https://i1.mifile.cn/f/i/g/2015/cn-index/note7320-220.png&quot;, &quot;price&quot; : 3299.0 &#125; &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;goods&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米电视4S&quot;, &quot;images&quot; : &quot;https://i1.mifile.cn/f/i/g/2015/cn-index/4s75.png&quot;, &quot;price&quot; : 7999.0 &#125; &#125; ] &#125;&#125; 结果过滤默认情况下，elasticsearch在搜索的结果中，会把文档中保存在_source的所有字段都返回。 如果我们只想获取其中的部分字段，我们可以添加_source的过滤 直接指定字段示例： 123456789GET /test/_search&#123; "_source": ["title","price"], "query": &#123; "term": &#123; "price": 2899 &#125; &#125;&#125; 返回的结果： 1234567891011121314151617181920212223242526&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 1.0, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "2", "_score" : 1.0, "_source" : &#123; "price" : 2899.0, "title" : "大米手机" &#125; &#125; ] &#125;&#125; 指定includes和excludes我们也可以通过： includes：来指定想要显示的字段 excludes：来指定不想要显示的字段 二者都是可选的。 示例： 1234567891011GET /test/_search&#123; "_source": &#123; "includes":["title","price"] &#125;, "query": &#123; "term": &#123; "price": 2899 &#125; &#125;&#125; 与下面的结果将是一样的： 1234567891011GET /test/_search&#123; "_source": &#123; "excludes": ["images"] &#125;, "query": &#123; "term": &#123; "price": 2899 &#125; &#125;&#125; 高级查询布尔组合（bool)bool把各种其它查询通过must（与）、must_not（非）、should（或）的方式进行组合 12345678910GET /test/_search&#123; "query":&#123; "bool":&#123; "must": &#123; "match": &#123; "title": "大米" &#125;&#125;, "must_not": &#123; "match": &#123; "title": "电视" &#125;&#125;, "should": &#123; "match": &#123; "title": "手机" &#125;&#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.5753642, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "2", "_score" : 0.5753642, "_source" : &#123; "title" : "大米手机", "images" : "http://image.codeopen.club/12479122.jpg", "price" : 2899.0 &#125; &#125; ] &#125;&#125; 范围查询(range)range 查询找出那些落在指定区间内的数字或者时间 1234567891011GET /test/_search&#123; "query":&#123; "range": &#123; "price": &#123; "gte": 1000.0, "lt": 3000.00 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 1.0, "hits" : [ &#123; "_index" : "test", "_type" : "goods", "_id" : "2", "_score" : 1.0, "_source" : &#123; "title" : "大米手机", "images" : "http://image.codeopen.club/12479122.jpg", "price" : 2899.0 &#125; &#125; ] &#125;&#125; range查询允许以下字符： 操作符 说明 gt 大于 gte 大于等于 lt 小于 lte 小于等于 模糊查询(fuzzy)我们新增一个商品： 123456POST /test/goods/4&#123; "title":"apple手机", "images":"https://img.pconline.com.cn/images/product/5700/570024/iPhone6plus.jpg", "price":6899.00&#125; fuzzy 查询是 term 查询的模糊等价。它允许用户搜索词条与实际词条的拼写出现偏差，但是偏差的编辑距离不得超过2： 12345678GET /test/_search&#123; "query": &#123; "fuzzy": &#123; "title": "appla" &#125; &#125;&#125; 上面的查询，也能查询到apple手机 我们可以通过fuzziness来指定允许的编辑距离： 1234567891011GET /test/_search&#123; "query": &#123; "fuzzy": &#123; "title": &#123; "value":"appla", "fuzziness":1 &#125; &#125; &#125;&#125; 过滤(filter) 条件查询中进行过滤 所有的查询都会影响到文档的评分及排名。如果我们需要在查询结果中进行过滤，并且不希望过滤条件影响评分，那么就不要把过滤条件作为查询条件来用。而是使用filter方式： 1234567891011GET /test/_search&#123; "query":&#123; "bool":&#123; "must":&#123; "match": &#123; "title": "小米手机" &#125;&#125;, "filter":&#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3800.00&#125;&#125; &#125; &#125; &#125;&#125; 注意：filter中还可以再次进行bool组合条件过滤。 无查询条件，直接过滤 如果一次查询只有过滤，没有查询条件，不希望进行评分，我们可以使用constant_score取代只有 filter 语句的 bool 查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。 123456789GET /test/_search&#123; "query":&#123; "constant_score": &#123; "filter": &#123; "range":&#123;"price":&#123;"gt":2000.00,"lt":3000.00&#125;&#125; &#125; &#125;&#125; 排序单字段排序sort 可以让我们按照不同的字段进行排序，并且通过order指定排序的方式 123456789101112131415GET /test/_search&#123; "query": &#123; "match": &#123; "title": "小米手机" &#125; &#125;, "sort": [ &#123; "price": &#123; "order": "desc" &#125; &#125; ]&#125; 多字段排序假定我们想要结合使用 price和 _score（得分） 进行查询，并且匹配的结果首先按照价格排序，然后按照相关性得分排序： 123456789101112131415GET /goods/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:&#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;小米手机&quot; &#125;&#125;, &quot;filter&quot;:&#123; &quot;range&quot;:&#123;&quot;price&quot;:&#123;&quot;gt&quot;:200000,&quot;lt&quot;:300000&#125;&#125; &#125; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;price&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125;, &#123; &quot;_score&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125; ]&#125; 聚合aggregations聚合可以让我们极其方便的实现对数据的统计、分析。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？ 实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。 基本概念Elasticsearch中的聚合，包含多种类型，最常用的两种，一个叫桶，一个叫度量： 桶（bucket） 桶的作用，是按照某种方式对数据进行分组，每一组数据在ES中称为一个桶，例如我们根据国籍对人划分，可以得到中国桶、英国桶，日本桶……或者我们按照年龄段对人进行划分：0~10,10~20,20~30,30~40等。 Elasticsearch中提供的划分桶的方式有很多： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 …… 综上所述，我们发现bucket aggregations 只负责对数据进行分组，并不进行计算，因此往往bucket中往往会嵌套另一种聚合：metrics aggregations即度量 度量（metrics） 分组完成以后，我们一般会对组中的数据进行聚合运算，例如求平均值、最大、最小、求和等，这些在ES中称为度量 比较常用的一些度量聚合方式： Avg Aggregation：求平均值 Max Aggregation：求最大值 Min Aggregation：求最小值 Percentiles Aggregation：求百分比 Stats Aggregation：同时返回avg、max、min、sum、count等 Sum Aggregation：求和 Top hits Aggregation：求前几 Value Count Aggregation：求总数 …… 为了测试聚合，我们先批量导入一些数据 创建索引： 12345678910111213141516171819PUT /cars&#123; "settings": &#123; "number_of_shards": 1, "number_of_replicas": 0 &#125;, "mappings": &#123; "transactions": &#123; "properties": &#123; "color": &#123; "type": "keyword" &#125;, "make": &#123; "type": "keyword" &#125; &#125; &#125; &#125;&#125; 注意：在ES中，需要进行聚合、排序、过滤的字段其处理方式比较特殊，因此不能被分词。这里我们将color和make这两个文字类型的字段设置为keyword类型，这个类型不会被分词，将来就可以参与聚合 导入数据 1234567891011121314151617POST /cars/transactions/_bulk&#123; "index": &#123;&#125;&#125;&#123; "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 15000, "color" : "blue", "make" : "toyota", "sold" : "2014-07-02" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 12000, "color" : "green", "make" : "toyota", "sold" : "2014-08-19" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 80000, "color" : "red", "make" : "bmw", "sold" : "2014-01-01" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 25000, "color" : "blue", "make" : "ford", "sold" : "2014-02-12" &#125; 聚合为桶首先，我们按照 汽车的颜色color来划分桶 1234567891011GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125; &#125; &#125;&#125; size： 查询条数，这里设置为0，因为我们不关心搜索到的数据，只关心聚合结果，提高效率 aggs：声明这是一个聚合查询，是aggregations的缩写 popular_colors：给这次聚合起一个名字，任意。 terms：划分桶的方式，这里是根据词条划分 field：划分桶的字段 结果： 1234567891011121314151617181920212223242526272829303132333435&#123; "took" : 8, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "red", "doc_count" : 4 &#125;, &#123; "key" : "blue", "doc_count" : 2 &#125;, &#123; "key" : "green", "doc_count" : 2 &#125; ] &#125; &#125;&#125; hits：查询结果为空，因为我们设置了size为0 aggregations：聚合的结果 popular_colors：我们定义的聚合名称 buckets：查找到的桶，每个不同的color字段值都会形成一个桶 key：这个桶对应的color字段的值 doc_count：这个桶中的文档数量 通过聚合的结果我们发现，目前红色的小车比较畅销！ 桶内度量前面的例子告诉我们每个桶里面的文档数量，这很有用。 但通常，我们的应用需要提供更复杂的文档度量。 例如，每种颜色汽车的平均价格是多少？ 因此，我们需要告诉Elasticsearch使用哪个字段，使用何种度量方式进行运算，这些信息要嵌套在桶内，度量的运算会基于桶内的文档进行 现在，我们为刚刚的聚合结果添加 求价格平均值的度量： 123456789101112131415161718GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125;, "aggs":&#123; "avg_price": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125;&#125; aggs：我们在上一个aggs(popular_colors)中添加新的aggs。可见度量也是一个聚合 avg_price：聚合的名称 avg：度量的类型，这里是求平均值 field：度量运算的字段 结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; "took" : 10, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "red", "doc_count" : 4, "avg_price" : &#123; "value" : 32500.0 &#125; &#125;, &#123; "key" : "blue", "doc_count" : 2, "avg_price" : &#123; "value" : 20000.0 &#125; &#125;, &#123; "key" : "green", "doc_count" : 2, "avg_price" : &#123; "value" : 21000.0 &#125; &#125; ] &#125; &#125;&#125; 可以看到每个桶中都有自己的avg_price字段，这是度量聚合的结果 桶内嵌套桶刚刚的案例中，我们在桶内嵌套度量运算。事实上桶不仅可以嵌套运算， 还可以再嵌套其它桶。也就是说在每个分组中，再分更多组。 比如：我们想统计每种颜色的汽车中，分别属于哪个制造商，按照make字段再进行分桶 1234567891011121314151617181920212223GET /cars/_search&#123; "size" : 0, "aggs" : &#123; "popular_colors" : &#123; "terms" : &#123; "field" : "color" &#125;, "aggs":&#123; "avg_price": &#123; "avg": &#123; "field": "price" &#125; &#125;, "maker":&#123; "terms":&#123; "field":"make" &#125; &#125; &#125; &#125; &#125;&#125; 原来的color桶和avg计算我们不变 maker：在嵌套的aggs下新添一个桶，叫做maker terms：桶的划分类型依然是词条 filed：这里根据make字段进行划分 结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "red", "doc_count" : 4, "maker" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "honda", "doc_count" : 3 &#125;, &#123; "key" : "bmw", "doc_count" : 1 &#125; ] &#125;, "avg_price" : &#123; "value" : 32500.0 &#125; &#125;, &#123; "key" : "blue", "doc_count" : 2, "maker" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "ford", "doc_count" : 1 &#125;, &#123; "key" : "toyota", "doc_count" : 1 &#125; ] &#125;, "avg_price" : &#123; "value" : 20000.0 &#125; &#125;, &#123; "key" : "green", "doc_count" : 2, "maker" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "ford", "doc_count" : 1 &#125;, &#123; "key" : "toyota", "doc_count" : 1 &#125; ] &#125;, "avg_price" : &#123; "value" : 21000.0 &#125; &#125; ] &#125; &#125;&#125; 我们可以看到，新的聚合maker被嵌套在原来每一个color的桶中。 每个颜色下面都根据 make字段进行了分组 我们能读取到的信息： 红色车共有4辆 红色车的平均售价是 $32，500 美元。 其中3辆是 Honda 本田制造，1辆是 BMW 宝马制造。 划分桶的其它方式前面讲了，划分桶的方式有很多，例如： Date Histogram Aggregation：根据日期阶梯分组，例如给定阶梯为周，会自动每周分为一组 Histogram Aggregation：根据数值阶梯分组，与日期类似 Terms Aggregation：根据词条内容分组，词条内容完全匹配的为一组 Range Aggregation：数值和日期的范围分组，指定开始和结束，然后按段分组 刚刚的案例中，我们采用的是Terms Aggregation，即根据词条划分桶。 接下来，我们再学习几个比较实用的： 阶梯分桶Histogram 原理： histogram是把数值类型的字段，按照一定的阶梯大小进行分组。你需要指定一个阶梯值（interval）来划分阶梯大小。 举例： 比如你有价格字段，如果你设定interval的值为200，那么阶梯就会是这样的： 0，200，400，600，… 上面列出的是每个阶梯的key，也是区间的启点。 如果一件商品的价格是450，会落入哪个阶梯区间呢？计算公式如下： 1bucket_key = Math.floor((value - offset) / interval) * interval + offset value：就是当前数据的值，本例中是450 offset：起始偏移量，默认为0 interval：阶梯间隔，比如200 因此你得到的key = Math.floor((450 - 0) / 200) * 200 + 0 = 400 操作一下： 比如，我们对汽车的价格进行分组，指定间隔interval为5000： 123456789101112GET /cars/_search&#123; "size":0, "aggs":&#123; "price":&#123; "histogram": &#123; "field": "price", "interval": 5000 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&#123; "took" : 5, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "price" : &#123; "buckets" : [ &#123; "key" : 10000.0, "doc_count" : 2 &#125;, &#123; "key" : 15000.0, "doc_count" : 1 &#125;, &#123; "key" : 20000.0, "doc_count" : 2 &#125;, &#123; "key" : 25000.0, "doc_count" : 1 &#125;, &#123; "key" : 30000.0, "doc_count" : 1 &#125;, &#123; "key" : 35000.0, "doc_count" : 0 &#125;, &#123; "key" : 40000.0, "doc_count" : 0 &#125;, &#123; "key" : 45000.0, "doc_count" : 0 &#125;, &#123; "key" : 50000.0, "doc_count" : 0 &#125;, &#123; "key" : 55000.0, "doc_count" : 0 &#125;, &#123; "key" : 60000.0, "doc_count" : 0 &#125;, &#123; "key" : 65000.0, "doc_count" : 0 &#125;, &#123; "key" : 70000.0, "doc_count" : 0 &#125;, &#123; "key" : 75000.0, "doc_count" : 0 &#125;, &#123; "key" : 80000.0, "doc_count" : 1 &#125; ] &#125; &#125;&#125; 你会发现，中间有大量的文档数量为0 的桶，看起来很丑。 我们可以增加一个参数min_doc_count为1，来约束最少文档数量为1，这样文档数量为0的桶会被过滤 示例： 12345678910111213GET /cars/_search&#123; "size":0, "aggs":&#123; "price":&#123; "histogram": &#123; "field": "price", "interval": 5000, "min_doc_count": 1 &#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; "took" : 1, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "price" : &#123; "buckets" : [ &#123; "key" : 10000.0, "doc_count" : 2 &#125;, &#123; "key" : 15000.0, "doc_count" : 1 &#125;, &#123; "key" : 20000.0, "doc_count" : 2 &#125;, &#123; "key" : 25000.0, "doc_count" : 1 &#125;, &#123; "key" : 30000.0, "doc_count" : 1 &#125;, &#123; "key" : 80000.0, "doc_count" : 1 &#125; ] &#125; &#125;&#125; 完美，！ 如果你用kibana将结果变为柱形图，会更好看： 范围分桶range范围分桶与阶梯分桶类似，也是把数字按照阶段进行分组，只不过range方式需要你自己指定每一组的起始和结束大小。 Spring Data ElasticsearchElasticsearch提供的Java客户端有一些不太方便的地方： 很多地方需要拼接Json字符串，在java中拼接字符串有多恐怖你应该懂的 需要自己把对象序列化为json存储 查询到结果也需要自己反序列化为对象 因此，我们这里就不讲解原生的Elasticsearch客户端API了。 而是学习Spring提供的套件：Spring Data Elasticsearch 简介Spring Data Elasticsearch是Spring Data项目下的一个子模块。 查看 Spring Data的官网：http://projects.spring.io/spring-data/ Spring Data 是的使命是给各种数据访问提供统一的编程接口，不管是关系型数据库（如MySQL），还是非关系数据库（如Redis），或者类似Elasticsearch这样的索引数据库。从而简化开发人员的代码，提高开发效率。 包含很多不同数据操作的模块： Spring Data Elasticsearch的页面：https://projects.spring.io/spring-data-elasticsearch/ 特征： 支持Spring的基于@Configuration的java配置方式，或者XML配置方式 提供了用于操作ES的便捷工具类ElasticsearchTemplate。包括实现文档到POJO之间的自动智能映射。 利用Spring的数据转换服务实现的功能丰富的对象映射 基于注解的元数据映射方式，而且可扩展以支持更多不同的数据格式 根据持久层接口自动生成对应实现方法，无需人工编写基本操作代码（类似mybatis，根据接口自动得到实现）。当然，也支持人工定制查询 创建Demo工程我们新建一个Maven项目，学习Elasticsearch pom依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.study.demo&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;elasticsearch&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.7.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; application.yml文件配置： 12345spring: data: elasticsearch: cluster-name: elasticsearch cluster-nodes: 119.23.208.179:9300 索引操作创建索引和映射 实体类 首先我们准备好实体类： 12345678public class Item &#123; Long id; String title; //标题 String category;// 分类 String brand; // 品牌 Double price; // 价格 String images; // 图片地址&#125; 映射 Spring Data通过注解来声明字段的映射属性，有下面的三个注解： @Document 作用在类，标记实体类为文档对象，一般有两个属性 indexName：对应索引库名称 type：对应在索引库中的类型 shards：分片数量，默认5 replicas：副本数量，默认1 @Id 作用在成员变量，标记一个字段作为id主键 @Field 作用在成员变量，标记为文档的字段，并指定字段映射属性： type：字段类型，是是枚举：FieldType index：是否索引，布尔类型，默认是true store：是否存储，布尔类型，默认是false analyzer：分词器名称 示例： 1234567891011121314151617181920@Document(indexName = "item",type = "docs", shards = 1, replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text, analyzer = "ik_max_word") private String title; //标题 @Field(type = FieldType.Keyword) private String category;// 分类 @Field(type = FieldType.Keyword) private String brand; // 品牌 @Field(type = FieldType.Double) private Double price; // 价格 @Field(index = false, type = FieldType.Keyword) private String images; // 图片地址&#125; 创建索引 ElasticsearchTemplate中提供了创建索引的API： 可以根据类的信息自动生成，也可以手动指定indexName和Settings 映射 映射相关的API： 一样，可以根据类的字节码信息（注解配置）来生成映射，或者手动编写映射 我们这里采用类的字节码信息创建索引并映射： 1234567@Testpublic void createIndex() &#123; // 创建索引，会根据Item类的@Document注解信息来创建 esTemplate.createIndex(Item.class); // 配置映射，会根据Item类中的id、Field等字段来自动完成映射 esTemplate.putMapping(Item.class);&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445GET /item&#123; "item": &#123; "aliases": &#123;&#125;, "mappings": &#123; "docs": &#123; "properties": &#123; "brand": &#123; "type": "keyword" &#125;, "category": &#123; "type": "keyword" &#125;, "images": &#123; "type": "keyword", "index": false &#125;, "price": &#123; "type": "double" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125; &#125;, "settings": &#123; "index": &#123; "refresh_interval": "1s", "number_of_shards": "1", "provided_name": "item", "creation_date": "1525405022589", "store": &#123; "type": "fs" &#125;, "number_of_replicas": "0", "uuid": "4sE9SAw3Sqq1aAPz5F6OEg", "version": &#123; "created": "6020499" &#125; &#125; &#125; &#125;&#125; 删除索引删除索引的API： 可以根据类名或索引名删除。 示例： 1234@Testpublic void deleteIndex() &#123; esTemplate.deleteIndex("test");&#125; 结果： 新增文档数据Repository接口Spring Data 的强大之处，就在于你不用写任何DAO处理，自动根据方法名或类的信息进行CRUD操作。只要你定义一个接口，然后继承Repository提供的一些子接口，就能具备各种基本的CRUD功能。 来看下Repository的继承关系： 我们看到有一个ElasticsearchCrudRepository接口： 所以，我们只需要定义接口，然后继承它就OK了。 12public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123;&#125; 接下来，我们测试新增数据： 一个对象123456789@Autowiredprivate ItemRepository itemRepository;@Testpublic void index() &#123; Item item = new Item(1L, "小米手机7", " 手机", "小米", 3499.00, "http://image.codeopen.club/13123.jpg"); itemRepository.save(item);&#125; 去页面查询看看： 12345678910111213141516171819202122232425262728293031&#123; "took": 0, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 1, "max_score": 1, "hits": [ &#123; "_index": "item", "_type": "docs", "_id": "1", "_score": 1, "_source": &#123; "id": 1, "title": "小米手机7", "category": " 手机", "brand": "小米", "price": 3499, "images": "http://image.codeopen.club/13123.jpg" &#125; &#125; &#125; ] &#125;&#125; 批量新增代码： 12345678@Testpublic void indexList() &#123; List&lt;Item&gt; list = new ArrayList&lt;&gt;(); list.add(new Item(2L, "坚果手机R1", " 手机", "锤子", 3699.00, "http://image.codeopen.club/123.jpg")); list.add(new Item(3L, "华为META10", " 手机", "华为", 4499.00, "http://image.codeopen.club/3.jpg")); // 接收对象集合，实现批量新增 itemRepository.saveAll(list);&#125; 再次去页面查询： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&#123; "took": 5, "timed_out": false, "_shards": &#123; "total": 1, "successful": 1, "skipped": 0, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "item", "_type": "docs", "_id": "2", "_score": 1, "_source": &#123; "id": 2, "title": "坚果手机R1", "category": " 手机", "brand": "锤子", "price": 3699, "images": "http://image.codeopen.club/13123.jpg" &#125; &#125;, &#123; "_index": "item", "_type": "docs", "_id": "3", "_score": 1, "_source": &#123; "id": 3, "title": "华为META10", "category": " 手机", "brand": "华为", "price": 4499, "images": "http://image.codeopen.club/13123.jpg" &#125; &#125;, &#123; "_index": "item", "_type": "docs", "_id": "1", "_score": 1, "_source": &#123; "id": 1, "title": "小米手机7", "category": " 手机", "brand": "小米", "price": 3499, "images": "http://image.codeopen.club/13123.jpg" &#125; &#125; ] &#125;&#125; 修改修改和新增是同一个接口，区分的依据就是id，这一点跟我们在页面发起PUT请求是类似的。 查询基本查询ElasticsearchRepository提供了一些基本的查询方法： 我们来试试查询所有： 12345678@Testpublic void query()&#123; // 查询全部，并安装价格降序排序 Iterable&lt;Item&gt; items = this.itemRepository.findAll(Sort.by("price").descending()); for (Item item : items) &#123; System.out.println("item = " + item); &#125;&#125; 结果： 自定义方法Spring Data 的另一个强大功能，是根据方法名称自动实现功能。 比如：你的方法名叫做：findByTitle，那么它就知道你是根据title查询，然后自动帮你完成，无需写实现类。 当然，方法名称要符合一定的约定： Keyword Sample Elasticsearch Query String And findByNameAndPrice {&quot;bool&quot; : {&quot;must&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Or findByNameOrPrice {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;price&quot; : &quot;?&quot;}} ]}} Is findByName {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Not findByNameNot {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}} Between findByPriceBetween {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} LessThanEqual findByPriceLessThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} GreaterThanEqual findByPriceGreaterThan {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Before findByPriceBefore {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : null,&quot;to&quot; : ?,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} After findByPriceAfter {&quot;bool&quot; : {&quot;must&quot; : {&quot;range&quot; : {&quot;price&quot; : {&quot;from&quot; : ?,&quot;to&quot; : null,&quot;include_lower&quot; : true,&quot;include_upper&quot; : true}}}}} Like findByNameLike {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} StartingWith findByNameStartingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;?*&quot;,&quot;analyze_wildcard&quot; : true}}}}} EndingWith findByNameEndingWith {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;*?&quot;,&quot;analyze_wildcard&quot; : true}}}}} Contains/Containing findByNameContaining {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;name&quot; : {&quot;query&quot; : &quot;**?**&quot;,&quot;analyze_wildcard&quot; : true}}}}} In findByNameIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must&quot; : {&quot;bool&quot; : {&quot;should&quot; : [ {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}, {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}} ]}}}} NotIn findByNameNotIn(Collection&lt;String&gt;names) {&quot;bool&quot; : {&quot;must_not&quot; : {&quot;bool&quot; : {&quot;should&quot; : {&quot;field&quot; : {&quot;name&quot; : &quot;?&quot;}}}}}} Near findByStoreNear Not Supported Yet ! True findByAvailableTrue {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} False findByAvailableFalse {&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : false}}}} OrderBy findByAvailableTrueOrderByNameDesc {&quot;sort&quot; : [{ &quot;name&quot; : {&quot;order&quot; : &quot;desc&quot;} }],&quot;bool&quot; : {&quot;must&quot; : {&quot;field&quot; : {&quot;available&quot; : true}}}} 例如，我们来按照价格区间查询，定义这样的一个方法： 12345678910public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123; /** * 根据价格区间查询 * @param price1 * @param price2 * @return */ List&lt;Item&gt; findByPriceBetween(double price1, double price2);&#125; 然后添加一些测试数据： 1234567891011@Testpublic void indexList() &#123; List&lt;Item&gt; list = new ArrayList&lt;&gt;(); list.add(new Item(1L, "小米手机7", "手机", "小米", 3299.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(2L, "坚果手机R1", "手机", "锤子", 3699.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(3L, "华为META10", "手机", "华为", 4499.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(4L, "小米Mix2S", "手机", "小米", 4299.00, "http://image.leyou.com/13123.jpg")); list.add(new Item(5L, "荣耀V10", "手机", "华为", 2799.00, "http://image.leyou.com/13123.jpg")); // 接收对象集合，实现批量新增 itemRepository.saveAll(list);&#125; 不需要写实现类，然后我们直接去运行： 1234567@Testpublic void queryByPriceBetween()&#123; List&lt;Item&gt; list = this.itemRepository.findByPriceBetween(2000.00, 3500.00); for (Item item : list) &#123; System.out.println("item = " + item); &#125;&#125; 结果： 自定义查询先来看最基本的match query： 123456789101112131415@Testpublic void search()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本分词查询 queryBuilder.withQuery(QueryBuilders.matchQuery("title", "小米手机")); // 搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 总条数 long total = items.getTotalElements(); System.out.println("total = " + total); for (Item item : items) &#123; System.out.println(item); &#125;&#125; NativeSearchQueryBuilder：Spring提供的一个查询条件构建器，帮助构建json格式的请求体 QueryBuilders.matchQuery(“title”, “小米手机”)：利用QueryBuilders来生成一个查询。QueryBuilders提供了大量的静态方法，用于生成各种不同类型的查询： Page&lt;item&gt;：默认是分页查询，因此返回的是一个分页的结果对象，包含属性： totalElements：总条数 totalPages：总页数 Iterator：迭代器，本身实现了Iterator接口，因此可直接迭代得到当前页的数据 其它属性： 结果： 分页查询利用NativeSearchQueryBuilder可以方便的实现分页： 123456789101112131415161718192021222324252627@Testpublic void searchByPage()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本分词查询 queryBuilder.withQuery(QueryBuilders.termQuery("category", "手机")); // 分页： int page = 0; int size = 2; queryBuilder.withPageable(PageRequest.of(page,size)); // 搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 总条数 long total = items.getTotalElements(); System.out.println("总条数 = " + total); // 总页数 System.out.println("总页数 = " + items.getTotalPages()); // 当前页 System.out.println("当前页：" + items.getNumber()); // 每页大小 System.out.println("每页大小：" + items.getSize()); for (Item item : items) &#123; System.out.println(item); &#125;&#125; 可以发现，Elasticsearch中的分页是从第0页开始。 排序排序也通用通过NativeSearchQueryBuilder完成： 1234567891011121314151617181920@Testpublic void searchAndSort()&#123; // 构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 添加基本分词查询 queryBuilder.withQuery(QueryBuilders.termQuery("category", "手机")); // 排序 queryBuilder.withSort(SortBuilders.fieldSort("price").order(SortOrder.ASC)); // 搜索，获取结果 Page&lt;Item&gt; items = this.itemRepository.search(queryBuilder.build()); // 总条数 long total = items.getTotalElements(); System.out.println("总条数 = " + total); for (Item item : items) &#123; System.out.println(item); &#125;&#125; 结果： 聚合聚合为桶桶就是分组，比如这里我们按照品牌brand进行分组： 12345678910111213141516171819202122232425@Testpublic void testAgg()&#123; NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;""&#125;, null)); // 1、添加一个新的聚合，聚合类型为terms，聚合名称为brands，聚合字段为brand queryBuilder.addAggregation( AggregationBuilders.terms("brands").field("brand")); // 2、查询,需要把结果强转为AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;) this.itemRepository.search(queryBuilder.build()); // 3、解析 // 3.1、从结果中取出名为brands的那个聚合， // 因为是利用String类型字段来进行的term聚合，所以结果要强转为StringTerm类型 StringTerms agg = (StringTerms) aggPage.getAggregation("brands"); // 3.2、获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); // 3.3、遍历 for (StringTerms.Bucket bucket : buckets) &#123; // 3.4、获取桶中的key，即品牌名称 System.out.println(bucket.getKeyAsString()); // 3.5、获取桶中的文档数量 System.out.println(bucket.getDocCount()); &#125;&#125; 显示的结果： 关键API： AggregationBuilders：聚合的构建工厂类。所有聚合都由这个类来构建，看看他的静态方法： AggregatedPage：聚合查询的结果类。它是Page&lt;T&gt;的子接口： AggregatedPage在Page功能的基础上，拓展了与聚合相关的功能，它其实就是对聚合结果的一种封装，大家可以对照聚合结果的JSON结构来看。 而返回的结果都是Aggregation类型对象，不过根据字段类型不同，又有不同的子类表示 我们看下页面的查询的JSON结果与Java类的对照关系： 嵌套聚合，求平均值代码： 1234567891011121314151617181920212223242526272829@Testpublic void testSubAgg()&#123; NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;""&#125;, null)); // 1、添加一个新的聚合，聚合类型为terms，聚合名称为brands，聚合字段为brand queryBuilder.addAggregation( AggregationBuilders.terms("brands").field("brand") .subAggregation(AggregationBuilders.avg("priceAvg").field("price")) // 在品牌聚合桶内进行嵌套聚合，求平均值 ); // 2、查询,需要把结果强转为AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;) this.itemRepository.search(queryBuilder.build()); // 3、解析 // 3.1、从结果中取出名为brands的那个聚合， // 因为是利用String类型字段来进行的term聚合，所以结果要强转为StringTerm类型 StringTerms agg = (StringTerms) aggPage.getAggregation("brands"); // 3.2、获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); // 3.3、遍历 for (StringTerms.Bucket bucket : buckets) &#123; // 3.4、获取桶中的key，即品牌名称 3.5、获取桶中的文档数量 System.out.println(bucket.getKeyAsString() + "，共" + bucket.getDocCount() + "台"); // 3.6.获取子聚合结果： InternalAvg avg = (InternalAvg) bucket.getAggregations().asMap().get("priceAvg"); System.out.println("平均售价：" + avg.getValue()); &#125;&#125; 结果：]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Registry私服安装及使用]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FRegistry%E7%A7%81%E6%9C%8D%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简介官方的 Docker Hub 是一个用于管理公共镜像的地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候我们的服务器无法访问互联网，或者你不希望将自己的镜像放到公网当中，那么你就需要 Docker Registry，它可以用来存储和管理自己的镜像。 安装编写docker-compose.yml在之前的 Docker 私有仓库 章节中已经提到过如何配置和使用容器运行私有仓库，这里我们使用 docker-compose 来安装，配置如下： 12345678910version: '3.1'services: registry: image: registry restart: always container_name: registry ports: - 5000:5000 volumes: - /usr/local/docker/registry/data:/var/lib/registry 测试启动成功后需要测试服务端是否能够正常提供服务，有两种方式： 浏览器端访问 1http://ip:5000/v2/ 终端访问 1curl http://ip:5000/v2/ 配置 Docker Registry 客户端在 /etc/docker/daemon.json中增加如下内容（如果文件不存在请新建该文件） 12345678&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ], &quot;insecure-registries&quot;: [ &quot;ip:5000&quot; ]&#125; 注意：该文件必须符合 json 规范，否则 Docker 将不能启动。 之后重新启动服务。 12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 检查客户端配置是否生效使用 docker info 命令手动检查，如果从配置中看到如下内容，说明配置成功（192.168.0.113） 测试镜像上传我们以 mysql 为例测试镜像上传功能 12345678910111213141516## 拉取一个镜像docker pull mysql:5.7.23## 查看全部镜像docker images## 标记本地镜像并指向目标仓库（ip:port/image_name:tag，该格式为标记版本号）docker tag nginx 192.168.0.113:5000/mysql:5.7.23## 提交镜像到仓库docker push 192.168.0.113:5000/mysql:5.7.23 查看全部镜像1curl -XGET http://192.168.0.113:5000/v2/_catalog 查看指定镜像以 mysql 为例，查看已提交的列表 1curl -XGET http://192.168.0.113:5000/v2/mysql/tags/list 测试拉取镜像 先删除镜像 12docker rmi mysql:5.7.23docker rmi 192.168.0.113:5000/mysql:5.7.23 再拉取镜像 1docker pull 192.168.75.133:5000/nginx 部署 Docker Registry WebUI私服安装成功后就可以使用 docker 命令行工具对 registry 做各种操作了。然而不太方便的地方是不能直观的查看 registry 中的资源情况。如果可以使用 UI 工具管理镜像就更好了。这里介绍两个 Docker Registry WebUI 工具 docker-registry-frontend docker-registry-web 两个工具的功能和界面都差不多，我们以 docker-registry-fontend 为例讲解 docker-registry-frontend我们使用 docker-compose 来安装和运行，docker-compose.yml 配置如下： 123456789101112version: &apos;3.1&apos;services: frontend: image: konradkleine/docker-registry-frontend:v2 ports: - 8080:80 volumes: - ./certs/frontend.crt:/etc/apache2/server.crt:ro - ./certs/frontend.key:/etc/apache2/server.key:ro environment: - ENV_DOCKER_REGISTRY_HOST=192.168.0.113 - ENV_DOCKER_REGISTRY_PORT=5000 注意：请将配置文件中的主机和端口换成自己仓库的地址 运行成功后在浏览器访问：http://192.168.0.113]]></content>
      <categories>
        <category>Registry</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Registry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql安装]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FMySQL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. 编写docker-compose.yml文件Mysql 51234567891011121314151617181920version: '3'services: mysql: restart: always image: mysql:5.7.23 container_name: mysql ports: - 3306:3306 environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123456 command: --character-set-server=utf8 --collation-server=utf8_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 --max_allowed_packet=128M --sql-mode="STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO" volumes: - ./data:/var/lib/mysql Mysql 8**1234567891011121314151617181920212223version: '3.1'services: db: image: mysql restart: always environment: MYSQL_ROOT_PASSWORD: 123456 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8 --collation-server=utf8_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 ports: - 3306:3306 volumes: - ./data:/var/lib/mysql adminer: image: adminer restart: always ports: - 8080:8080 2. 使用命令docker-compose up -d启动]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB安装]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FMongoDB%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[docker-compose.yml：123456789101112version: "3"services: mongo: restart: always image: mongo container_name: mongo volumes: - data:/data/db ports: - 27017:27017volumes: data:]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven私服Nexus3快速搭建使用]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FMaven%E7%A7%81%E6%9C%8DNexus3%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简介 官网：http://books.sonatype.com/nexus-book/reference3/index.html 下载：https://help.sonatype.com/repomanager3/download 话不多说，马上开撸 安装在这里，博主没有使用官方的安装方式，而是使用docker-compose去安装nexus3，个人认为这种方式，简单方便，当然安装前提需要安装docker和docker-compose，这些无脑操作就不赘述了 docker-compose文件docker-compose.yml 12345678910111213version: '3'services: nexus: #restart: always image: sonatype/nexus3 container_name: nexus environment: TZ: Asia/Shanghai INSTALL4J_ADD_VM_PARAMS: "-Xms256m -Xmx256m -XX:MaxDirectMemorySize=512m" ports: - 8081:8081 volumes: - ./data:/nexus-data 自定义配置参数参考：https://hub.docker.com/r/sonatype/nexus3/ 启动nexus 使用启动命令： 1docker-compose up -d 查看启动信息： 1docker logs -f nexus 启动信息没有报错且出现如图则启动成功： 访问nexus http://ip:8081 默认用户名：admin 默认密码：admin123 配置maven中的私服信息在settings.xml中的services节点中添加： 12345678910&lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; 在maven项目中使用配置pom.xml文件 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://132.232.137.183:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://132.232.137.183:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt;]]></content>
      <categories>
        <category>Nexus3</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Nexus3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kibana安装]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2Fkibana%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[docker-compose.yml123456789101112version: '3'services: kibana: container_name: kibana image: docker.elastic.co/kibana/kibana:6.5.4 volumes: - ./kibana.yml:/usr/share/kibana/config/kibana.yml ports: - 5601:5601 environment: SERVER_HOST: 0.0.0.0 ELASTICSEARCH_URL: http://IP:9200/ 注:IP为elasticsearch的IP地址]]></content>
      <categories>
        <category>kibana</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker构建jar包并创建docker-compose启动镜像]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2Fdocker%E6%9E%84%E5%BB%BAjar%E5%8C%85%E5%B9%B6%E5%88%9B%E5%BB%BAdocker-compose%E5%90%AF%E5%8A%A8%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[创建Dockerfile文件123456789FROM openjdk:8-jreRUN mkdir /appCOPY jar包.jar /app/CMD java -jar /app/jar包.jar --spring.profiles.active=prodEXPOSE 暴露端口号 通过dockerfile文本生成工程镜像 1docker build -t canteen . docker build -t canteen:命令构建镜像名为canteen的镜像，-t:给镜像取名为canteen 编写docker-compose.yml12345678910version: &apos;3.1&apos;services: canteen: restart: always image: canteen container_name: canteen prots: - 8002:8002 environment: TZ: Asia/Shanghai]]></content>
      <categories>
        <category>Docker build</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Docker build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab安装]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2Fgitlab%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[简介GitLab 是利用 Ruby on Rails 一个开源的版本管理系统，实现一个自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。它拥有与 Github 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序 (Wall) 进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。 编写docker-compose.yml文件 12345678910111213141516171819202122version: &apos;3&apos;services: web: image: &apos;twang2218/gitlab-ce-zh:10.5&apos; restart: always hostname: &apos;192.168.0.125&apos; privileged: true environment: TZ: &apos;Asia/Shanghai&apos; GITLAB_OMNIBUS_CONFIG: | external_url &apos;http://192.168.0.125&apos; gitlab_rails[&apos;gitlab_shell_ssh_port&apos;] = 2222 unicorn[&apos;port&apos;] = 8888 nginx[&apos;listen_port&apos;] = 80 ports: - &apos;80:80&apos; - &apos;8443:443&apos; - &apos;2222:22&apos; volumes: - /usr/local/docker/gitlab/config:/etc/gitlab - /usr/local/docker/gitlab/data:/var/opt/gitlab - /usr/local/docker/gitlab/logs:/var/log/gitlab 使用命令docker-compose up启动访问http://192.168.0.125，验证gitlab是否安装成功gitlab默认用户root]]></content>
      <categories>
        <category>Gitlab</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FDocker%20Compose%E6%A8%A1%E6%9D%BF%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[模板文件是使用 Compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。 默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。 123456789version: "3"services: webapp: image: examples/web ports: - "80:80" volumes: - "/data" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。 如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中再次设置。 下面分别介绍各个指令的用法。 build指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。 12345version: '3'services: webapp: build: ./dir 你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径。 使用 dockerfile 指令指定 Dockerfile 文件名。 使用 arg 指令指定构建镜像时的变量。 123456789version: '3'services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 使用 cache_from 指定构建镜像的缓存 12345build: context: . cache_from: - alpine:latest - corp/web_app:3.14 cap_add, cap_drop指定容器的内核能力（capacity）分配。 例如，让容器拥有所有能力可以指定为： 12cap_add: - ALL 去掉 NET_ADMIN 能力可以指定为： 12cap_drop: - NET_ADMIN command覆盖容器启动后默认执行的命令。 1command: echo "hello world" configs仅用于 Swarm mode cgroup_parent指定父 cgroup 组，意味着将继承该组的资源限制。 例如，创建了一个 cgroup 组名称为 cgroups_1。 1cgroup_parent: cgroups_1 container_name指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。 1container_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。 deploy仅用于 Swarm mode devices指定设备映射关系。 12devices: - "/dev/ttyUSB1:/dev/ttyUSB0" depends_on解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web 1234567891011121314version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。 dns自定义 DNS 服务器。可以是一个值，也可以是一个列表。 12345dns: 8.8.8.8dns: - 8.8.8.8 - 114.114.114.114 dns_search配置 DNS 搜索域。可以是一个值，也可以是一个列表。 12345dns_search: example.comdns_search: - domain1.example.com - domain2.example.com tmpfs挂载一个 tmpfs 文件系统到容器。 1234tmpfs: /runtmpfs: - /run - /tmp env_file从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。 如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。 123456env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。 12# common.env: Set development environmentPROG_ENV=development environment设置环境变量。你可以使用数组或字典两种格式。 只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括 1y|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF expose暴露端口，但不映射到宿主机，只被连接的服务访问。 仅可以指定内部端口为参数 123expose: - "3000" - "8000" external_links 注意：不建议使用该指令。 链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的外部容器。 1234external_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql extra_hosts类似 Docker 中的 --add-host 参数，指定额外的 host 名称映射信息。 123extra_hosts: - "googledns:8.8.8.8" - "dockerhub:52.1.157.61" 会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。 128.8.8.8 googledns52.1.157.61 dockerhub healthcheck通过命令检查容器是否健康运行。 12345healthcheck: test: ["CMD", "curl", "-f", "http://localhost"] interval: 1m30s timeout: 10s retries: 3 image指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。 123image: ubuntuimage: orchardup/postgresqlimage: a4bc65fd labels为容器添加 Docker 元数据（metadata）信息。例如可以为容器添加辅助说明信息。 1234labels: com.startupteam.description: "webapp for a startup team" com.startupteam.department: "devops department" com.startupteam.release: "rc3 for v1.0" links 注意：不推荐使用该指令。 logging配置日志选项。 1234logging: driver: syslog options: syslog-address: "tcp://192.168.0.42:123" 目前支持三种日志驱动类型。 123driver: "json-file"driver: "syslog"driver: "none" options 配置日志驱动的相关参数。 123options: max-size: "200k" max-file: "10" network_mode设置网络模式。使用和 docker run 的 --network 参数一样的值。 12345network_mode: "bridge"network_mode: "host"network_mode: "none"network_mode: "service:[service name]"network_mode: "container:[container name/id]" networks配置容器连接的网络。 1234567891011version: "3"services: some-service: networks: - some-network - other-networknetworks: some-network: other-network: pid跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 ID 来相互访问和操作。 1pid: &quot;host&quot; ports暴露端口信息。 使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。 12345ports: - "3000" - "8000:8000" - "49100:22" - "127.0.0.1:8001:8001" 注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 YAML 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。 secrets存储敏感数据，例如 mysql 服务密码。 12345678910111213141516version: "3.1"services:mysql: image: mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password secrets: - db_root_password - my_other_secretsecrets: my_secret: file: ./my_secret.txt my_other_secret: external: true security_opt指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。例如配置标签的用户名和角色名。 123security_opt: - label:user:USER - label:role:ROLE stop_signal设置另一个信号来停止容器。在默认情况下使用的是 SIGTERM 停止容器。 1stop_signal: SIGUSR1 sysctls配置容器内核参数。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits指定容器的 ulimits 限制值。 例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。 该指令中路径支持相对路径。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 其它指令此外，还有包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir等指令，基本跟 docker run 中对应参数的功能一致。 指定服务容器启动后执行的入口文件。 1entrypoint: /code/entrypoint.sh 指定容器中运行应用的用户名。 1user: nginx 指定容器中工作目录。 1working_dir: /code 指定容器中搜索域名、主机名、mac 地址等。 123domainname: your_website.comhostname: testmac_address: 08-00-27-00-0C-0A 允许容器中运行一些特权命令。 1privileged: true 指定容器退出后的重启策略为始终重启。该命令对保持服务始终运行十分有效，在生产环境中推荐配置为 always 或者 unless-stopped。 1restart: always 以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。 1yamlread_only: true 打开标准输入，可以接受外部输入。 1stdin_open: true 模拟一个伪终端。 1tty: true 读取变量Compose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。 例如，下面的 Compose 文件将从运行它的环境中读取变量 ${MONGO_VERSION} 的值，并写入执行的指令中。 12345version: "3"services:db: image: "mongo:$&#123;MONGO_VERSION&#125;" 如果执行 MONGO_VERSION=3.2 docker-compose up 则会启动一个 mongo:3.2 镜像的容器；如果执行 MONGO_VERSION=2.8 docker-compose up 则会启动一个 mongo:2.8 镜像的容器。 若当前目录存在 .env 文件，执行 docker-compose 命令时将从该文件中读取变量。 在当前目录新建 .env 文件并写入以下内容。 12# 支持 # 号注释MONGO_VERSION=3.6 执行 docker-compose up 则会启动一个 mongo:3.6 镜像的容器。]]></content>
      <categories>
        <category>Docker Compose</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FDocker%20Compose%2F</url>
    <content type="text"><![CDATA[什么是 Docker ComposeDocker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用。 概述Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。 其代码目前在 https://github.com/docker/compose 上开源。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 通过第一部分中的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 Docker Compose安装与卸载Compose 支持 Linux、macOS、Windows 10 三大平台。 Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。 前两种方式是传统方式，适合本地环境下安装使用；最后一种方式则不破坏系统环境，更适合云计算场景。 Docker for Mac 、Docker for Windows 自带 docker-compose 二进制文件，安装 Docker 之后可以直接使用。 123$ docker-compose --versiondocker-compose version 1.17.1, build 6d101fb Linux 系统请使用以下介绍的方法安装。在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。例如，在 Linux 64 位系统上直接下载对应的二进制包。 12$ sudo curl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 卸载1$ sudo rm /usr/local/bin/docker-compose]]></content>
      <categories>
        <category>Docker Compose</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose命令]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FDocker%20Compose%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[命令对象与格式对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。 执行 docker-compose [COMMAND] --help 或者 docker-compose help [COMMAND] 可以查看具体某个命令的使用格式。 docker-compose 命令的基本的使用格式是 1docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] 命令选项 -f, --file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。 -p, --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。 --x-networking 使用 Docker 的可拔插网络后端特性 --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge --verbose 输出更多调试信息。 -v, --version 打印版本并退出。 build格式为 docker-compose build [options] [SERVICE...]。 构建（重新构建）项目中的服务容器。 服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。 可以随时在项目目录下运行 docker-compose build 来重新构建服务。 选项包括： --force-rm 删除构建过程中的临时容器。 --no-cache 构建镜像过程中不使用 cache（这将加长构建过程）。 --pull 始终尝试通过 pull 来获取更新版本的镜像。 config验证 Compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。 down此命令将会停止 up 命令所启动的容器，并移除网络 exec进入指定的容器。 help获得一个命令的帮助。 images列出 Compose 文件中包含的镜像。 kill格式为 docker-compose kill [options] [SERVICE...]。 通过发送 SIGKILL 信号来强制停止服务容器。 支持通过 -s 参数来指定发送的信号，例如通过如下指令发送 SIGINT 信号。 1$ docker-compose kill -s SIGINT logs格式为 docker-compose logs [options] [SERVICE...]。 查看服务容器的输出。默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分。可以通过 --no-color 来关闭颜色。 该命令在调试问题的时候十分有用。 pause格式为 docker-compose pause [SERVICE...]。 暂停一个服务容器。 port格式为 docker-compose port [options] SERVICE PRIVATE_PORT。 打印某个容器端口所映射的公共端口。 选项： --protocol=proto 指定端口协议，tcp（默认值）或者 udp。 --index=index 如果同一服务存在多个容器，指定命令对象容器的序号（默认为 1）。 ps格式为 docker-compose ps [options] [SERVICE...]。 列出项目中目前的所有容器。 选项： pull 格式为 docker-compose pull [options] [SERVICE...]。 拉取服务依赖的镜像。 选项： --ignore-pull-failures 忽略拉取镜像过程中的错误。 pushrestart格式为 docker-compose restart [options] [SERVICE...]。 重启项目中的服务。 选项： -t, --timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。 rm格式为 docker-compose rm [options] [SERVICE...]。 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 选项： -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。 -v 删除容器所挂载的数据卷。 run格式为 docker-compose run [options] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]。 在指定服务上执行一个命令。 例如： 1$ docker-compose run ubuntu ping docker.com 将会启动一个 ubuntu 服务容器，并执行 ping docker.com 命令。 默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。 该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。 两个不同点： 给定命令将会覆盖原有的自动运行命令； 不会自动创建端口，以避免冲突。 如果不希望自动启动关联的容器，可以使用 --no-deps 选项，例如 1$ docker-compose run --no-deps web python manage.py shell 将不会启动 web 容器所关联的其它容器。 选项： -d 后台运行容器。 --name NAME 为容器指定一个名字。 --entrypoint CMD 覆盖默认的容器启动指令。 -e KEY=VAL 设置环境变量值，可多次使用选项来设置多个环境变量。 -u, --user=&quot;&quot; 指定运行容器的用户名或者 uid。 --no-deps 不自动启动关联的服务容器。 --rm 运行命令后自动删除容器，d 模式下将忽略。 -p, --publish=[] 映射容器端口到本地主机。 --service-ports 配置服务端口并映射到本地主机。 -T 不分配伪 tty，意味着依赖 tty 的指令将无法运行。 scale格式为 docker-compose scale [options] [SERVICE=NUM...]。 设置指定服务运行的容器个数。 通过 service=num 的参数来设置数量。例如： 1$ docker-compose scale web=3 db=2 将启动 3 个容器运行 web 服务，2 个容器运行 db 服务。 一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 start格式为 docker-compose start [SERVICE...]。 stop格式为 docker-compose stop [options] [SERVICE...]。 停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 top查看各个服务容器内运行的进程。 unpause格式为 docker-compose unpause [SERVICE...]。 恢复处于暂停状态中的服务。 up格式为 docker-compose up [options] [SERVICE...]。 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。 链接的服务都将会被自动启动，除非已经处于运行状态。 可以说，大部分时候都可以直接通过该命令来启动一个项目。 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。 当通过 Ctrl-C 停止命令时，所有容器将会停止。 如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用 docker-compose up --no-recreate。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。如果用户只想重新部署某个服务，可以使用 docker-compose up --no-deps -d &lt;SERVICE_NAME&gt; 来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。 选项： -d 在后台运行服务容器。 --no-color 不使用颜色来区分不同的服务的控制台输出。 --no-deps 不启动服务所链接的容器。 --force-recreate 强制重新创建容器，不能与 --no-recreate 同时使用。 --no-recreate 如果容器已经存在了，则不重新创建，不能与 --force-recreate 同时使用。 --no-build 不自动构建缺失的服务镜像。 -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 version格式为 docker-compose version。 打印版本信息。]]></content>
      <categories>
        <category>Docker Compose</category>
      </categories>
      <tags>
        <tag>Docker Compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS Docker 安装]]></title>
    <url>%2F2019%2F05%2F23%2Fdocker%2FCentOS%20Docker%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Docker支持以下的CentOS版本： CentOS 7 (64-bit) CentOS 6.5 (64-bit) 或更高的版本 前提条件目前，CentOS 仅发行版本中的内核支持 Docker。 Docker 运行在 CentOS 7 上，要求系统为64位、系统内核版本为 3.10 以上。 Docker 运行在 CentOS-6.5 或更高的版本的 CentOS 上，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。 使用 yum 安装（CentOS 7下）Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。 通过 uname -r 命令查看你当前的内核版本 [root@runoob ~]# uname -r 3.10.0-327.el7.x86_64 安装Docker CE版 移除旧的版本 12345678910sudo yum remove docker \ docker-client \ docker-client-latest \ ocker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 安装一些必要的系统工具： 1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息： 1sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新yum缓存 1sudo yum makecache fast 安装Docker-ce 1sudo yum -y install docker-ce 启动Docker服务 1sudo systemctl start docker 设置Docker开机自启 1sudo systemctl enable docker 关闭Docker服务 1sudo systemctl stop docker 镜像加速 在 /etc/docker/daemon.json中写入如下内容（如果文件不存在请新建该文件） 12345&#123; "registry-mirrors": [ "https://registry.docker-cn.com" ]&#125; 删除Docker CE12$ sudo yum remove docker-ce$ sudo rm -rf /var/lib/docker]]></content>
      <categories>
        <category>Docker</category>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云短信服务工具类]]></title>
    <url>%2F2019%2F05%2F23%2FdevNote%2F%E9%98%BF%E9%87%8C%E4%BA%91%E7%9F%AD%E4%BF%A1%E6%9C%8D%E5%8A%A1%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[短信服务（Short Message Service）是阿里云为用户提供的一种通信服务的能力。支持国内和国际快速发送验证码、短信通知和推广短信，服务范围覆盖全球200多个国家和地区。国内短信支持三网合一专属通道，与工信部携号转网平台实时互联。电信级运维保障，实时监控自动切换，到达率高达99%。完美支撑双11期间20亿短信发送，6亿用户触达。 工具内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import com.aliyuncs.DefaultAcsClient;import com.aliyuncs.IAcsClient;import com.aliyuncs.dysmsapi.model.v20170525.QuerySendDetailsRequest;import com.aliyuncs.dysmsapi.model.v20170525.QuerySendDetailsResponse;import com.aliyuncs.dysmsapi.model.v20170525.SendSmsRequest;import com.aliyuncs.dysmsapi.model.v20170525.SendSmsResponse;import com.aliyuncs.exceptions.ClientException;import com.aliyuncs.profile.DefaultProfile;import com.aliyuncs.profile.IClientProfile;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.core.env.Environment;import org.springframework.stereotype.Component;import java.text.SimpleDateFormat;import java.util.Date;/** * 短信工具类 * @author Administrator * */@Componentpublic class SmsUtil &#123; //产品名称:云通信短信API产品,开发者无需替换 static final String product = "Dysmsapi"; //产品域名,开发者无需替换 static final String domain = "dysmsapi.aliyuncs.com"; @Autowired private Environment env; // TODO 此处需要替换成开发者自己的AK(在阿里云访问控制台寻找) /** * 发送短信 * @param mobile 手机号 * @param template_code 模板号 * @param sign_name 签名 * @param param 参数 * @return * @throws ClientException */ public SendSmsResponse sendSms(String mobile,String template_code,String sign_name,String param) throws ClientException &#123; String accessKeyId =env.getProperty("aliyun.sms.accessKeyId"); String accessKeySecret = env.getProperty("aliyun.sms.accessKeySecret"); //可自助调整超时时间 System.setProperty("sun.net.client.defaultConnectTimeout", "10000"); System.setProperty("sun.net.client.defaultReadTimeout", "10000"); //初始化acsClient,暂不支持region化 IClientProfile profile = DefaultProfile.getProfile("cn-hangzhou", accessKeyId, accessKeySecret); DefaultProfile.addEndpoint("cn-hangzhou", "cn-hangzhou", product, domain); IAcsClient acsClient = new DefaultAcsClient(profile); //组装请求对象-具体描述见控制台-文档部分内容 SendSmsRequest request = new SendSmsRequest(); //必填:待发送手机号 request.setPhoneNumbers(mobile); //必填:短信签名-可在短信控制台中找到 request.setSignName(sign_name); //必填:短信模板-可在短信控制台中找到 request.setTemplateCode(template_code); //可选:模板中的变量替换JSON串,如模板内容为"亲爱的$&#123;name&#125;,您的验证码为$&#123;code&#125;"时,此处的值为 request.setTemplateParam(param); //选填-上行短信扩展码(无特殊需求用户请忽略此字段) //request.setSmsUpExtendCode("90997"); //可选:outId为提供给业务方扩展字段,最终在短信回执消息中将此值带回给调用者 request.setOutId("yourOutId"); //hint 此处可能会抛出异常，注意catch SendSmsResponse sendSmsResponse = acsClient.getAcsResponse(request); return sendSmsResponse; &#125; public QuerySendDetailsResponse querySendDetails(String mobile,String bizId) throws ClientException &#123; String accessKeyId =env.getProperty("accessKeyId"); String accessKeySecret = env.getProperty("accessKeySecret"); //可自助调整超时时间 System.setProperty("sun.net.client.defaultConnectTimeout", "10000"); System.setProperty("sun.net.client.defaultReadTimeout", "10000"); //初始化acsClient,暂不支持region化 IClientProfile profile = DefaultProfile.getProfile("cn-hangzhou", accessKeyId, accessKeySecret); DefaultProfile.addEndpoint("cn-hangzhou", "cn-hangzhou", product, domain); IAcsClient acsClient = new DefaultAcsClient(profile); //组装请求对象 QuerySendDetailsRequest request = new QuerySendDetailsRequest(); //必填-号码 request.setPhoneNumber(mobile); //可选-流水号 request.setBizId(bizId); //必填-发送日期 支持30天内记录查询，格式yyyyMMdd SimpleDateFormat ft = new SimpleDateFormat("yyyyMMdd"); request.setSendDate(ft.format(new Date())); //必填-页大小 request.setPageSize(10L); //必填-当前页码从1开始计数 request.setCurrentPage(1L); //hint 此处可能会抛出异常，注意catch QuerySendDetailsResponse querySendDetailsResponse = acsClient.getAcsResponse(request); return querySendDetailsResponse; &#125;&#125;]]></content>
      <categories>
        <category>JavaUtils</category>
      </categories>
      <tags>
        <tag>JavaUtils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通用异常处理]]></title>
    <url>%2F2019%2F05%2F23%2FdevNote%2F%E9%80%9A%E7%94%A8%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[在项目中出现异常是在所难免的，但是出现异常后怎么处理，这就是很有学问了。 场景预设场景我们预设这样一个场景，假如我们做新增商品，需要接受下面的参数： 12price: 价格name: 名称 然后对数据做简单校验 价格不能为空 新增时，自动形成ID，然后随商品对象一起返回 代码为了操作方便，使用假数据，不涉及持久层 实体类： 123456@Data // lombokpublic class Item&#123; private Integer id; private String name; private Long price;&#125; service： 12345678910@Servicepublic class ItemService &#123; // 商品新增 public Item saveItem(Item item)&#123; int id = new Random().nextInt(100); item.setId(id); return item; &#125;&#125; controller： 12345678910111213141516@RestController@RequestMapping("item")public class ItemController &#123; @Autowired private ItemService itemService; @PostMapping public ResponseEntity&lt;Item&gt; saveItem(Item item)&#123; // 校验价格 if (item.getPrice() == null)&#123; return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(null); &#125; return ResponseEntity.status(HttpStatus.CREATED).body(itemService.saveItem(item)); &#125;&#125; 响应状态码： Code HTTP Operation Body Contents Description 200 GET,PUT 资源 操作成功 201 POST 资源，元数据 对象创建成功 202 POST,PUT,DELETE,PATCH N/A 请求已经被接受 204 PUT,DELETE,PATCH N/A 操作已经执行成功，但是没有返回数据 301 GET link 资源已被移除 303 GET link 重定向 304 GET N/A 资源没有被修改 400 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 参数列表错误(缺少，格式不匹配) 401 GET,POST,PUT,DELETE,PATCH 错误提示(消息) w未授权 403 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 访问受限，授权过期 404 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 资源，服务未找到 405 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 不允许的http方法 409 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 资源冲突，或者资源被锁定 415 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 不支持的数据(媒体)类型 429 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 请求过多被限制 500 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 系统内部错误 501 GET,POST,PUT,DELETE,PATCH 错误提示(消息) 接口未实现 统一异常处理rest客户端：https://insomnia.rest/ 初步测试现在我们启动项目，做下测试： 通过insomnia去访问： 发现参数不正确时，返回了400。看起来没问题 统一异常处理我们先修改controller的代码，把异常抛出： 12345678@PostMappingpublic ResponseEntity&lt;Item&gt; saveItem(Item item)&#123; // 校验价格 if (item.getPrice() == null)&#123; throw new RuntimeException("价格不能为空"); &#125; return ResponseEntity.status(HttpStatus.CREATED).body(itemService.saveItem(item));&#125; 接下来，我们使用SpringMVC提供的统一异常拦截器，因为时统一处理，我们放到common项目中： 新建一个类CommonExceptionHandler: 123456789@ControllerAdvice@Slf4jpublic class CommonExceptionHandler &#123; @ExceptionHandler(RuntimeException.class) public ResponseEntity&lt;String&gt; handleException(RuntimeException e)&#123; return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(e.getMessage()); &#125;&#125; 解读： @ControllerAdvice：默认情况下，会拦截所有加了@Controller注解的类 @ExceptionHandler(RuntimeException.class)：作用在方法上，声明要处理的异常类型，可以有多个，这里指定的时RuntimeException，被声明的方法可以看做是一个SPringMVC的Handler： 参数是要处理的异常，类型必须要匹配 返回结果可以是ModelAndView、ResponseEntity等，基本与handler类似 这里等于是从新定义了返回结果，我们可以随意指定想要的返回类型。此处使用了String 然后在service项目中引入common 12345&lt;dependency&gt; &lt;groupId&gt;com.study.common&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 重启项目测试： 发现成功获取了提示信息 但是，发现状态码和提示信息被写死了 自定义通用异常处理自定义通用异常消息枚举123456789@Getter@NoArgsConstructor@AllArgsConstructorpublic enum ExceptionEnum &#123; PRICE_CANNOT_BE_NULL(400,"价格不能为空") ; private int code; private String msg;&#125; 自定义异常返回结果类123456789101112@Datapublic class ExceptionResult &#123; private int status; private String message; private Long timestamp; public ExceptionResult(ExceptionEnum em) &#123; this.status = em.getCode(); this.message = em.getMsg(); this.timestamp = System.currentTimeMillis(); &#125;&#125; 自定义通用异常类123456@NoArgsConstructor@AllArgsConstructor@Getterpublic class CustomException extends RuntimeException &#123; private ExceptionEnum exceptionEnum;&#125; 自定义通用异常处理类12345678910@ControllerAdvice@Slf4jpublic class CommonExceptionHandler &#123; @ExceptionHandler(CustomException.class) public ResponseEntity&lt;ExceptionResult&gt; handleException(CustomException e)&#123; return ResponseEntity.status(e.getExceptionEnum().getCode()) .body(new ExceptionResult(e.getExceptionEnum())); &#125;&#125; 使用通用异常处理12345678910111213141516@RestController@RequestMapping("item")public class ItemController &#123; @Autowired private ItemService itemService; @PostMapping public ResponseEntity&lt;Item&gt; saveItem(Item item)&#123; // 校验价格 if (item.getPrice() == null)&#123; throw new CustomException(ExceptionEnum.PRICE_CANNOT_BE_NULL); &#125; return ResponseEntity.status(HttpStatus.CREATED).body(itemService.saveItem(item)); &#125;&#125; 启动项目，查看结果]]></content>
      <categories>
        <category>通用异常处理</category>
      </categories>
      <tags>
        <tag>通用异常处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用maven-tomcat插件实现tomcat热部署]]></title>
    <url>%2F2019%2F05%2F23%2FdevNote%2F%E4%BD%BF%E7%94%A8maven-tomcat%E6%8F%92%E4%BB%B6%E5%AE%9E%E7%8E%B0tomcat%E7%83%AD%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[配置tomcat开启manager项目 在tomcat目录中修改 vi config/tomcat-users.xml文件，在&lt;/tomcat-users&gt;前添加 123&lt;role rolename="manager-gui"/&gt;&lt;role rolename="manager-script"/&gt;&lt;user username="tomcat" password="managepwd" roles="manager-gui,manager-script"/&gt; 启动 tomcat 1./bin/startup.sh 点击Manager App出现登录弹窗 输入上面配置的用户名：tomcat，密码managepwd回车即可进入manager项目 集成tomcat插件配置pom.xml文件 123456789101112131415161718192021&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;!-- 控制tomcat端口号,部署到服务器时失效，以服务器启动的 tomcat 为准 --&gt; &lt;port&gt;80&lt;/port&gt; &lt;!-- 项目发布到tomcat后的名称 --&gt; &lt;!-- / 相当于把项目发布名称为ROOT --&gt; &lt;path&gt;/&lt;/path&gt; &lt;!-- 用户名和密码均以服务器上的 tomcat 配置为准 --&gt; &lt;username&gt;tomcat&lt;/username&gt; &lt;password&gt;managepwd&lt;/password&gt; &lt;!-- IP 以自己为准 --&gt; &lt;url&gt;http://132.232.137.183:8080/manager/text&lt;/url&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 部署项目 点击Add configuration 配置启动脚本 因为/对应的项目是ROOT文件，已存在，因此我们直接使用tomcat7:redeploy 点击Apply-&gt;OK，然后RUN 在webapp目录下新增WEB-INF/web.xml文件，不然会报错123456&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app version="3.0" xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd"&gt;&lt;/web-app&gt; 然后重新RUN验证即可访问http://132.232.137.183:8080/]]></content>
      <categories>
        <category>maven-tomcat</category>
      </categories>
      <tags>
        <tag>maven-tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA 常用快捷键]]></title>
    <url>%2F2019%2F05%2F23%2FdevNote%2FIntellij%20IDEA%20%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[Intellij IDEA 常用快捷键 智能提示 设置为Alt + / 代码模板生成 psvm sout ifn 生成getting、setting方法等 alt + Insert 编辑快捷键 删除行：ctrl + y 复制行：ctrl + d 注释行：ctrl + / 自动按照语法选中代码：ctrl + w 反向按照语法撤销选中代码：ctrl + shrft + w 前后单词移动光标：ctrl + left/right 查找 查找类或资源，提供模糊品牌：ctrl + n 全局搜索：双击 shift 内容搜索：ctrl + shift + f 代码格式化 格式化 import：ctrl + alt + o 格式化代码： ctrl + alt + l 切换标签窗体：ctrl + tab 选择最近打开的文件：ctrl + e 打开最近编辑过的文件：ctrl + shift + e 自我修复：alt + enter 撤销：ctrl + z 取消撤销：ctrl + shift +z]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在项目中使用 Maven 私服]]></title>
    <url>%2F2019%2F05%2F23%2FdevNote%2F%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%BD%BF%E7%94%A8%20Maven%20%E7%A7%81%E6%9C%8D%2F</url>
    <content type="text"><![CDATA[配置认证信息在 Maven settings.xml 中添加 Nexus 认证信息(servers 节点下)： 1234567891011&lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; Snapshots 与 Releases 的区别 nexus-releases: 用于发布 Release 版本 nexus-snapshots: 用于发布 Snapshot 版本（快照版） Release 版本与 Snapshot 定义如下： 12Release: 1.0.0/1.0.0-RELEASESnapshot: 1.0.0-SNAPSHOT 在项目 pom.xml 中设置的版本号添加 SNAPSHOT 标识的都会发布为 SNAPSHOT 版本，没有 SNAPSHOT 标识的都会发布为 RELEASE 版本。 SNAPSHOT 版本会自动加一个时间作为标识，如：1.0.0-SNAPSHOT发布后为变成 1.0.0-SNAPSHOT-20180522.123456-1.jar 配置自动化部署在 pom.xml 中添加如下代码： 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://127.0.0.1:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://127.0.0.1:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; 注意事项： id 名称必须要与 settings.xml 中 Servers 配置的 id 名称保持一致。 项目版本号中有 SNAPSHOT 标识的，会发布到 Nexus Snapshots Repository, 否则发布到 Nexus Release Repository，并根据 ID 去匹配授权账号。 部署到仓库1mvn deploy -Dmaven.test.skip=true 上传第三方 JAR 包 Nexus 3.0 不支持页面上传，可使用 maven 命令： 如第三方JAR包：kaptcha-2.3.jar12345678mvn deploy:deploy-file ^ -DgroupId=com.google.code.kaptcha ^ -DartifactId=kaptcha ^ -Dversion=2.3 ^ -Dpackaging=jar ^ -Dfile=jar包本地路径 ^ -Durl=http://私服ip:端口/repository/maven-releases/ ^ -DrepositoryId=nexus-releases 注意事项： 建议在上传第三方 JAR包时，创建单独的第三方JAR包管理仓库，便于管理有维护。（maven-3rd） -DrepositoryId=nexus-releases对应的是settings.xml中Servers配置的ID名称。（授权） 配置代理仓库1234567891011121314151617181920212223242526&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Repository&lt;/name&gt; &lt;url&gt;http://私服ip:端口/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Plugin Repository&lt;/name&gt; &lt;url&gt;http://私服ip:端口/repository/maven-public/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt;]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
</search>
